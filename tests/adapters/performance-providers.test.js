/**
 * ðŸ§ª High-Performance Provider Adapters Test Suite
 * Comprehensive tests for Together AI, Fireworks AI, DeepInfra, and Replicate
 */

import { jest } from '@jest/globals';
import TogetherAdapter from '../../src/loaders/adapters/TogetherAdapter.js';
import FireworksAdapter from '../../src/loaders/adapters/FireworksAdapter.js';

// Mock global fetch
global.fetch = jest.fn();

describe('High-Performance Provider Adapters', () => {
  beforeEach(() => {
    fetch.mockClear();
  });

  describe('TogetherAdapter', () => {
    let adapter;

    beforeEach(() => {
      adapter = new TogetherAdapter({
        apiKey: 'test-together-key',
        enableBatchMode: true,
        maxBatchSize: 5,
        preferredRegion: 'us-west-2'
      });
    });

    test('should initialize with correct configuration', () => {
      expect(adapter.provider).toBe('together');
      expect(adapter.baseURL).toBe('https://api.together.xyz/v1');
      expect(adapter.enableBatchMode).toBe(true);
      expect(adapter.maxBatchSize).toBe(5);
      expect(adapter.preferredRegion).toBe('us-west-2');
    });

    test('should load Together AI model successfully', async () => {
      // Mock model availability check
      fetch.mockResolvedValueOnce({\n        ok: true,\n        json: () => Promise.resolve({\n          data: [{ id: 'meta-llama/Llama-2-70b-chat-hf' }]\n        })\n      });\n\n      const model = await adapter.load('meta-llama/Llama-2-70b-chat-hf', {\n        testConnection: false\n      });\n\n      expect(model.id).toBe('together:meta-llama/Llama-2-70b-chat-hf');\n      expect(model.provider).toBe('together');\n      expect(model.metadata.streaming).toBe(true);\n      expect(model.metadata.high_performance).toBe(true);\n      expect(model.metadata.open_source).toBe(true);\n    });\n\n    test('should build correct request for chat models', () => {\n      const model = {\n        modelId: 'meta-llama/Llama-2-70b-chat-hf',\n        providerModel: 'meta-llama/Llama-2-70b-chat-hf',\n        metadata: { category: 'chat', maxOutput: 2048 }\n      };\n      \n      const request = adapter.buildTogetherRequest('Hello', model, { maxTokens: 1000 });\n      \n      expect(request).toMatchObject({\n        model: 'meta-llama/Llama-2-70b-chat-hf',\n        messages: [{ role: 'user', content: 'Hello' }],\n        max_tokens: 1000,\n        temperature: 0.7\n      });\n    });\n\n    test('should build correct request for completion models', () => {\n      const model = {\n        modelId: 'EleutherAI/gpt-neox-20b',\n        providerModel: 'EleutherAI/gpt-neox-20b',\n        metadata: { category: 'completion' }\n      };\n      \n      const request = adapter.buildTogetherRequest('Hello', model, {});\n      \n      expect(request).toMatchObject({\n        model: 'EleutherAI/gpt-neox-20b',\n        prompt: 'Hello',\n        stop: []\n      });\n    });\n\n    test('should get correct endpoint for different model categories', () => {\n      const chatModel = { metadata: { category: 'chat' } };\n      expect(adapter.getTogetherEndpoint(chatModel)).toBe('https://api.together.xyz/v1/chat/completions');\n      \n      const instructModel = { metadata: { category: 'instruct' } };\n      expect(adapter.getTogetherEndpoint(instructModel)).toBe('https://api.together.xyz/v1/chat/completions');\n      \n      const completionModel = { metadata: { category: 'completion' } };\n      expect(adapter.getTogetherEndpoint(completionModel)).toBe('https://api.together.xyz/v1/completions');\n    });\n\n    test('should parse chat completion response correctly', () => {\n      const model = {\n        id: 'together:meta-llama/Llama-2-70b-chat-hf',\n        metadata: { category: 'chat' }\n      };\n      \n      const data = {\n        choices: [{\n          message: { content: 'Hello response' },\n          finish_reason: 'stop'\n        }],\n        usage: {\n          prompt_tokens: 10,\n          completion_tokens: 5,\n          total_tokens: 15\n        }\n      };\n      \n      const result = adapter.parseTogetherResponse(data, model);\n      \n      expect(result.text).toBe('Hello response');\n      expect(result.usage.totalTokens).toBe(15);\n      expect(result.metadata.openSource).toBe(true);\n      expect(result.metadata.highPerformance).toBe(true);\n    });\n\n    test('should parse text completion response correctly', () => {\n      const model = {\n        id: 'together:EleutherAI/gpt-neox-20b',\n        metadata: { category: 'completion' }\n      };\n      \n      const data = {\n        choices: [{\n          text: 'Completed text',\n          finish_reason: 'stop'\n        }],\n        usage: {\n          prompt_tokens: 8,\n          completion_tokens: 3,\n          total_tokens: 11\n        }\n      };\n      \n      const result = adapter.parseTogetherResponse(data, model);\n      \n      expect(result.text).toBe('Completed text');\n      expect(result.usage.totalTokens).toBe(11);\n    });\n\n    test('should handle batch processing', async () => {\n      const model = { id: 'test-model', metadata: {} };\n      \n      // Add items to batch queue\n      const result1 = adapter.addToBatch('prompt1', model, {});\n      const result2 = adapter.addToBatch('prompt2', model, {});\n      \n      expect(result1).resolves.toMatchObject({\n        queued: true,\n        queueSize: 1\n      });\n      \n      expect(adapter.batchQueue).toHaveLength(2);\n    });\n\n    test('should auto-process batch when queue is full', async () => {\n      const model = { id: 'test-model', metadata: {} };\n      \n      // Mock successful completion responses\n      fetch.mockResolvedValue({\n        ok: true,\n        json: () => Promise.resolve({\n          choices: [{ message: { content: 'response' } }],\n          usage: { total_tokens: 10 }\n        })\n      });\n      \n      // Fill the queue to trigger batch processing\n      for (let i = 0; i < adapter.maxBatchSize; i++) {\n        adapter.addToBatch(`prompt${i}`, model, { batch: true });\n      }\n      \n      // The queue should be processed when it reaches maxBatchSize\n      expect(adapter.batchQueue.length).toBeLessThanOrEqual(adapter.maxBatchSize);\n    });\n\n    test('should fetch available models from API', async () => {\n      const mockModels = [\n        {\n          id: 'meta-llama/Llama-2-70b-chat-hf',\n          display_name: 'Llama 2 70B Chat',\n          context_length: 4096\n        },\n        {\n          id: 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n          display_name: 'Mixtral 8x7B Instruct',\n          context_length: 32768\n        }\n      ];\n      \n      fetch.mockResolvedValueOnce({\n        ok: true,\n        json: () => Promise.resolve({ data: mockModels })\n      });\n      \n      const models = await adapter.fetchAvailableModels();\n      \n      expect(models).toEqual(mockModels);\n      expect(fetch).toHaveBeenCalledWith(\n        'https://api.together.xyz/v1/models',\n        expect.objectContaining({\n          headers: expect.objectContaining({\n            'Authorization': 'Bearer test-together-key'\n          })\n        })\n      );\n    });\n\n    test('should calculate cost correctly', () => {\n      const usage = { prompt_tokens: 1000, completion_tokens: 500 };\n      const cost = adapter.calculateCost(usage, 'meta-llama/Llama-2-70b-chat-hf');\n      expect(cost).toBeCloseTo(0.0009 + 0.00045); // 0.9/1M * 1000 + 0.9/1M * 500\n    });\n\n    test('should dispose resources correctly', async () => {\n      // Add some items to batch queue\n      const model = { id: 'test-model', metadata: {} };\n      adapter.addToBatch('prompt1', model, {});\n      adapter.addToBatch('prompt2', model, {});\n      \n      expect(adapter.batchQueue).toHaveLength(2);\n      \n      await adapter.dispose();\n      \n      expect(adapter.models.size).toBe(0);\n      expect(adapter.modelCache.size).toBe(0);\n      expect(adapter.batchQueue).toHaveLength(0);\n    });\n  });\n\n  describe('FireworksAdapter', () => {\n    let adapter;\n\n    beforeEach(() => {\n      adapter = new FireworksAdapter({\n        apiKey: 'fw-test-key',\n        enableFireAttention: true,\n        enableStructuredOutput: true,\n        enableHIPAA: true,\n        enableSOC2: true\n      });\n    });\n\n    test('should initialize with correct configuration', () => {\n      expect(adapter.provider).toBe('fireworks');\n      expect(adapter.baseURL).toBe('https://api.fireworks.ai/inference/v1');\n      expect(adapter.enableFireAttention).toBe(true);\n      expect(adapter.enableStructuredOutput).toBe(true);\n      expect(adapter.enableHIPAA).toBe(true);\n      expect(adapter.enableSOC2).toBe(true);\n    });\n\n    test('should include compliance headers', () => {\n      const headers = adapter.getHeaders();\n      \n      expect(headers).toMatchObject({\n        'Authorization': 'Bearer fw-test-key',\n        'Content-Type': 'application/json',\n        'X-Fireworks-HIPAA': 'true',\n        'X-Fireworks-SOC2': 'true'\n      });\n    });\n\n    test('should load Fireworks model with enterprise features', async () => {\n      const model = await adapter.load('accounts/fireworks/models/llama-v3p1-70b-instruct', {\n        testConnection: false\n      });\n\n      expect(model.id).toBe('fireworks:accounts/fireworks/models/llama-v3p1-70b-instruct');\n      expect(model.provider).toBe('fireworks');\n      expect(model.metadata.streaming).toBe(true);\n      expect(model.metadata.enterprise).toBe(true);\n      expect(model.metadata.fire_attention).toBe(true);\n      expect(model.metadata.compliance).toContain('HIPAA');\n      expect(model.metadata.compliance).toContain('SOC2');\n    });\n\n    test('should build correct request for instruction models', () => {\n      const model = {\n        modelId: 'accounts/fireworks/models/llama-v3p1-70b-instruct',\n        metadata: { category: 'instruct', maxOutput: 4096 }\n      };\n      \n      const request = adapter.buildFireworksRequest('Hello', model, { maxTokens: 1000 });\n      \n      expect(request).toMatchObject({\n        model: 'accounts/fireworks/models/llama-v3p1-70b-instruct',\n        messages: [{ role: 'user', content: 'Hello' }],\n        max_tokens: 1000,\n        use_fire_attention: true\n      });\n    });\n\n    test('should build request with function calling', () => {\n      const model = {\n        modelId: 'accounts/fireworks/models/firefunction-v2',\n        metadata: { category: 'function' }\n      };\n      \n      const functions = [{\n        name: 'get_weather',\n        description: 'Get weather information',\n        parameters: { type: 'object' }\n      }];\n      \n      const request = adapter.buildFireworksRequest('Weather in NYC?', model, { functions });\n      \n      expect(request).toMatchObject({\n        messages: [{ role: 'user', content: 'Weather in NYC?' }],\n        functions,\n        function_call: 'auto'\n      });\n    });\n\n    test('should build request with structured output', () => {\n      const model = {\n        modelId: 'accounts/fireworks/models/firefunction-v2',\n        metadata: { category: 'function' }\n      };\n      \n      const responseFormat = { type: 'json_object' };\n      \n      const request = adapter.buildFireworksRequest('Generate JSON', model, { responseFormat });\n      \n      expect(request.response_format).toEqual(responseFormat);\n    });\n\n    test('should get correct endpoint for different categories', () => {\n      const functionModel = { metadata: { category: 'function' } };\n      expect(adapter.getFireworksEndpoint(functionModel)).toBe(\n        'https://api.fireworks.ai/inference/v1/chat/completions'\n      );\n      \n      const instructModel = { metadata: { category: 'instruct' } };\n      expect(adapter.getFireworksEndpoint(instructModel)).toBe(\n        'https://api.fireworks.ai/inference/v1/chat/completions'\n      );\n      \n      const completionModel = { metadata: { category: 'completion' } };\n      expect(adapter.getFireworksEndpoint(completionModel)).toBe(\n        'https://api.fireworks.ai/inference/v1/completions'\n      );\n    });\n\n    test('should parse response with function calls', () => {\n      const model = {\n        id: 'fireworks:accounts/fireworks/models/firefunction-v2',\n        metadata: { category: 'function' }\n      };\n      \n      const data = {\n        choices: [{\n          message: {\n            content: 'I need to get weather data.',\n            function_call: {\n              name: 'get_weather',\n              arguments: '{\"location\": \"New York\"}'\n            }\n          },\n          finish_reason: 'function_call'\n        }],\n        usage: {\n          prompt_tokens: 15,\n          completion_tokens: 10,\n          total_tokens: 25\n        }\n      };\n      \n      const result = adapter.parseFireworksResponse(data, model);\n      \n      expect(result.text).toBe('I need to get weather data.');\n      expect(result.metadata.functionCall).toEqual({\n        name: 'get_weather',\n        arguments: '{\"location\": \"New York\"}'\n      });\n      expect(result.metadata.fireAttention).toBe(true);\n    });\n\n    test('should handle caching', async () => {\n      const model = { id: 'test-model', metadata: {} };\n      const prompt = 'Test prompt';\n      const options = { maxTokens: 100 };\n      \n      // Generate cache key\n      const cacheKey = adapter.generateCacheKey(prompt, model, options);\n      expect(typeof cacheKey).toBe('string');\n      \n      // Test cache storage and retrieval\n      const response = { text: 'cached response' };\n      adapter.responseCache.set(cacheKey, {\n        response,\n        timestamp: Date.now()\n      });\n      \n      expect(adapter.responseCache.has(cacheKey)).toBe(true);\n    });\n\n    test('should handle batch processing', async () => {\n      adapter.enableBatching = true;\n      adapter.maxBatchSize = 3;\n      \n      const model = { id: 'test-model', metadata: {} };\n      \n      // Add items to batch\n      const result1 = adapter.addToBatch('prompt1', model, {});\n      const result2 = adapter.addToBatch('prompt2', model, {});\n      \n      expect(result1).resolves.toMatchObject({\n        queued: true,\n        queueSize: 1\n      });\n      \n      expect(adapter.batchQueue).toHaveLength(2);\n    });\n\n    test('should calculate cost correctly', () => {\n      const usage = { prompt_tokens: 1000, completion_tokens: 500 };\n      const cost = adapter.calculateCost(usage, 'accounts/fireworks/models/llama-v3p1-70b-instruct');\n      expect(cost).toBeCloseTo(0.0009 + 0.00045); // 0.9/1M * 1000 + 0.9/1M * 500\n    });\n\n    test('should provide comprehensive adapter info', () => {\n      const info = adapter.getInfo();\n      \n      expect(info).toMatchObject({\n        name: 'FireworksAdapter',\n        version: '1.0.0',\n        provider: 'fireworks',\n        fireAttention: true,\n        structuredOutput: true,\n        functionCalling: false,\n        compliance: {\n          HIPAA: true,\n          SOC2: true\n        }\n      });\n      \n      expect(info.features).toContain('fire_attention');\n      expect(info.features).toContain('enterprise_compliance');\n      expect(info.categories).toContain('function');\n    });\n\n    test('should dispose resources correctly', async () => {\n      // Add some cached responses and batch items\n      adapter.responseCache.set('test-key', { response: {}, timestamp: Date.now() });\n      adapter.addToBatch('test', { id: 'test' }, {});\n      \n      expect(adapter.responseCache.size).toBe(1);\n      expect(adapter.batchQueue.length).toBe(1);\n      \n      await adapter.dispose();\n      \n      expect(adapter.models.size).toBe(0);\n      expect(adapter.responseCache.size).toBe(0);\n      expect(adapter.batchQueue).toHaveLength(0);\n    });\n  });\n\n  // Integration tests for performance features\n  describe('Performance Integration', () => {\n    test('adapters should handle concurrent requests', async () => {\n      const adapter = new TogetherAdapter({ apiKey: 'test-key' });\n      \n      // Mock successful responses\n      fetch.mockResolvedValue({\n        ok: true,\n        json: () => Promise.resolve({\n          choices: [{ message: { content: 'response' } }],\n          usage: { total_tokens: 10 }\n        })\n      });\n      \n      const model = await adapter.load('meta-llama/Llama-2-70b-chat-hf', { testConnection: false });\n      \n      // Make multiple concurrent requests\n      const promises = Array.from({ length: 5 }, (_, i) => \n        adapter.complete(`prompt ${i}`, { model })\n      );\n      \n      const results = await Promise.all(promises);\n      \n      expect(results).toHaveLength(5);\n      results.forEach(result => {\n        expect(result).toHaveProperty('text');\n        expect(result).toHaveProperty('provider', 'together');\n      });\n    });\n\n    test('adapters should handle rate limiting gracefully', async () => {\n      const adapter = new FireworksAdapter({ apiKey: 'test-key' });\n      \n      // Mock rate limit response\n      fetch.mockRejectedValueOnce(new Error('Fireworks AI rate limit exceeded. Retry after 60 seconds.'));\n      \n      const model = await adapter.load('accounts/fireworks/models/llama-v3p1-8b-instruct', { testConnection: false });\n      \n      await expect(adapter.complete('test prompt', { model })).rejects.toThrow(/rate limit/i);\n    });\n\n    test('adapters should support streaming responses', async () => {\n      const adapter = new TogetherAdapter({ apiKey: 'test-key' });\n      \n      // Mock streaming response\n      const mockStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue('data: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\\n\\n');\n          controller.enqueue('data: {\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\\n\\n');\n          controller.enqueue('data: [DONE]\\n\\n');\n          controller.close();\n        }\n      });\n      \n      fetch.mockResolvedValueOnce({\n        ok: true,\n        body: mockStream\n      });\n      \n      const model = await adapter.load('meta-llama/Llama-2-13b-chat-hf', { testConnection: false });\n      \n      const chunks = [];\n      const streamGenerator = await adapter.complete('test', { model, stream: true });\n      \n      for await (const chunk of streamGenerator) {\n        chunks.push(chunk.text);\n      }\n      \n      expect(chunks).toEqual(['Hello', ' world']);\n    });\n  });\n});"}