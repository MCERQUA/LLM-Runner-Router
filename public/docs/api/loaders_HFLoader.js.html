<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/HFLoader.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/HFLoader.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * HuggingFace Model Loader
 * Supports loading models directly from HuggingFace Hub
 * Integrates with @huggingface/hub and transformers.js
 */

import { BaseLoader } from './BaseLoader.js';
import { Logger } from '../utils/Logger.js';
import { ModelDownloader } from '../services/ModelDownloader.js';
import path from 'path';
import fs from 'fs/promises';

class HFLoader extends BaseLoader {
  constructor() {
    super();
    this.logger = new Logger('HFLoader');
    this.models = new Map();
    this.hubClient = null;
    this.transformers = null;
    this.downloader = new ModelDownloader();
  }

  /**
   * Initialize HuggingFace dependencies
   * 
   * @example
   * // Check initialization status
   * const loader = new HFLoader();
   * const initialized = await loader.initialize();
   * 
   * if (initialized) {
   *   console.log('HuggingFace libraries available');
   *   // Can use hub client and transformers.js
   * } else {
   *   console.log('Some dependencies missing, fallback mode');
   *   // Will use direct download fallback
   * }
   * 
   * @example
   * // Initialize with error handling
   * const loader = new HFLoader();
   * try {
   *   await loader.initialize();
   *   console.log('Hub client ready:', !!loader.hubClient);
   *   console.log('Transformers.js ready:', !!loader.transformers);
   * } catch (error) {
   *   console.error('Initialization failed:', error.message);
   * }
   */
  async initialize() {
    try {
      // Try to import HuggingFace libraries
      const imports = await Promise.allSettled([
        import('@huggingface/hub'),
        import('@xenova/transformers')
      ]);
      
      if (imports[0].status === 'fulfilled') {
        this.hubClient = imports[0].value;
        this.logger.info('HuggingFace Hub client initialized');
      }
      
      if (imports[1].status === 'fulfilled') {
        this.transformers = imports[1].value;
        this.logger.info('Transformers.js initialized');
      }
      
      return true;
    } catch (error) {
      this.logger.warn('Some HuggingFace dependencies not available', error);
      return false;
    }
  }

  /**
   * Check if this loader supports the given source
   * 
   * @example
   * // Check various HuggingFace source formats
   * const loader = new HFLoader();
   * 
   * console.log(loader.supports('hf:gpt2'));                    // true
   * console.log(loader.supports('huggingface:microsoft/DialoGPT-medium')); // true
   * console.log(loader.supports('https://huggingface.co/bert-base-uncased')); // true
   * console.log(loader.supports('microsoft/DialoGPT-medium'));  // true
   * console.log(loader.supports('./local-model.bin'));         // false
   * 
   * @example
   * // Filter supported models from a list
   * const modelSources = [
   *   'gpt2',
   *   'microsoft/DialoGPT-medium',
   *   'https://huggingface.co/bert-base-uncased',
   *   './local/model.onnx',
   *   'hf:distilbert-base-uncased'
   * ];
   * 
   * const supported = modelSources.filter(src => loader.supports(src));
   * console.log(`${supported.length} HuggingFace models found:`, supported);
   * 
   * @example
   * // Dynamic loader routing
   * const source = 'microsoft/DialoGPT-medium';
   * 
   * if (hfLoader.supports(source)) {
   *   await hfLoader.load({ source });
   * } else if (ggufLoader.supports(source)) {
   *   await ggufLoader.load({ source });
   * } else {
   *   console.error('No loader supports this source');
   * }
   */
  supports(source) {
    if (typeof source !== 'string') return false;
    
    // Check for HuggingFace patterns
    if (source.startsWith('hf:')) return true;
    if (source.startsWith('huggingface:')) return true;
    if (source.includes('huggingface.co')) return true;
    if (source.match(/^[\w-]+\/[\w.-]+$/)) return true; // org/model format
    
    return false;
  }

  /**
   * Load a HuggingFace model
   * 
   * @example
   * // Load with Transformers.js (recommended)
   * const model = await loader.load({
   *   source: 'gpt2',
   *   task: 'text-generation',
   *   quantized: true  // Use quantized version for speed
   * });
   * 
   * const result = await model.generate('Hello world', { max_new_tokens: 20 });
   * console.log(result.generated_text);
   * 
   * @example
   * // Load BERT for text classification
   * const bert = await loader.load({
   *   source: 'distilbert-base-uncased-finetuned-sst-2-english',
   *   task: 'text-classification'
   * });
   * 
   * const sentiment = await bert.predict('I love this product!');
   * console.log('Sentiment:', sentiment[0].label); // POSITIVE/NEGATIVE
   * 
   * @example
   * // Load from Hub with custom cache
   * const model = await loader.load({
   *   source: 'microsoft/DialoGPT-medium',
   *   useTransformers: false, // Force Hub download
   *   cacheDir: './custom-cache',
   *   task: 'conversational'
   * });
   * 
   * @example
   * // Load with progress tracking
   * const model = await loader.load({
   *   source: 'facebook/bart-large-cnn',
   *   task: 'summarization',
   *   progress_callback: (progress) => {
   *     console.log(`Loading: ${progress.status}`);
   *     if (progress.loaded &amp;&amp; progress.total) {
   *       const percent = (progress.loaded / progress.total * 100).toFixed(1);
   *       console.log(`Progress: ${percent}%`);
   *     }
   *   }
   * });
   * 
   * @example
   * // Load with error handling and fallbacks
   * let model;
   * try {
   *   // Try transformers.js first
   *   model = await loader.load({
   *     source: 't5-small',
   *     task: 'text2text-generation',
   *     useTransformers: true
   *   });
   * } catch (error) {
   *   console.log('Transformers.js failed, trying Hub download');
   *   try {
   *     model = await loader.load({
   *       source: 't5-small',
   *       useTransformers: false
   *     });
   *   } catch (hubError) {
   *     console.error('All loading methods failed:', hubError.message);
   *     throw hubError;
   *   }
   * }
   */
  async load(config) {
    const modelId = config.id || `hf-${Date.now()}`;
    
    try {
      // Initialize if not already done
      if (!this.hubClient &amp;&amp; !this.transformers) {
        await this.initialize();
      }

      // Parse model identifier
      const modelName = this.parseModelName(config.source);
      this.logger.info(`Loading HuggingFace model: ${modelName}`);
      
      let model;
      
      // Determine loading strategy based on available libraries and config
      if (config.useTransformers !== false &amp;&amp; this.transformers) {
        // Use Transformers.js for in-browser/Node.js inference
        model = await this.loadWithTransformers(modelName, config);
      } else if (this.hubClient) {
        // Use HuggingFace Hub to download model files
        model = await this.loadFromHub(modelName, config);
      } else {
        // Fallback to direct download
        model = await this.loadDirect(modelName, config);
      }
      
      // Store model
      this.models.set(modelId, {
        ...model,
        config,
        modelName
      });

      this.logger.info(`HuggingFace model loaded successfully: ${modelId}`);
      
      return {
        id: modelId,
        name: config.name || modelName,
        format: 'huggingface',
        loaded: true,
        model: model.instance,
        metadata: model.metadata,
        predict: (input) => this.predict(modelId, input),
        stream: (input) => this.stream(modelId, input),
        unload: () => this.unload(modelId),
        tokenize: (text) => this.tokenize(modelId, text),
        generate: (prompt, options) => this.generate(modelId, prompt, options)
      };
    } catch (error) {
      this.logger.error(`Failed to load HuggingFace model: ${error.message}`);
      throw error;
    }
  }

  /**
   * Parse model name from various formats
   * 
   * @example
   * // Parse different input formats
   * const loader = new HFLoader();
   * 
   * console.log(loader.parseModelName('hf:gpt2'));                    // 'gpt2'
   * console.log(loader.parseModelName('huggingface:microsoft/DialoGPT-medium')); // 'microsoft/DialoGPT-medium'
   * console.log(loader.parseModelName('https://huggingface.co/bert-base-uncased')); // 'bert-base-uncased'
   * console.log(loader.parseModelName('microsoft/DialoGPT-medium'));  // 'microsoft/DialoGPT-medium'
   * 
   * @example
   * // Extract org and model names
   * const fullName = loader.parseModelName('microsoft/DialoGPT-medium');
   * const [org, modelName] = fullName.split('/');
   * console.log(`Organization: ${org}, Model: ${modelName}`);
   * 
   * @example
   * // Handle edge cases
   * const inputs = [
   *   'simple-model',
   *   'org/model-name',
   *   'hf:org/model-with-dashes',
   *   'https://huggingface.co/very/long/path/model'
   * ];
   * 
   * inputs.forEach(input => {
   *   const parsed = loader.parseModelName(input);
   *   console.log(`${input} -> ${parsed}`);
   * });
   */
  parseModelName(source) {
    // Remove prefixes
    let modelName = source
      .replace('hf:', '')
      .replace('huggingface:', '')
      .replace('https://huggingface.co/', '');
    
    // Extract org/model format
    const match = modelName.match(/([\w-]+\/[\w.-]+)/);
    if (match) {
      return match[1];
    }
    
    return modelName;
  }

  /**
   * Load model using Transformers.js
   * 
   * @example
   * // Load GPT-2 for text generation
   * const model = await loader.loadWithTransformers('gpt2', {
   *   task: 'text-generation',
   *   quantized: true
   * });
   * 
   * // model.instance is the transformers.js pipeline
   * const result = await model.instance('Hello world');
   * console.log(result[0].generated_text);
   * 
   * @example
   * // Load BERT with custom environment
   * const model = await loader.loadWithTransformers('bert-base-uncased', {
   *   task: 'fill-mask',
   *   localFiles: './models/bert/',  // Use local files
   *   quantized: false  // Use full precision
   * });
   * 
   * @example
   * // Load with progress tracking
   * const model = await loader.loadWithTransformers('t5-small', {
   *   task: 'text2text-generation',
   *   progress_callback: (progress) => {
   *     console.log('Loading progress:', progress.status);
   *     if (progress.file) {
   *       console.log('Current file:', progress.file);
   *     }
   *   }
   * });
   * 
   * @example
   * // Load image model
   * const visionModel = await loader.loadWithTransformers('google/vit-base-patch16-224', {
   *   task: 'image-classification',
   *   quantized: true
   * });
   * 
   * // Use with image data
   * const image = await fetch('./test-image.jpg').then(r => r.blob());
   * const classification = await visionModel.instance(image);
   * console.log('Top prediction:', classification[0]);
   */
  async loadWithTransformers(modelName, config) {
    if (!this.transformers) {
      throw new Error('Transformers.js not available');
    }

    const { pipeline, env } = this.transformers;
    
    // Configure environment
    if (config.localFiles) {
      env.localURL = config.localFiles;
    }
    if (config.remoteURL) {
      env.remoteURL = config.remoteURL;
    }
    
    // Determine task type
    const task = config.task || await this.inferTask(modelName);
    
    // Create pipeline
    const pipe = await pipeline(task, modelName, {
      quantized: config.quantized !== false, // Use quantized by default
      progress_callback: (progress) => {
        this.logger.debug(`Loading progress: ${progress.status}`);
      }
    });
    
    // Get model info
    const metadata = {
      task,
      modelId: modelName,
      tokenizer: pipe.tokenizer ? true : false,
      processor: pipe.processor ? true : false
    };
    
    return {
      instance: pipe,
      metadata,
      type: 'transformers'
    };
  }

  /**
   * Load model from HuggingFace Hub
   */
  async loadFromHub(modelName, config) {
    if (!this.hubClient) {
      throw new Error('HuggingFace Hub client not available');
    }

    const { listFiles, downloadFile } = this.hubClient;
    
    try {
      // List model files
      const files = await listFiles({
        repo: modelName,
        repo_type: 'model'
      });
      
      // Identify model files
      const modelFiles = files.filter(f => 
        f.path.endsWith('.bin') || 
        f.path.endsWith('.safetensors') || 
        f.path.endsWith('.onnx') ||
        f.path.endsWith('.json')
      );
      
      // Download model files
      const cacheDir = config.cacheDir || './models/cache';
      const modelDir = path.join(cacheDir, modelName);
      await fs.mkdir(modelDir, { recursive: true });
      
      const downloadedFiles = {};
      for (const file of modelFiles) {
        const localPath = path.join(modelDir, file.path);
        await fs.mkdir(path.dirname(localPath), { recursive: true });
        
        // Download file
        const blob = await downloadFile({
          repo: modelName,
          path: file.path
        });
        
        // Save to disk
        const buffer = Buffer.from(await blob.arrayBuffer());
        await fs.writeFile(localPath, buffer);
        downloadedFiles[file.path] = localPath;
      }
      
      // Load config
      const configPath = downloadedFiles['config.json'];
      let modelConfig = {};
      if (configPath) {
        const configText = await fs.readFile(configPath, 'utf8');
        modelConfig = JSON.parse(configText);
      }
      
      return {
        instance: {
          files: downloadedFiles,
          config: modelConfig
        },
        metadata: {
          modelId: modelName,
          files: Object.keys(downloadedFiles),
          ...modelConfig
        },
        type: 'hub'
      };
    } catch (error) {
      this.logger.error(`Failed to load from Hub: ${error.message}`);
      throw error;
    }
  }

  /**
   * Direct download fallback
   */
  async loadDirect(modelName, config) {
    try {
      // Use ModelDownloader service
      const modelPath = await this.downloader.download(
        `https://huggingface.co/${modelName}/resolve/main/model.safetensors`,
        config.cacheDir
      );
      
      return {
        instance: { path: modelPath },
        metadata: { modelId: modelName },
        type: 'direct'
      };
    } catch (error) {
      this.logger.error(`Direct download failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Infer task type from model name
   */
  async inferTask(modelName) {
    const nameLower = modelName.toLowerCase();
    
    if (nameLower.includes('bert') &amp;&amp; nameLower.includes('uncased')) {
      return 'fill-mask';
    }
    if (nameLower.includes('gpt') || nameLower.includes('llama') || nameLower.includes('mistral')) {
      return 'text-generation';
    }
    if (nameLower.includes('t5') || nameLower.includes('bart')) {
      return 'text2text-generation';
    }
    if (nameLower.includes('vit') || nameLower.includes('resnet')) {
      return 'image-classification';
    }
    if (nameLower.includes('whisper')) {
      return 'automatic-speech-recognition';
    }
    if (nameLower.includes('stable-diffusion')) {
      return 'text-to-image';
    }
    
    // Default to text generation
    return 'text-generation';
  }

  /**
   * Tokenize text
   * 
   * @example
   * // Basic tokenization
   * const model = await loader.load({ source: 'bert-base-uncased' });
   * const tokens = await loader.tokenize(model.id, 'Hello world!');
   * console.log('Tokens:', tokens);
   * 
   * @example
   * // Tokenize with BERT model
   * const bertModel = await loader.load({
   *   source: 'bert-base-uncased',
   *   task: 'fill-mask'
   * });
   * 
   * const text = 'The cat sat on the [MASK].';
   * const tokens = await loader.tokenize(bertModel.id, text);
   * console.log('BERT tokens:', tokens);
   * 
   * @example
   * // Compare tokenization methods
   * const text = 'This is a test sentence.';
   * 
   * // With transformers tokenizer
   * const transformerTokens = await loader.tokenize(model.id, text);
   * 
   * // Fallback tokenization
   * const fallbackTokens = text.split(' ');
   * 
   * console.log('Transformer tokens:', transformerTokens.length);
   * console.log('Simple split tokens:', fallbackTokens.length);
   * 
   * @example
   * // Tokenize multiple texts
   * const texts = [
   *   'Short text.',
   *   'This is a much longer piece of text with many words.',
   *   'Special characters: !@#$%^&amp;*()'
   * ];
   * 
   * for (const text of texts) {
   *   const tokens = await loader.tokenize(model.id, text);
   *   console.log(`"${text}" -> ${tokens.length} tokens`);
   * }
   */
  async tokenize(modelId, text) {
    const model = this.models.get(modelId);
    if (!model) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    if (model.type === 'transformers' &amp;&amp; model.instance.tokenizer) {
      const tokens = await model.instance.tokenizer(text);
      return tokens;
    }
    
    // Fallback to simple tokenization
    return text.split(' ');
  }

  /**
   * Generate text (for generative models)
   * 
   * @example
   * // Simple text generation
   * const model = await loader.load({ source: 'gpt2' });
   * const result = await loader.generate(model.id, 'Once upon a time');
   * console.log(result.generated_text);
   * 
   * @example
   * // Generation with custom parameters
   * const result = await loader.generate(model.id, 'The future of AI is', {
   *   maxTokens: 100,      // Generate up to 100 tokens
   *   temperature: 0.8,    // More creative
   *   doSample: true,      // Use sampling
   *   topP: 0.9,          // Nucleus sampling
   *   repetition_penalty: 1.2  // Reduce repetition
   * });
   * 
   * @example
   * // Conversational generation
   * const chatModel = await loader.load({ 
   *   source: 'microsoft/DialoGPT-medium',
   *   task: 'text-generation'
   * });
   * 
   * const conversation = [
   *   'User: Hello, how are you?',
   *   'Bot: I am doing well, thank you!',
   *   'User: What can you help me with?'
   * ].join(' ');
   * 
   * const response = await loader.generate(chatModel.id, conversation, {
   *   maxTokens: 50,
   *   temperature: 0.7
   * });
   * 
   * @example
   * // Batch generation
   * const prompts = [
   *   'The weather today is',
   *   'My favorite food is',
   *   'The best movie ever made is'
   * ];
   * 
   * const results = await Promise.all(
   *   prompts.map(prompt => loader.generate(model.id, prompt, {
   *     maxTokens: 20,
   *     temperature: 0.7
   *   }))
   * );
   * 
   * results.forEach((result, i) => {
   *   console.log(`${prompts[i]} -> ${result.generated_text}`);
   * });
   */
  async generate(modelId, prompt, options = {}) {
    const model = this.models.get(modelId);
    if (!model) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    if (model.type === 'transformers') {
      const result = await model.instance(prompt, {
        max_new_tokens: options.maxTokens || 100,
        temperature: options.temperature || 0.7,
        do_sample: options.doSample !== false,
        top_p: options.topP || 0.95,
        ...options
      });
      
      return result;
    }
    
    // Fallback
    return { generated_text: prompt + ' [Model output placeholder]' };
  }

  /**
   * Run prediction with the model
   * 
   * @example
   * // Text classification
   * const classifier = await loader.load({
   *   source: 'distilbert-base-uncased-finetuned-sst-2-english',
   *   task: 'text-classification'
   * });
   * 
   * const result = await loader.predict(classifier.id, 'I love this movie!');
   * console.log('Sentiment:', result[0].label, 'Score:', result[0].score);
   * 
   * @example
   * // Question answering
   * const qa = await loader.load({
   *   source: 'distilbert-base-cased-distilled-squad',
   *   task: 'question-answering'
   * });
   * 
   * const context = 'Paris is the capital of France.';
   * const question = 'What is the capital of France?';
   * 
   * const answer = await loader.predict(qa.id, {
   *   question,
   *   context
   * });
   * console.log('Answer:', answer.answer);
   * 
   * @example
   * // Fill mask (BERT-style)
   * const fillMask = await loader.load({
   *   source: 'bert-base-uncased',
   *   task: 'fill-mask'
   * });
   * 
   * const result = await loader.predict(fillMask.id, 'The cat sat on the [MASK].');
   * result.forEach(prediction => {
   *   console.log(`${prediction.token_str}: ${prediction.score.toFixed(3)}`);
   * });
   * 
   * @example
   * // Hub-downloaded model (non-transformers)
   * const hubModel = await loader.load({
   *   source: 'microsoft/DialoGPT-medium',
   *   useTransformers: false  // Force hub download
   * });
   * 
   * const result = await loader.predict(hubModel.id, 'Hello there');
   * console.log('Result:', result); // Will show warning about direct inference
   */
  async predict(modelId, input) {
    const model = this.models.get(modelId);
    if (!model) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    try {
      if (model.type === 'transformers') {
        // Use transformers pipeline
        const result = await model.instance(input);
        return result;
      } else {
        // For hub or direct downloads, we'd need additional inference logic
        // This would typically involve loading the model with appropriate runtime
        return {
          warning: 'Direct inference not implemented for hub models',
          input,
          modelType: model.type
        };
      }
    } catch (error) {
      this.logger.error(`Prediction failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Stream predictions
   * 
   * @example
   * // Stream text generation
   * const generator = await loader.load({
   *   source: 'gpt2',
   *   task: 'text-generation'
   * });
   * 
   * console.log('Streaming generation:');
   * for await (const token of loader.stream(generator.id, 'The future of AI')) {
   *   process.stdout.write(token);
   * }
   * console.log('\n--- Generation complete ---');
   * 
   * @example
   * // Stream with token accumulation
   * const model = await loader.load({ source: 'gpt2' });
   * let fullText = 'Once upon a time';
   * let tokenCount = 0;
   * 
   * for await (const token of loader.stream(model.id, 'Once upon a time')) {
   *   fullText += token;
   *   tokenCount++;
   *   
   *   console.log(`Token ${tokenCount}: "${token}"`);
   *   
   *   if (tokenCount >= 10) break; // Limit for demo
   * }
   * 
   * console.log('Complete story start:', fullText);
   * 
   * @example
   * // Non-streaming model fallback
   * const classifier = await loader.load({
   *   source: 'distilbert-base-uncased-finetuned-sst-2-english',
   *   task: 'text-classification'
   * });
   * 
   * // This model doesn't support streaming, so it yields the full result
   * for await (const result of loader.stream(classifier.id, 'Great product!')) {
   *   console.log('Classification result:', result);
   * }
   * 
   * @example
   * // Real-time chat streaming
   * const chatModel = await loader.load({
   *   source: 'microsoft/DialoGPT-medium',
   *   task: 'text-generation'
   * });
   * 
   * const userMessage = 'Hello, how can you help me?';
   * let botResponse = '';
   * 
   * console.log('User:', userMessage);
   * process.stdout.write('Bot: ');
   * 
   * for await (const token of loader.stream(chatModel.id, userMessage)) {
   *   botResponse += token;
   *   process.stdout.write(token);
   *   
   *   // Add natural typing delay
   *   await new Promise(resolve => setTimeout(resolve, 50));
   * }
   * 
   * console.log('\n--- Conversation turn complete ---');
   */
  async *stream(modelId, input) {
    const model = this.models.get(modelId);
    if (!model) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    try {
      if (model.type === 'transformers') {
        // Check if model supports streaming
        const task = model.metadata.task;
        
        if (task === 'text-generation' || task === 'text2text-generation') {
          // Generate with streaming
          const options = {
            max_new_tokens: 100,
            do_sample: true,
            temperature: 0.7,
            return_full_text: false
          };
          
          // Simulate streaming by generating tokens
          let generated = '';
          const tokens = input.split(' ');
          
          for (let i = 0; i &lt; 20; i++) { // Generate up to 20 tokens
            const partial = await model.instance(input + generated, {
              ...options,
              max_new_tokens: 1
            });
            
            const newToken = this.extractNewToken(partial, generated);
            if (newToken) {
              generated += newToken;
              yield newToken;
            }
            
            // Check for end token
            if (newToken === '&lt;/s>' || newToken === '&lt;|endoftext|>') {
              break;
            }
          }
        } else {
          // Non-streaming task
          const result = await this.predict(modelId, input);
          yield result;
        }
      } else {
        // Fallback for non-transformers models
        const result = await this.predict(modelId, input);
        yield result;
      }
    } catch (error) {
      this.logger.error(`Streaming failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Extract new token from generation
   */
  extractNewToken(result, previousText) {
    if (Array.isArray(result) &amp;&amp; result[0]?.generated_text) {
      const fullText = result[0].generated_text;
      return fullText.substring(previousText.length);
    }
    return '';
  }

  /**
   * Unload a model
   */
  async unload(modelId) {
    const model = this.models.get(modelId);
    if (model) {
      // Clean up model resources
      if (model.type === 'transformers' &amp;&amp; model.instance.dispose) {
        await model.instance.dispose();
      }
      
      this.models.delete(modelId);
      this.logger.info(`Model ${modelId} unloaded`);
      return true;
    }
    return false;
  }

  /**
   * Get model info
   */
  getModelInfo(modelId) {
    const model = this.models.get(modelId);
    if (!model) {
      return null;
    }

    return {
      id: modelId,
      format: 'huggingface',
      loaded: true,
      type: model.type,
      modelName: model.modelName,
      metadata: model.metadata
    };
  }

  /**
   * Validate model availability
   * 
   * @example
   * // Validate before loading
   * const validation = await loader.validate({ source: 'gpt2' });
   * 
   * if (validation.valid) {
   *   console.log('Model is available on HuggingFace Hub');
   *   console.log('Downloads:', validation.modelInfo.downloads);
   *   console.log('Likes:', validation.modelInfo.likes);
   *   await loader.load({ source: 'gpt2' });
   * } else {
   *   console.error('Model not found:', validation.error);
   * }
   * 
   * @example
   * // Batch validate multiple models
   * const modelNames = [
   *   'gpt2',
   *   'bert-base-uncased',
   *   'nonexistent-model',
   *   'microsoft/DialoGPT-medium'
   * ];
   * 
   * const validations = await Promise.all(
   *   modelNames.map(async (name) => {
   *     const result = await loader.validate({ source: name });
   *     return { name, ...result };
   *   })
   * );
   * 
   * const validModels = validations.filter(v => v.valid);
   * console.log(`${validModels.length}/${modelNames.length} models are available`);
   * 
   * validModels.forEach(model => {
   *   console.log(`${model.name}: ${model.modelInfo.downloads} downloads`);
   * });
   * 
   * @example
   * // Validate with detailed error handling
   * try {
   *   const validation = await loader.validate({ 
   *     source: 'maybe-exists/some-model' 
   *   });
   *   
   *   if (validation.valid) {
   *     const info = validation.modelInfo;
   *     console.log(`Model found: ${info.id}`);
   *     console.log(`Tags: ${info.tags.join(', ')}`);
   *     
   *     if (info.downloads > 1000) {
   *       console.log('Popular model, should be reliable');
   *     }
   *   } else {
   *     console.log(`Model validation failed: ${validation.error}`);
   *   }
   * } catch (error) {
   *   console.error('Network error during validation:', error.message);
   * }
   * 
   * @example
   * // Use validation for model selection
   * const candidates = ['gpt2', 'gpt2-medium', 'gpt2-large'];
   * let selectedModel = null;
   * 
   * for (const candidate of candidates) {
   *   const validation = await loader.validate({ source: candidate });
   *   if (validation.valid) {
   *     selectedModel = candidate;
   *     console.log(`Selected model: ${candidate}`);
   *     break;
   *   }
   * }
   * 
   * if (selectedModel) {
   *   await loader.load({ source: selectedModel });
   * } else {
   *   console.error('No valid models found in candidates');
   * }
   */
  async validate(config) {
    try {
      const modelName = this.parseModelName(config.source);
      
      // Try to check if model exists on HuggingFace
      const response = await fetch(`https://huggingface.co/api/models/${modelName}`);
      
      if (response.ok) {
        const modelInfo = await response.json();
        return {
          valid: true,
          modelInfo: {
            id: modelInfo.modelId,
            downloads: modelInfo.downloads,
            likes: modelInfo.likes,
            tags: modelInfo.tags
          }
        };
      }
      
      return {
        valid: false,
        error: `Model ${modelName} not found on HuggingFace Hub`
      };
    } catch (error) {
      return {
        valid: false,
        error: error.message
      };
    }
  }
}
export default HFLoader;
export { HFLoader };
</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Mon Aug 18 2025 01:43:35 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
