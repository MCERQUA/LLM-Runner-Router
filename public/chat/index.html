<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- Enhanced Title -->
    <title>LLM Router Live Demo - Interactive AI Model Routing & Streaming Chat</title>
    
    <!-- Meta Description -->
    <meta name="description" content="Try LLM Runner Router live demo: Interactive chat with real-time AI model routing, streaming responses, strategy selection, and performance monitoring. No API keys required.">
    
    <!-- Keywords -->
    <meta name="keywords" content="LLM demo, AI chat demo, model routing demo, streaming AI, live AI demo, interactive machine learning, WebGPU demo, AI model comparison, real-time AI">
    
    <!-- Author & Robot Instructions -->
    <meta name="author" content="Echo AI Systems">
    <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    
    <!-- Open Graph Tags -->
    <meta property="og:title" content="LLM Router Live Demo - Interactive AI Model Routing">
    <meta property="og:description" content="Experience intelligent AI model routing in real-time. Free interactive demo with streaming responses and performance analytics.">
    <meta property="og:type" content="webapp">
    <meta property="og:url" content="https://github.com/MCERQUA/LLM-Runner-Router/chat">
    <meta property="og:site_name" content="LLM Runner Router">
    <meta property="og:locale" content="en_US">
    
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="LLM Router Interactive Demo">
    <meta name="twitter:description" content="Try live AI model routing demo with streaming responses and real-time performance monitoring.">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="https://github.com/MCERQUA/LLM-Runner-Router/chat">
    
    <!-- WebApp Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "LLM Router Interactive Demo",
        "description": "Interactive demonstration of AI model routing with real-time streaming and performance monitoring",
        "applicationCategory": "DemoApplication",
        "operatingSystem": "Any",
        "url": "https://github.com/MCERQUA/LLM-Runner-Router/chat",
        "author": {
            "@type": "Organization",
            "name": "Echo AI Systems"
        },
        "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD"
        }
    }
    </script>
    <link rel="stylesheet" href="/chat/styles.css">
</head>
<body>
    <div class="container">
        <header class="header">
            <div class="logo">
                <h1>üß† LLM Runner Router</h1>
                <p>Real HuggingFace Model Demo</p>
            </div>
            <div class="status-indicator">
                <div class="status-dot offline" id="statusDot"></div>
                <span id="statusText">Initializing...</span>
            </div>
            <div class="header-actions">
                <a href="/chat/model-selector.html" class="admin-btn" title="Model Manager">
                    ü§ñ Models
                </a>
                <a href="/chat/admin.html" class="admin-btn" title="Admin Settings">
                    ‚öôÔ∏è Settings
                </a>
            </div>
        </header>

        <div class="main-content">
            <div class="sidebar">
                <div class="controls-panel">
                    <h3>üéõÔ∏è Router Configuration</h3>
                    
                    <div class="control-group">
                        <label for="strategySelect">Routing Strategy:</label>
                        <select id="strategySelect" class="control-input">
                            <option value="balanced">‚öñÔ∏è Balanced</option>
                            <option value="quality-first">üèÜ Quality First</option>
                            <option value="cost-optimized">üíµ Cost Optimized</option>
                            <option value="speed-priority">‚ö° Speed Priority</option>
                            <option value="random">üé≤ Random</option>
                            <option value="round-robin">üîÑ Round Robin</option>
                        </select>
                    </div>

                    <div class="control-group">
                        <label for="temperatureSlider">Temperature: <span id="tempValue">0.7</span></label>
                        <input type="range" id="temperatureSlider" class="control-slider" min="0" max="1" step="0.1" value="0.7">
                    </div>

                    <div class="control-group">
                        <label for="maxTokensSlider">Max Tokens: <span id="tokensValue">150</span></label>
                        <input type="range" id="maxTokensSlider" class="control-slider" min="50" max="500" step="25" value="150">
                    </div>

                    <div class="control-group">
                        <label class="checkbox-label">
                            <input type="checkbox" id="streamingCheckbox" checked>
                            <span class="checkmark"></span>
                            Enable Streaming
                        </label>
                    </div>
                </div>

                <div class="model-status">
                    <h3>üìä Model Status</h3>
                    <div class="model-list" id="modelList">
                        <div class="model-item">
                            <div class="model-name">Demo Mode</div>
                            <div class="model-status-badge simulated">Simulated</div>
                        </div>
                    </div>
                </div>

                <div class="stats-panel">
                    <h3>üìà Session Stats</h3>
                    <div class="stat-item">
                        <span class="stat-label">Messages:</span>
                        <span class="stat-value" id="messageCount">0</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">Avg Response:</span>
                        <span class="stat-value" id="avgResponse">-</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">Total Tokens:</span>
                        <span class="stat-value" id="totalTokens">0</span>
                    </div>
                </div>
            </div>

            <div class="chat-container">
                <div class="chat-messages" id="chatMessages">
                    <div class="welcome-message">
                        <div class="message-content">
                            <h3>üëã Welcome to LLM Runner Router - HuggingFace Demo!</h3>
                            <p>This is a <strong>real working demo</strong> that runs open source models from HuggingFace directly in your browser using our intelligent orchestration system.</p>
                            <div class="feature-highlights">
                                <div class="highlight-item">ü§ñ <strong>Open Source Models:</strong> DialoGPT, Zephyr-7B, local inference</div>
                                <div class="highlight-item">üéØ <strong>Smart Routing:</strong> Automatically selects optimal model based on strategy</div>
                                <div class="highlight-item">üîÑ <strong>Multi-Engine:</strong> WebGPU acceleration with WASM fallback</div>
                                <div class="highlight-item">üìä <strong>Live Analytics:</strong> Real performance metrics and model status</div>
                                <div class="highlight-item">üÜì <strong>Completely Free:</strong> No API keys, no costs, all local</div>
                            </div>
                            <p><strong>Ask me anything!</strong> The LLM Router will intelligently select and load the best HuggingFace model for your query.</p>
                            <p><small>üí° <strong>Demo Mode:</strong> This showcases the LLM Router interface and capabilities. In a real deployment, it would connect to actual HuggingFace models for local inference.</small></p>
                        </div>
                    </div>
                </div>

                <div class="input-container">
                    <div class="input-wrapper">
                        <textarea id="messageInput" placeholder="Type your message here... (Press Ctrl+Enter to send)" 
                                rows="2" maxlength="1000"></textarea>
                        <div class="input-actions">
                            <button id="clearChat" class="btn-secondary" title="Clear Chat">üóëÔ∏è</button>
                            <button id="sendButton" class="btn-primary" title="Send Message">
                                <span class="btn-text">Send</span>
                                <span class="btn-icon">üì§</span>
                            </button>
                        </div>
                    </div>
                    <div class="input-footer">
                        <span class="char-count" id="charCount">0/1000</span>
                        <span class="shortcut-hint">Ctrl+Enter to send</span>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="floating-notifications" id="notifications"></div>

    <script src="/chat/chat-production.js"></script>
</body>
</html>