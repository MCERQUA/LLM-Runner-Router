# Basic Usage Examples

This guide demonstrates the fundamental usage patterns of the LLM-Runner-Router system. Perfect for getting started with the core functionality.

## Table of Contents
- [Quick Start](#quick-start)
- [Loading Models](#loading-models)
- [Simple Inference](#simple-inference)
- [Configuration](#configuration)
- [Error Handling](#error-handling)
- [Environment Detection](#environment-detection)

## Quick Start

### Installation and Basic Setup

```javascript
// Install the package
// npm install llm-runner-router

// Basic import and initialization
import LLMRouter from 'llm-runner-router';

// Create router instance
const router = new LLMRouter();

// Initialize the system
await router.initialize();

console.log('Router ready!');
```

### Your First Inference

```javascript
import { quick } from 'llm-runner-router';

// Quick inference with auto-model selection
const response = await quick("What is artificial intelligence?");

console.log(response.text);
console.log(`Generated by: ${response.model}`);
console.log(`Tokens: ${response.tokens}, Latency: ${response.latency}ms`);
```

## Loading Models

### Simple Model Loading

```javascript
import LLMRouter from 'llm-runner-router';

const router = new LLMRouter();
await router.initialize();

// Load a local GGUF model
const model1 = await router.load('models/llama-7b-q4.gguf');

// Load from HuggingFace Hub
const model2 = await router.load('huggingface:microsoft/DialoGPT-medium');

// Load with URL
const model3 = await router.load('https://example.com/models/my-model.bin');

console.log('All models loaded successfully!');
```

### Advanced Model Loading

```javascript
// Load with specific configuration
const model = await router.load({
  source: 'models/llama-13b.gguf',
  format: 'gguf',          // Optional format override
  immediate: false,        // Lazy loading
  config: {
    quantization: 'q4_k_m',
    context: 4096,
    threads: 8,
    useGPU: true
  }
});

console.log(`Model loaded: ${model.id}`);
console.log(`Format: ${model.format}`);
console.log(`Size: ${model.size} bytes`);
```

## Simple Inference

### Basic Text Generation

```javascript
const router = new LLMRouter();
await router.initialize();

// Load your preferred model
await router.load('models/llama-7b.gguf');

// Generate text
const result = await router.quick("Explain quantum computing in simple terms");

console.log('Generated Text:');
console.log(result.text);
console.log(`\nModel: ${result.model}`);
console.log(`Tokens: ${result.tokens}`);
console.log(`Time: ${result.latency}ms`);
```

### With Custom Parameters

```javascript
const result = await router.quick(
  "Write a creative story about a robot",
  {
    maxTokens: 200,
    temperature: 0.8,    // More creative
    topP: 0.9,
    topK: 40
  }
);

console.log(result.text);
```

### Multiple Inference Examples

```javascript
const prompts = [
  "What is machine learning?",
  "Explain neural networks",
  "What is deep learning?"
];

for (const prompt of prompts) {
  console.log(`\n--- ${prompt} ---`);
  const result = await router.quick(prompt, { maxTokens: 100 });
  console.log(result.text);
}
```

## Configuration

### Basic Configuration

```javascript
const router = new LLMRouter({
  strategy: 'balanced',        // Routing strategy
  maxModels: 50,              // Max models in registry
  cacheTTL: 3600000,          // Cache TTL (1 hour)
  logLevel: 'info'            // Logging level
});

await router.initialize();
```

### Advanced Configuration

```javascript
const router = new LLMRouter({
  strategy: 'quality-first',
  maxModels: 100,
  cacheTTL: 7200000,          // 2 hours
  logLevel: 'debug',
  
  // Engine preferences
  preferredEngine: 'webgpu',
  
  // Memory limits
  maxMemoryUsage: 8 * 1024 * 1024 * 1024, // 8GB
  
  // Timeout settings
  defaultTimeout: 30000,      // 30 seconds
  
  // Retry configuration
  retryConfig: {
    maxAttempts: 3,
    backoff: 'exponential',
    initialDelay: 1000
  }
});

await router.initialize();
```

### Runtime Configuration Updates

```javascript
// Update configuration at runtime
router.updateConfig({
  cacheTTL: 1800000,          // 30 minutes
  maxTokens: 1000
});

// Get current configuration
const config = router.getConfig();
console.log('Current config:', config);
```

## Error Handling

### Basic Error Handling

```javascript
import LLMRouter from 'llm-runner-router';

async function safeInference() {
  const router = new LLMRouter();
  
  try {
    await router.initialize();
    const result = await router.quick("Hello world");
    console.log(result.text);
  } catch (error) {
    console.error('Error during inference:', error.message);
    
    // Specific error handling
    if (error.name === 'ModelNotFoundError') {
      console.log('Try loading a model first');
    } else if (error.name === 'TimeoutError') {
      console.log('Request timed out, try again');
    }
  } finally {
    await router.cleanup();
  }
}

safeInference();
```

### Model Loading Error Handling

```javascript
async function loadModelSafely(modelPath) {
  try {
    const model = await router.load(modelPath);
    console.log(`‚úÖ Model loaded: ${model.id}`);
    return model;
  } catch (error) {
    console.error(`‚ùå Failed to load model: ${modelPath}`);
    console.error(`Error: ${error.message}`);
    
    // Try alternative models
    const alternatives = [
      'models/backup-model.gguf',
      'huggingface:microsoft/DialoGPT-small'
    ];
    
    for (const alt of alternatives) {
      try {
        console.log(`üîÑ Trying alternative: ${alt}`);
        return await router.load(alt);
      } catch (altError) {
        console.log(`‚ùå Alternative failed: ${alt}`);
      }
    }
    
    throw new Error('No models could be loaded');
  }
}

// Usage
try {
  const model = await loadModelSafely('models/preferred-model.gguf');
} catch (error) {
  console.error('All model loading attempts failed');
}
```

## Environment Detection

### Cross-Platform Usage

```javascript
import LLMRouter from 'llm-runner-router';

const router = new LLMRouter();

// Check environment before initialization
const status = router.getStatus();
console.log('Environment:', status.environment);

// Environment-specific configuration
let config = {};

switch (status.environment) {
  case 'browser':
    config = {
      preferredEngine: 'webgpu',
      maxMemoryUsage: 2 * 1024 * 1024 * 1024 // 2GB for browser
    };
    break;
    
  case 'node':
    config = {
      preferredEngine: 'node',
      maxMemoryUsage: 8 * 1024 * 1024 * 1024 // 8GB for Node.js
    };
    break;
    
  case 'worker':
    config = {
      preferredEngine: 'wasm',
      maxMemoryUsage: 1 * 1024 * 1024 * 1024 // 1GB for workers
    };
    break;
}

router.updateConfig(config);
await router.initialize();
```

### Browser-Specific Example

```html
<!DOCTYPE html>
<html>
<head>
    <title>LLM Router in Browser</title>
</head>
<body>
    <div id="output"></div>
    <script type="module">
        import LLMRouter from './node_modules/llm-runner-router/dist/browser.js';
        
        async function runInBrowser() {
            const router = new LLMRouter({
                preferredEngine: 'webgpu',
                logLevel: 'info'
            });
            
            await router.initialize();
            
            // Load a web-compatible model
            await router.load('https://example.com/models/small-model.onnx');
            
            const result = await router.quick("Hello from browser!");
            
            document.getElementById('output').textContent = result.text;
        }
        
        runInBrowser().catch(console.error);
    </script>
</body>
</html>
```

### Node.js-Specific Example

```javascript
// node-example.js
import LLMRouter from 'llm-runner-router';
import fs from 'fs/promises';
import path from 'path';

async function runInNode() {
  const router = new LLMRouter({
    preferredEngine: 'node',
    logLevel: 'debug'
  });
  
  await router.initialize();
  
  // Load local model
  const modelPath = path.join(process.cwd(), 'models', 'llama-7b.gguf');
  
  // Check if model exists
  try {
    await fs.access(modelPath);
    await router.load(modelPath);
  } catch (error) {
    console.log('Local model not found, downloading...');
    // Download model logic here
  }
  
  const result = await router.quick("Hello from Node.js!");
  console.log(result.text);
  
  await router.cleanup();
}

runInNode().catch(console.error);
```

## Complete Working Example

```javascript
// complete-example.js
import LLMRouter from 'llm-runner-router';

class SimpleAI {
  constructor() {
    this.router = new LLMRouter({
      strategy: 'balanced',
      logLevel: 'info'
    });
  }
  
  async initialize() {
    await this.router.initialize();
    
    // Load a default model
    try {
      await this.router.load('models/default-model.gguf');
      console.log('‚úÖ Default model loaded');
    } catch (error) {
      console.log('‚ö†Ô∏è No local model, using auto-selection');
    }
  }
  
  async ask(question) {
    try {
      const result = await this.router.quick(question, {
        maxTokens: 150,
        temperature: 0.7
      });
      
      return {
        answer: result.text,
        model: result.model,
        time: result.latency
      };
    } catch (error) {
      return {
        error: error.message,
        answer: "I'm sorry, I couldn't process that question."
      };
    }
  }
  
  async cleanup() {
    await this.router.cleanup();
  }
}

// Usage
async function main() {
  const ai = new SimpleAI();
  
  try {
    await ai.initialize();
    
    const questions = [
      "What is artificial intelligence?",
      "How do neural networks work?",
      "What is the future of AI?"
    ];
    
    for (const question of questions) {
      console.log(`\n‚ùì ${question}`);
      const response = await ai.ask(question);
      
      if (response.error) {
        console.log(`‚ùå Error: ${response.error}`);
      } else {
        console.log(`ü§ñ ${response.answer}`);
        console.log(`üìä Model: ${response.model}, Time: ${response.time}ms`);
      }
    }
  } finally {
    await ai.cleanup();
  }
}

main().catch(console.error);
```

## Common Troubleshooting

### Model Loading Issues

```javascript
// Debug model loading
async function debugModelLoading(modelPath) {
  try {
    console.log(`Attempting to load: ${modelPath}`);
    
    // Check if file exists (Node.js only)
    if (typeof process !== 'undefined') {
      const fs = await import('fs/promises');
      await fs.access(modelPath);
      console.log('‚úÖ File exists');
    }
    
    const model = await router.load(modelPath);
    console.log('‚úÖ Model loaded successfully');
    console.log(`Model info:`, {
      id: model.id,
      format: model.format,
      size: model.size
    });
    
  } catch (error) {
    console.log('‚ùå Loading failed:', error.message);
    console.log('Available loaders:', router.getAvailableLoaders());
  }
}
```

### Performance Monitoring

```javascript
// Monitor performance
setInterval(() => {
  const status = router.getStatus();
  console.log('Status:', {
    modelsLoaded: status.modelsLoaded,
    memoryUsage: status.memoryUsage,
    engine: status.engine
  });
}, 10000); // Every 10 seconds
```

This completes the basic usage examples. These examples provide a solid foundation for getting started with the LLM-Runner-Router system.