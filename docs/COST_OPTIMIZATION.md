# ðŸ’° LLM-Runner-Router Cost Optimization Guide

*Master the economics of AI model orchestration for maximum value and minimum cost*

## ðŸ“– Table of Contents

- [Cost Model Overview](#cost-model-overview)
- [Model Selection Economics](#model-selection-economics)
- [Usage Optimization Strategies](#usage-optimization-strategies)
- [Caching for Cost Reduction](#caching-for-cost-reduction)
- [Request Batching and Queuing](#request-batching-and-queuing)
- [Model Lifecycle Cost Management](#model-lifecycle-cost-management)
- [Resource Pooling and Sharing](#resource-pooling-and-sharing)
- [Performance vs Cost Trade-offs](#performance-vs-cost-trade-offs)
- [Cost Monitoring and Analytics](#cost-monitoring-and-analytics)
- [Budget Management and Alerts](#budget-management-and-alerts)
- [Provider-Specific Optimizations](#provider-specific-optimizations)
- [Cost-Effective Scaling](#cost-effective-scaling)

## ðŸ’¡ Cost Model Overview

### Understanding LLM Costs

```javascript
class CostModel {
  constructor() {
    this.costFactors = {
      // API-based models (per token)
      'gpt-4-turbo': {
        input: 0.00001,     // $0.01 per 1K tokens
        output: 0.00003,    // $0.03 per 1K tokens
        context: 0.00001    // Context window cost
      },
      'gpt-3.5-turbo': {
        input: 0.0000005,   // $0.0005 per 1K tokens
        output: 0.0000015,  // $0.0015 per 1K tokens
        context: 0.0000005
      },
      'claude-3-opus': {
        input: 0.000015,    // $0.015 per 1K tokens
        output: 0.000075,   // $0.075 per 1K tokens
        context: 0.000015
      },
      
      // Self-hosted models (compute cost)
      'llama-2-70b': {
        compute: 0.0004,    // Per inference (GPU time)
        memory: 0.00001,    // Per MB-hour
        storage: 0.000001   // Per GB-hour
      },
      'mistral-7b': {
        compute: 0.0001,
        memory: 0.000005,
        storage: 0.000001
      }
    };
    
    this.infraCosts = {
      gpu_h100: 2.50,      // Per hour
      gpu_a100: 1.80,      // Per hour
      gpu_v100: 0.90,      // Per hour
      cpu_large: 0.20,     // Per hour
      storage: 0.023,      // Per GB per month
      bandwidth: 0.09      // Per GB
    };
  }
  
  calculateTokenCost(model, inputTokens, outputTokens) {
    const pricing = this.costFactors[model];
    if (!pricing.input) return null; // Not token-based
    
    return (inputTokens * pricing.input) + (outputTokens * pricing.output);
  }
  
  calculateComputeCost(model, inferenceTime, memoryUsage) {
    const pricing = this.costFactors[model];
    if (!pricing.compute) return null; // Not compute-based
    
    return pricing.compute + (memoryUsage * pricing.memory * (inferenceTime / 3600000));
  }
  
  projectMonthlyCost(usage) {
    let totalCost = 0;
    
    for (const [model, stats] of Object.entries(usage)) {
      const modelCost = this.calculateModelCost(model, stats);
      totalCost += modelCost;
    }
    
    return totalCost;
  }
}
```

### Cost Tracking System

```javascript
class CostTracker {
  constructor() {
    this.usage = new Map();
    this.budgets = new Map();
    this.alerts = new Map();
    this.costHistory = [];
  }
  
  recordUsage(userId, model, tokens, inferenceTime, quality) {
    const cost = this.calculateCost(model, tokens, inferenceTime);
    const timestamp = Date.now();
    
    // Update user usage
    const userUsage = this.usage.get(userId) || {
      totalCost: 0,
      modelUsage: new Map(),
      history: []
    };
    
    userUsage.totalCost += cost;
    
    const modelUsage = userUsage.modelUsage.get(model) || {
      requests: 0,
      totalCost: 0,
      totalTokens: tokens.input + tokens.output,
      avgQuality: 0
    };
    
    modelUsage.requests++;
    modelUsage.totalCost += cost;
    modelUsage.totalTokens += tokens.input + tokens.output;
    modelUsage.avgQuality = (modelUsage.avgQuality + quality) / 2;
    
    userUsage.modelUsage.set(model, modelUsage);
    userUsage.history.push({
      timestamp,
      model,
      cost,
      tokens,
      quality,
      efficiency: quality / cost // Quality per dollar
    });
    
    this.usage.set(userId, userUsage);
    
    // Check budget limits
    this.checkBudgetLimits(userId);
  }
  
  checkBudgetLimits(userId) {
    const usage = this.usage.get(userId);
    const budget = this.budgets.get(userId);
    
    if (!budget || !usage) return;
    
    const utilizationRate = usage.totalCost / budget.limit;
    
    if (utilizationRate >= 0.9 && !this.alerts.has(`${userId}-90`)) {
      this.sendAlert(userId, 'budget-90', {
        usage: usage.totalCost,
        budget: budget.limit,
        remaining: budget.limit - usage.totalCost
      });
      this.alerts.set(`${userId}-90`, Date.now());
    }
    
    if (utilizationRate >= 1.0 && !this.alerts.has(`${userId}-exceeded`)) {
      this.sendAlert(userId, 'budget-exceeded', {
        usage: usage.totalCost,
        budget: budget.limit,
        overage: usage.totalCost - budget.limit
      });
      this.enforcebudgetLimit(userId);
    }
  }
}
```

## ðŸŽ¯ Model Selection Economics

### Intelligent Model Selection

```javascript
class EconomicModelSelector {
  constructor(costModel) {
    this.costModel = costModel;
    this.performanceCache = new Map();
    this.costEfficiencyRankings = new Map();
  }
  
  async selectOptimalModel(request) {
    const { 
      prompt, 
      qualityThreshold = 0.8, 
      budgetLimit = null,
      latencyRequirement = null,
      userTier = 'standard'
    } = request;
    
    // Get candidate models based on user tier
    const candidates = this.getCandidateModels(userTier);
    
    // Score each model on multiple dimensions
    const scoredModels = await Promise.all(
      candidates.map(model => this.scoreModel(model, request))
    );
    
    // Filter by constraints
    const viableModels = scoredModels.filter(scored => {
      if (scored.quality < qualityThreshold) return false;
      if (budgetLimit && scored.estimatedCost > budgetLimit) return false;
      if (latencyRequirement && scored.estimatedLatency > latencyRequirement) return false;
      return true;
    });
    
    if (viableModels.length === 0) {
      throw new Error('No models meet the specified constraints');
    }
    
    // Sort by cost-effectiveness (quality per dollar)
    viableModels.sort((a, b) => b.costEffectiveness - a.costEffectiveness);
    
    return {
      primary: viableModels[0],
      alternatives: viableModels.slice(1, 3),
      reasoning: this.explainSelection(viableModels[0], request)
    };
  }
  
  async scoreModel(model, request) {
    const { prompt, context = {} } = request;
    
    // Estimate performance metrics
    const performance = await this.estimatePerformance(model, prompt);
    const cost = await this.estimateCost(model, performance);
    
    // Calculate cost-effectiveness
    const costEffectiveness = performance.quality / cost.total;
    
    return {
      model,
      quality: performance.quality,
      estimatedLatency: performance.latency,
      estimatedCost: cost.total,
      costEffectiveness,
      breakdown: {
        performance,
        cost
      }
    };
  }
  
  async estimatePerformance(model, prompt) {
    // Use historical data and prompt analysis
    const promptComplexity = this.analyzePromptComplexity(prompt);
    const historicalData = this.getHistoricalPerformance(model);
    
    // Adjust for prompt complexity
    const quality = historicalData.avgQuality * this.getComplexityMultiplier(promptComplexity);
    const latency = historicalData.avgLatency * this.getLatencyMultiplier(promptComplexity);
    const tokens = this.estimateTokens(prompt, model);
    
    return {
      quality: Math.min(quality, 1.0),
      latency,
      estimatedTokens: tokens
    };
  }
  
  async estimateCost(model, performance) {
    const pricing = this.costModel.costFactors[model];
    
    if (pricing.input) {
      // Token-based pricing
      return {
        input: performance.estimatedTokens.input * pricing.input,
        output: performance.estimatedTokens.output * pricing.output,
        total: (performance.estimatedTokens.input * pricing.input) + 
               (performance.estimatedTokens.output * pricing.output)
      };
    } else {
      // Compute-based pricing
      const computeTime = performance.latency / 1000; // Convert to seconds
      return {
        compute: computeTime * pricing.compute,
        memory: computeTime * pricing.memory,
        total: computeTime * (pricing.compute + pricing.memory)
      };
    }
  }
  
  getCandidateModels(userTier) {
    const modelTiers = {
      'free': ['llama-2-7b', 'mistral-7b'],
      'standard': ['gpt-3.5-turbo', 'llama-2-7b', 'mistral-7b', 'claude-3-haiku'],
      'premium': ['gpt-4-turbo', 'claude-3-opus', 'llama-2-70b'],
      'enterprise': ['gpt-4-turbo', 'claude-3-opus', 'llama-2-70b', 'custom-models']
    };
    
    return modelTiers[userTier] || modelTiers['standard'];
  }
}
```

### Dynamic Pricing Strategy

```javascript
class DynamicPricingOptimizer {
  constructor() {
    this.demandPatterns = new Map();
    this.priceElasticity = new Map();
    this.competitorPricing = new Map();
  }
  
  async optimizeModelPricing(timeWindow = '1h') {
    const currentDemand = await this.analyzeDemand(timeWindow);
    const capacity = await this.getAvailableCapacity();
    
    const optimizedPricing = new Map();
    
    for (const [model, demand] of currentDemand) {
      const basePrice = this.getBasePrice(model);
      const capacityUtilization = demand.requests / capacity[model];
      
      // Demand-based pricing
      let priceMultiplier = 1.0;
      
      if (capacityUtilization > 0.8) {
        // High utilization - increase price to reduce demand
        priceMultiplier = 1.2 + (capacityUtilization - 0.8) * 2;
      } else if (capacityUtilization < 0.3) {
        // Low utilization - decrease price to increase demand
        priceMultiplier = 0.8 - (0.3 - capacityUtilization) * 0.5;
      }
      
      // Quality-based pricing
      const qualityMultiplier = this.getQualityMultiplier(model);
      
      // Final optimized price
      const optimizedPrice = basePrice * priceMultiplier * qualityMultiplier;
      
      optimizedPricing.set(model, {
        basePrice,
        optimizedPrice,
        priceMultiplier,
        qualityMultiplier,
        reasoning: {
          capacityUtilization,
          demandTrend: demand.trend,
          competitorComparison: this.compareToCompetitors(model, optimizedPrice)
        }
      });
    }
    
    return optimizedPricing;
  }
  
  getQualityMultiplier(model) {
    const qualityBenchmarks = {
      'gpt-4-turbo': 1.2,    // Premium quality
      'claude-3-opus': 1.15,
      'gpt-3.5-turbo': 1.0,  // Baseline
      'llama-2-70b': 0.9,
      'llama-2-7b': 0.7     // Economy option
    };
    
    return qualityBenchmarks[model] || 1.0;
  }
}
```

## ðŸš€ Usage Optimization Strategies

### Request Optimization

```javascript
class RequestOptimizer {
  constructor() {
    this.optimizationStrategies = [
      this.promptOptimization,
      this.contextOptimization,
      this.responseOptimization,
      this.batchingOptimization
    ];
  }
  
  async optimizeRequest(request) {
    let optimizedRequest = { ...request };
    let savings = { tokens: 0, cost: 0 };
    
    for (const strategy of this.optimizationStrategies) {
      const result = await strategy(optimizedRequest);
      optimizedRequest = result.request;
      savings.tokens += result.savings.tokens;
      savings.cost += result.savings.cost;
    }
    
    return {
      originalRequest: request,
      optimizedRequest,
      savings,
      optimizations: this.explainOptimizations(request, optimizedRequest)
    };
  }
  
  async promptOptimization(request) {
    const { prompt } = request;
    let optimizedPrompt = prompt;
    let tokensSaved = 0;
    
    // Remove redundant phrases
    const redundancies = [
      /please\s+/gi,
      /kindly\s+/gi,
      /\s+and\s+also\s+/gi
    ];
    
    for (const pattern of redundancies) {
      const original = optimizedPrompt;
      optimizedPrompt = optimizedPrompt.replace(pattern, ' ');
      tokensSaved += this.estimateTokenDifference(original, optimizedPrompt);
    }
    
    // Compress verbose instructions
    optimizedPrompt = this.compressInstructions(optimizedPrompt);
    
    // Use more efficient phrasing
    optimizedPrompt = this.improveEfficiency(optimizedPrompt);
    
    const costSavings = tokensSaved * this.getAverageTokenCost(request.model);
    
    return {
      request: { ...request, prompt: optimizedPrompt },
      savings: { tokens: tokensSaved, cost: costSavings }
    };
  }
  
  async contextOptimization(request) {
    let { context = [], maxContextLength = 4000 } = request;
    let tokensSaved = 0;
    
    if (!context.length) return { request, savings: { tokens: 0, cost: 0 } };
    
    // Prioritize context by relevance
    const relevanceScored = context.map(item => ({
      ...item,
      relevanceScore: this.calculateRelevance(item, request.prompt)
    }));
    
    relevanceScored.sort((a, b) => b.relevanceScore - a.relevanceScore);
    
    // Trim context to fit within limits while maintaining quality
    let optimizedContext = [];
    let currentTokens = 0;
    
    for (const item of relevanceScored) {
      const itemTokens = this.estimateTokens(item.content);
      
      if (currentTokens + itemTokens <= maxContextLength) {
        optimizedContext.push(item);
        currentTokens += itemTokens;
      } else {
        tokensSaved += itemTokens;
      }
    }
    
    const costSavings = tokensSaved * this.getAverageTokenCost(request.model);
    
    return {
      request: { ...request, context: optimizedContext },
      savings: { tokens: tokensSaved, cost: costSavings }
    };
  }
  
  compressInstructions(prompt) {
    const compressionRules = [
      { from: 'Please provide a detailed explanation of', to: 'Explain' },
      { from: 'I would like you to', to: '' },
      { from: 'Can you please tell me', to: 'What is' },
      { from: 'It would be helpful if you could', to: 'Please' }
    ];
    
    let compressed = prompt;
    for (const rule of compressionRules) {
      compressed = compressed.replace(new RegExp(rule.from, 'gi'), rule.to);
    }
    
    return compressed.trim();
  }
}
```

### Smart Preprocessing

```javascript
class SmartPreprocessor {
  constructor() {
    this.preprocessingRules = new Map();
    this.templateCache = new Map();
  }
  
  async preprocessRequest(request) {
    const { prompt, model, context } = request;
    
    // Template detection and reuse
    const template = this.detectTemplate(prompt);
    if (template) {
      return this.optimizeWithTemplate(request, template);
    }
    
    // Content compression
    const compressed = await this.compressContent(prompt, context);
    
    // Redundancy elimination
    const deduplicated = this.eliminateRedundancy(compressed);
    
    // Model-specific optimizations
    const modelOptimized = this.applyModelSpecificOptimizations(deduplicated, model);
    
    return {
      ...request,
      prompt: modelOptimized.prompt,
      context: modelOptimized.context,
      preprocessing: {
        template,
        compressionRatio: compressed.ratio,
        deduplicationSavings: deduplicated.savings,
        modelOptimizations: modelOptimized.optimizations
      }
    };
  }
  
  detectTemplate(prompt) {
    // Analyze prompt for common patterns
    const patterns = [
      {
        name: 'summarization',
        pattern: /summarize|summary|tldr/i,
        template: 'Summarize: {content}'
      },
      {
        name: 'translation',
        pattern: /translate|translation/i,
        template: 'Translate {source} to {target}: {content}'
      },
      {
        name: 'qna',
        pattern: /question|answer|what is|how to/i,
        template: 'Q: {question}\nA:'
      }
    ];
    
    for (const pattern of patterns) {
      if (pattern.pattern.test(prompt)) {
        return pattern;
      }
    }
    
    return null;
  }
  
  optimizeWithTemplate(request, template) {
    const cachedResult = this.templateCache.get(template.name);
    if (cachedResult && this.isSimilarRequest(request, cachedResult.request)) {
      // Reuse cached preprocessing
      return {
        ...request,
        prompt: this.applyTemplate(template.template, request),
        fromCache: true,
        savings: cachedResult.savings
      };
    }
    
    const optimized = this.applyTemplate(template.template, request);
    this.templateCache.set(template.name, { request, optimized });
    
    return {
      ...request,
      prompt: optimized,
      template: template.name
    };
  }
  
  async compressContent(prompt, context) {
    // Semantic compression - remove low-value words
    const stopWords = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'];
    
    let compressed = prompt;
    let originalLength = prompt.length;
    
    // Remove stop words where grammatically safe
    for (const word of stopWords) {
      const pattern = new RegExp(`\\b${word}\\b(?=\\s+\\w)`, 'gi');
      const beforeLength = compressed.length;
      compressed = compressed.replace(pattern, '');
      
      // Revert if compression is too aggressive
      if (compressed.length < beforeLength * 0.7) {
        compressed = prompt; // Revert
        break;
      }
    }
    
    // Context compression
    let compressedContext = context;
    if (context && context.length) {
      compressedContext = context.map(item => ({
        ...item,
        content: this.compressText(item.content, 0.8) // 80% target
      }));
    }
    
    return {
      prompt: compressed,
      context: compressedContext,
      ratio: compressed.length / originalLength
    };
  }
}
```

## ðŸ’¾ Caching for Cost Reduction

### Intelligent Caching System

```javascript
class CostAwareCacheManager {
  constructor() {
    this.cache = new Map();
    this.costMetrics = new Map();
    this.hitRates = new Map();
    this.cachePolicies = new Map();
  }
  
  async get(request) {
    const cacheKey = this.generateCacheKey(request);
    const cached = this.cache.get(cacheKey);
    
    if (cached) {
      await this.recordCacheHit(cacheKey, request);
      return cached;
    }
    
    return null;
  }
  
  async set(request, response, cost) {
    const cacheKey = this.generateCacheKey(request);
    const policy = this.determineCachePolicy(request, response, cost);
    
    if (!policy.shouldCache) {
      return;
    }
    
    const cacheEntry = {
      request,
      response,
      cost,
      timestamp: Date.now(),
      hits: 0,
      lastAccessed: Date.now(),
      ttl: policy.ttl,
      priority: policy.priority
    };
    
    this.cache.set(cacheKey, cacheEntry);
    this.costMetrics.set(cacheKey, { originalCost: cost, savings: 0 });
    
    // Enforce cache limits
    await this.enforceLimit();
  }
  
  determineCachePolicy(request, response, cost) {
    const { model, prompt } = request;
    
    // High-cost responses are prioritized for caching
    const costPriority = this.calculateCostPriority(cost);
    
    // Common prompts get longer TTL
    const commonnessPriority = this.calculateCommonnessPriority(prompt);
    
    // High-quality responses are prioritized
    const qualityPriority = this.calculateQualityPriority(response);
    
    const totalPriority = (costPriority + commonnessPriority + qualityPriority) / 3;
    
    return {
      shouldCache: totalPriority > 0.5,
      ttl: this.calculateTTL(totalPriority, cost),
      priority: totalPriority
    };
  }
  
  calculateCostPriority(cost) {
    // Higher cost = higher priority for caching
    if (cost > 0.10) return 1.0;     // Very expensive
    if (cost > 0.05) return 0.8;     // Expensive
    if (cost > 0.01) return 0.6;     // Moderate
    if (cost > 0.001) return 0.4;    // Cheap
    return 0.2;                      // Very cheap
  }
  
  calculateTTL(priority, cost) {
    const baseTTL = 3600000; // 1 hour
    const costMultiplier = Math.min(cost * 1000, 5); // Max 5x multiplier
    const priorityMultiplier = priority * 2;
    
    return baseTTL * costMultiplier * priorityMultiplier;
  }
  
  async enforceLimit() {
    const maxSize = 1000; // Max cache entries
    const maxCost = 100;  // Max total cost in cache
    
    if (this.cache.size <= maxSize) return;
    
    // Get entries sorted by cost-effectiveness
    const entries = Array.from(this.cache.entries())
      .map(([key, entry]) => ({
        key,
        entry,
        costEffectiveness: this.calculateCostEffectiveness(key, entry)
      }))
      .sort((a, b) => a.costEffectiveness - b.costEffectiveness);
    
    // Remove least cost-effective entries
    const toRemove = entries.slice(0, entries.length - maxSize);
    for (const { key } of toRemove) {
      this.cache.delete(key);
      this.costMetrics.delete(key);
    }
  }
  
  calculateCostEffectiveness(key, entry) {
    const metrics = this.costMetrics.get(key);
    if (!metrics || entry.hits === 0) return 0;
    
    const totalSavings = metrics.savings;
    const ageInHours = (Date.now() - entry.timestamp) / 3600000;
    
    // Cost effectiveness = Total savings / (Original cost + Age penalty)
    return totalSavings / (entry.cost + ageInHours * 0.01);
  }
  
  async recordCacheHit(cacheKey, request) {
    const entry = this.cache.get(cacheKey);
    const metrics = this.costMetrics.get(cacheKey);
    
    if (entry && metrics) {
      entry.hits++;
      entry.lastAccessed = Date.now();
      
      // Calculate savings
      const savedCost = await this.calculateRequestCost(request);
      metrics.savings += savedCost;
      
      // Update hit rate
      const hitRate = this.hitRates.get(request.model) || { hits: 0, misses: 0 };
      hitRate.hits++;
      this.hitRates.set(request.model, hitRate);
    }
  }
  
  generateCacheKey(request) {
    const { prompt, model, context = [] } = request;
    
    // Create semantic hash that captures meaning, not exact text
    const semanticHash = this.createSemanticHash({
      prompt: this.normalizePrompt(prompt),
      model,
      contextHash: this.hashContext(context)
    });
    
    return semanticHash;
  }
  
  normalizePrompt(prompt) {
    return prompt
      .toLowerCase()
      .replace(/\s+/g, ' ')
      .replace(/[^\w\s]/g, '')
      .trim();
  }
}
```

### Cache Warming Strategies

```javascript
class CacheWarmingManager {
  constructor(cacheManager) {
    this.cacheManager = cacheManager;
    this.warmingQueue = new PriorityQueue();
    this.popularPrompts = new Map();
  }
  
  async warmCache(strategy = 'predictive') {
    switch (strategy) {
      case 'predictive':
        await this.predictiveWarming();
        break;
      case 'historical':
        await this.historicalWarming();
        break;
      case 'scheduled':
        await this.scheduledWarming();
        break;
    }
  }
  
  async predictiveWarming() {
    // Analyze usage patterns to predict what to cache
    const patterns = await this.analyzeUsagePatterns();
    
    for (const pattern of patterns) {
      if (pattern.confidence > 0.8) {
        await this.warmPromptPattern(pattern);
      }
    }
  }
  
  async analyzeUsagePatterns() {
    const recentRequests = await this.getRecentRequests('24h');
    const patterns = [];
    
    // Group by time of day
    const hourlyPatterns = this.groupByHour(recentRequests);
    
    for (const [hour, requests] of hourlyPatterns) {
      const commonPrompts = this.findCommonPrompts(requests);
      
      for (const prompt of commonPrompts) {
        patterns.push({
          hour,
          prompt: prompt.text,
          frequency: prompt.count,
          confidence: prompt.count / requests.length,
          estimatedCost: prompt.avgCost
        });
      }
    }
    
    return patterns;
  }
  
  async warmPromptPattern(pattern) {
    const request = {
      prompt: pattern.prompt,
      model: this.selectOptimalModel(pattern),
      warmingRequest: true
    };
    
    // Execute request during low-usage periods
    const response = await this.router.generate(request.prompt, {
      model: request.model,
      priority: 'low'
    });
    
    // Cache the result
    await this.cacheManager.set(request, response, pattern.estimatedCost);
    
    console.log(`Warmed cache for pattern: ${pattern.prompt.substring(0, 50)}...`);
  }
  
  selectOptimalModel(pattern) {
    // Choose the most cost-effective model for cache warming
    const models = ['gpt-3.5-turbo', 'llama-2-7b', 'mistral-7b'];
    
    // Use cheaper models for warming when quality threshold is met
    return models.find(model => {
      const cost = this.estimateModelCost(model, pattern.prompt);
      const quality = this.estimateModelQuality(model, pattern.prompt);
      
      return cost < pattern.estimatedCost * 0.8 && quality > 0.7;
    }) || models[0];
  }
}
```

## ðŸ”„ Request Batching and Queuing

### Smart Batching System

```javascript
class SmartBatchProcessor {
  constructor() {
    this.batches = new Map();
    this.batchConfigs = new Map();
    this.processingQueue = new PriorityQueue();
  }
  
  async processRequest(request) {
    const batchKey = this.determineBatchKey(request);
    const config = this.getBatchConfig(request.model);
    
    // Add to batch
    if (!this.batches.has(batchKey)) {
      this.batches.set(batchKey, {
        requests: [],
        created: Date.now(),
        model: request.model,
        estimatedCost: 0
      });
    }
    
    const batch = this.batches.get(batchKey);
    batch.requests.push(request);
    batch.estimatedCost += this.estimateRequestCost(request);
    
    // Process if batch is ready
    if (this.shouldProcessBatch(batch, config)) {
      return await this.processBatch(batchKey);
    }
    
    // Return promise that resolves when batch is processed
    return new Promise((resolve, reject) => {
      request._resolve = resolve;
      request._reject = reject;
    });
  }
  
  determineBatchKey(request) {
    const { model, priority = 'normal', category } = request;
    
    // Group similar requests for optimal batching
    const promptCategory = this.categorizePrompt(request.prompt);
    
    return `${model}-${priority}-${promptCategory}`;
  }
  
  categorizePrompt(prompt) {
    // Categorize prompts for better batching
    const categories = [
      { name: 'summarization', keywords: ['summarize', 'summary', 'tldr'] },
      { name: 'translation', keywords: ['translate', 'translation'] },
      { name: 'question', keywords: ['what', 'how', 'why', 'when', 'where'] },
      { name: 'generation', keywords: ['write', 'create', 'generate'] },
      { name: 'analysis', keywords: ['analyze', 'review', 'evaluate'] }
    ];
    
    const promptLower = prompt.toLowerCase();
    
    for (const category of categories) {
      if (category.keywords.some(keyword => promptLower.includes(keyword))) {
        return category.name;
      }
    }
    
    return 'general';
  }
  
  shouldProcessBatch(batch, config) {
    const { requests, created } = batch;
    const age = Date.now() - created;
    
    // Process if batch is full
    if (requests.length >= config.maxBatchSize) {
      return true;
    }
    
    // Process if batch is old enough
    if (age >= config.maxWaitTime) {
      return true;
    }
    
    // Process if cost threshold is met
    if (batch.estimatedCost >= config.costThreshold) {
      return true;
    }
    
    return false;
  }
  
  async processBatch(batchKey) {
    const batch = this.batches.get(batchKey);
    if (!batch) return;
    
    this.batches.delete(batchKey);
    
    try {
      // Optimize batch for model
      const optimizedBatch = this.optimizeBatch(batch);
      
      // Execute batch request
      const results = await this.executeBatch(optimizedBatch);
      
      // Resolve individual promises
      for (let i = 0; i < batch.requests.length; i++) {
        const request = batch.requests[i];
        if (request._resolve) {
          request._resolve(results[i]);
        }
      }
      
      // Record batch savings
      await this.recordBatchSavings(batch, results);
      
    } catch (error) {
      // Reject all promises in batch
      for (const request of batch.requests) {
        if (request._reject) {
          request._reject(error);
        }
      }
    }
  }
  
  optimizeBatch(batch) {
    const { requests, model } = batch;
    
    // Combine similar prompts
    const combined = this.combinePrompts(requests);
    
    // Optimize for model-specific batching
    const modelOptimized = this.applyModelOptimizations(combined, model);
    
    return {
      ...batch,
      requests: modelOptimized,
      optimizations: {
        originalCount: requests.length,
        optimizedCount: modelOptimized.length,
        compressionRatio: modelOptimized.length / requests.length
      }
    };
  }
  
  combinePrompts(requests) {
    // Group requests that can be combined
    const combinable = [];
    const individual = [];
    
    for (const request of requests) {
      if (this.canCombine(request)) {
        combinable.push(request);
      } else {
        individual.push(request);
      }
    }
    
    // Combine prompts where beneficial
    if (combinable.length > 1) {
      const combined = this.createCombinedRequest(combinable);
      return [combined, ...individual];
    }
    
    return requests;
  }
  
  getBatchConfig(model) {
    const defaultConfig = {
      maxBatchSize: 10,
      maxWaitTime: 1000,     // 1 second
      costThreshold: 0.05    // $0.05
    };
    
    const modelConfigs = {
      'gpt-4-turbo': {
        maxBatchSize: 5,      // Expensive model, smaller batches
        maxWaitTime: 500,     // Process faster
        costThreshold: 0.10
      },
      'gpt-3.5-turbo': {
        maxBatchSize: 15,     // Cheaper, larger batches
        maxWaitTime: 2000,
        costThreshold: 0.03
      },
      'llama-2-7b': {
        maxBatchSize: 20,     // Local model, large batches
        maxWaitTime: 5000,
        costThreshold: 0.01
      }
    };
    
    return { ...defaultConfig, ...modelConfigs[model] };
  }
}
```

### Queue Management for Cost Optimization

```javascript
class CostOptimizedQueue {
  constructor() {
    this.queues = new Map(); // Per-model queues
    this.globalQueue = new PriorityQueue();
    this.processingStrategies = new Map();
  }
  
  enqueue(request, priority = 'normal') {
    const cost = this.estimateRequestCost(request);
    const deadline = request.deadline || Date.now() + 300000; // 5 min default
    
    const queueItem = {
      ...request,
      priority: this.calculatePriority(request, cost, deadline),
      cost,
      deadline,
      enqueuedAt: Date.now()
    };
    
    // Add to model-specific queue
    const modelQueue = this.getModelQueue(request.model);
    modelQueue.enqueue(queueItem);
    
    // Add to global queue for cross-model optimization
    this.globalQueue.enqueue(queueItem);
    
    // Process queue if conditions are met
    this.processQueues();
    
    return queueItem;
  }
  
  calculatePriority(request, cost, deadline) {
    const { userTier = 'standard', urgency = 'normal' } = request;
    
    // Base priority by user tier
    const tierPriority = {
      'enterprise': 100,
      'premium': 80,
      'standard': 60,
      'free': 40
    }[userTier];
    
    // Urgency modifier
    const urgencyModifier = {
      'critical': 50,
      'high': 30,
      'normal': 0,
      'low': -20
    }[urgency];
    
    // Cost efficiency bonus (lower cost = higher priority for same quality)
    const costEfficiency = 1 / Math.max(cost, 0.001);
    
    // Deadline pressure
    const timeToDeadline = deadline - Date.now();
    const deadlineBonus = Math.max(0, 50 - (timeToDeadline / 60000)); // Bonus as deadline approaches
    
    return tierPriority + urgencyModifier + costEfficiency + deadlineBonus;
  }
  
  async processQueues() {
    // Cost-aware queue processing
    const availableBudget = await this.getAvailableBudget();
    const optimalSchedule = await this.optimizeSchedule(availableBudget);
    
    for (const batch of optimalSchedule) {
      await this.processBatch(batch);
    }
  }
  
  async optimizeSchedule(budget) {
    const allRequests = this.getAllQueuedRequests();
    
    // Use dynamic programming to find optimal request selection
    const schedule = this.solveKnapsackOptimization(allRequests, budget);
    
    return this.groupIntoOptimalBatches(schedule);
  }
  
  solveKnapsackOptimization(requests, budget) {
    // Modified knapsack problem: maximize value (quality * urgency) within budget
    const n = requests.length;
    const dp = Array(n + 1).fill().map(() => Array(Math.floor(budget * 100) + 1).fill(0));
    const keep = Array(n + 1).fill().map(() => Array(Math.floor(budget * 100) + 1).fill(false));
    
    for (let i = 1; i <= n; i++) {
      const request = requests[i - 1];
      const cost = Math.floor(request.cost * 100); // Convert to cents
      const value = this.calculateRequestValue(request);
      
      for (let w = 0; w <= Math.floor(budget * 100); w++) {
        if (cost <= w) {
          const includeValue = dp[i - 1][w - cost] + value;
          if (includeValue > dp[i - 1][w]) {
            dp[i][w] = includeValue;
            keep[i][w] = true;
          } else {
            dp[i][w] = dp[i - 1][w];
          }
        } else {
          dp[i][w] = dp[i - 1][w];
        }
      }
    }
    
    // Backtrack to find selected requests
    const selected = [];
    let w = Math.floor(budget * 100);
    
    for (let i = n; i > 0 && w > 0; i--) {
      if (keep[i][w]) {
        selected.push(requests[i - 1]);
        w -= Math.floor(requests[i - 1].cost * 100);
      }
    }
    
    return selected;
  }
  
  calculateRequestValue(request) {
    const urgencyValue = {
      'critical': 100,
      'high': 80,
      'normal': 60,
      'low': 40
    }[request.urgency || 'normal'];
    
    const tierValue = {
      'enterprise': 50,
      'premium': 40,
      'standard': 30,
      'free': 20
    }[request.userTier || 'standard'];
    
    const deadlineValue = Math.max(0, 100 - ((request.deadline - Date.now()) / 60000));
    
    return urgencyValue + tierValue + deadlineValue;
  }
}
```

## ðŸ“Š Cost Monitoring and Analytics

### Real-Time Cost Dashboard

```javascript
class CostAnalyticsDashboard {
  constructor() {
    this.metrics = new Map();
    this.alerts = new Map();
    this.forecasts = new Map();
    this.realTimeData = {
      currentSpend: 0,
      hourlyRate: 0,
      efficiency: 0,
      topModels: [],
      topUsers: []
    };
  }
  
  updateRealTimeMetrics() {
    setInterval(async () => {
      await this.calculateCurrentSpend();
      await this.calculateHourlyRate();
      await this.calculateEfficiency();
      await this.updateTopSpenders();
      
      this.broadcastUpdate();
    }, 10000); // Update every 10 seconds
  }
  
  async calculateCurrentSpend() {
    const now = Date.now();
    const dayStart = new Date().setHours(0, 0, 0, 0);
    
    const todaySpend = await this.getTotalSpend(dayStart, now);
    this.realTimeData.currentSpend = todaySpend;
  }
  
  async calculateHourlyRate() {
    const now = Date.now();
    const hourAgo = now - 3600000;
    
    const hourlySpend = await this.getTotalSpend(hourAgo, now);
    this.realTimeData.hourlyRate = hourlySpend;
  }
  
  async calculateEfficiency() {
    // Efficiency = Quality per dollar
    const recentRequests = await this.getRecentRequests('1h');
    
    let totalCost = 0;
    let totalQuality = 0;
    
    for (const request of recentRequests) {
      totalCost += request.cost;
      totalQuality += request.quality || 0.8; // Default quality
    }
    
    this.realTimeData.efficiency = totalQuality / Math.max(totalCost, 0.001);
  }
  
  async generateCostReport(period = '24h') {
    const report = {
      period,
      timestamp: new Date().toISOString(),
      summary: await this.getCostSummary(period),
      breakdown: await this.getCostBreakdown(period),
      trends: await this.calculateTrends(period),
      recommendations: await this.generateRecommendations(period),
      forecast: await this.generateForecast(period)
    };
    
    return report;
  }
  
  async getCostBreakdown(period) {
    const requests = await this.getRequests(period);
    
    const breakdown = {
      byModel: new Map(),
      byUser: new Map(),
      byHour: new Map(),
      byCategory: new Map()
    };
    
    for (const request of requests) {
      // By model
      const modelCost = breakdown.byModel.get(request.model) || 0;
      breakdown.byModel.set(request.model, modelCost + request.cost);
      
      // By user
      const userCost = breakdown.byUser.get(request.userId) || 0;
      breakdown.byUser.set(request.userId, userCost + request.cost);
      
      // By hour
      const hour = new Date(request.timestamp).getHours();
      const hourCost = breakdown.byHour.get(hour) || 0;
      breakdown.byHour.set(hour, hourCost + request.cost);
      
      // By category
      const category = this.categorizeRequest(request);
      const categoryCost = breakdown.byCategory.get(category) || 0;
      breakdown.byCategory.set(category, categoryCost + request.cost);
    }
    
    return this.formatBreakdown(breakdown);
  }
  
  async generateRecommendations(period) {
    const breakdown = await this.getCostBreakdown(period);
    const efficiency = await this.analyzeEfficiency(period);
    
    const recommendations = [];
    
    // Model optimization recommendations
    const inefficientModels = this.findInefficientModels(breakdown, efficiency);
    for (const model of inefficientModels) {
      recommendations.push({
        type: 'model_optimization',
        priority: 'high',
        model: model.name,
        currentCost: model.cost,
        suggestion: `Consider switching to ${model.alternative} for ${model.useCase}`,
        estimatedSavings: model.potentialSavings
      });
    }
    
    // Usage pattern recommendations
    const usagePatterns = this.analyzeUsagePatterns(breakdown);
    if (usagePatterns.peakHours.cost > usagePatterns.offPeakHours.cost * 2) {
      recommendations.push({
        type: 'usage_optimization',
        priority: 'medium',
        suggestion: 'Consider shifting non-urgent requests to off-peak hours',
        estimatedSavings: usagePatterns.peakHours.cost * 0.3
      });
    }
    
    // Caching recommendations
    const cacheAnalysis = await this.analyzeCacheEffectiveness(period);
    if (cacheAnalysis.hitRate < 0.7) {
      recommendations.push({
        type: 'caching_optimization',
        priority: 'medium',
        currentHitRate: cacheAnalysis.hitRate,
        suggestion: 'Improve cache hit rate through better prompt normalization',
        estimatedSavings: cacheAnalysis.potentialSavings
      });
    }
    
    return recommendations.sort((a, b) => {
      const priorityOrder = { high: 3, medium: 2, low: 1 };
      return priorityOrder[b.priority] - priorityOrder[a.priority];
    });
  }
  
  async generateForecast(period) {
    const historicalData = await this.getHistoricalData('30d');
    const trends = this.calculateTrends(historicalData);
    
    // Linear regression for basic forecasting
    const forecast = {
      next24h: this.forecastSpend(trends, '24h'),
      next7d: this.forecastSpend(trends, '7d'),
      next30d: this.forecastSpend(trends, '30d'),
      confidence: this.calculateForecastConfidence(trends)
    };
    
    return forecast;
  }
  
  forecastSpend(trends, period) {
    const periodHours = {
      '24h': 24,
      '7d': 168,
      '30d': 720
    }[period];
    
    // Simple linear extrapolation
    const hourlyTrend = trends.hourlyGrowthRate;
    const baseHourlySpend = trends.currentHourlySpend;
    
    return baseHourlySpend * periodHours * (1 + hourlyTrend);
  }
}
```

---

## ðŸ’¡ Additional Resources

- **[Performance Guide](./PERFORMANCE.md)** - Performance optimization strategies
- **[Memory Management](./MEMORY_MANAGEMENT.md)** - Memory optimization techniques
- **[Best Practices](./BEST_PRACTICES.md)** - General best practices
- **[Scaling Guide](./SCALING.md)** - Cost-effective scaling strategies

---

*Remember: Cost optimization is an ongoing process. Regular monitoring, analysis, and adjustment of strategies ensures maximum value from your AI investments.*

**Built with ðŸ’™ by Echo AI Systems**