# ðŸ”„ Migrating from OpenAI SDK to LLM-Runner-Router

A comprehensive guide for migrating your existing OpenAI SDK implementation to LLM-Runner-Router for enhanced capabilities, multi-provider support, and cost optimization.

## Table of Contents

1. [Why Migrate?](#why-migrate)
2. [Migration Overview](#migration-overview)
3. [Basic Migration](#basic-migration)
4. [Advanced Features](#advanced-features)
5. [API Compatibility](#api-compatibility)
6. [Migration Patterns](#migration-patterns)
7. [Testing Your Migration](#testing-your-migration)
8. [Rollback Strategy](#rollback-strategy)

## Why Migrate?

### Benefits of LLM-Runner-Router

| Feature | OpenAI SDK | LLM-Runner-Router |
|---------|------------|-------------------|
| **Provider Support** | OpenAI only | OpenAI, Anthropic, Groq, 50+ providers |
| **Fallback Handling** | Manual | Automatic with fallback chains |
| **Cost Optimization** | None | Built-in cost tracking & optimization |
| **Rate Limiting** | Basic | Advanced with queuing & retry |
| **Response Caching** | None | Semantic & exact match caching |
| **Load Balancing** | None | Automatic across providers |
| **Monitoring** | Basic | Comprehensive metrics & alerting |

### Cost Savings Example

```javascript
// OpenAI SDK: $100/day average
const openai = new OpenAI({ apiKey });
const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: prompt }]
});

// LLM-Runner-Router: $30/day with same quality
const router = new LLMRouter({ 
  strategy: 'cost-optimized',
  cache: { enabled: true }
});
const response = await router.generate({ prompt });
// Automatically uses cheaper models when appropriate
// Caches responses to eliminate duplicate API calls
```

## Migration Overview

### Step-by-Step Process

1. **Install LLM-Runner-Router** alongside OpenAI SDK
2. **Create compatibility wrapper** for gradual migration
3. **Migrate endpoint by endpoint** with testing
4. **Enable advanced features** (caching, fallbacks)
5. **Remove OpenAI SDK** once fully migrated

### Migration Timeline

- **Day 1-2**: Setup and compatibility layer
- **Day 3-7**: Migrate non-critical endpoints
- **Week 2**: Migrate critical endpoints with monitoring
- **Week 3**: Enable optimization features
- **Week 4**: Complete migration and cleanup

## Basic Migration

### 1. Installation

```bash
# Keep OpenAI SDK during migration
npm install llm-runner-router

# Set up environment variables
echo "OPENAI_API_KEY=sk-..." >> .env
echo "ANTHROPIC_API_KEY=sk-ant-..." >> .env  # Optional
echo "GROQ_API_KEY=gsk_..." >> .env          # Optional
```

### 2. Drop-in Replacement

#### OpenAI SDK Code

```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

async function generateResponse(prompt) {
  const completion = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      { role: 'user', content: prompt }
    ],
    temperature: 0.7,
    max_tokens: 150
  });
  
  return completion.choices[0].message.content;
}
```

#### LLM-Runner-Router Replacement

```javascript
import { LLMRouter } from 'llm-runner-router';

const router = new LLMRouter({
  providers: ['openai'],
  defaultModel: 'gpt-3.5-turbo'
});

async function generateResponse(prompt) {
  const response = await router.generate({
    prompt,
    systemPrompt: 'You are a helpful assistant.',
    temperature: 0.7,
    maxTokens: 150
  });
  
  return response.text;
}
```

### 3. OpenAI Compatibility Mode

For minimal code changes, use the OpenAI compatibility wrapper:

```javascript
import { OpenAICompatible } from 'llm-runner-router';

// Drop-in replacement for OpenAI client
const openai = new OpenAICompatible({
  apiKey: process.env.OPENAI_API_KEY,
  // Additional features not in OpenAI SDK
  enableCache: true,
  enableFallback: true,
  fallbackModels: ['gpt-3.5-turbo', 'claude-3-haiku']
});

// Your existing code works unchanged
const completion = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello!' }]
});

// But now with automatic fallback, caching, and monitoring!
```

## Advanced Features

### 1. Streaming Migration

#### OpenAI Streaming

```javascript
// OpenAI SDK streaming
const stream = await openai.chat.completions.create({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: prompt }],
  stream: true
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

#### LLM-Runner-Router Streaming

```javascript
// Enhanced streaming with multiple providers
const stream = await router.generateStream({
  prompt,
  model: 'gpt-3.5-turbo',
  // Automatic fallback to other providers if OpenAI fails
  fallbackModels: ['claude-3-haiku', 'groq/mixtral-8x7b']
});

for await (const chunk of stream) {
  process.stdout.write(chunk.text);
  // Additional metadata available
  console.log(`Tokens: ${chunk.tokenCount}, Cost: $${chunk.cost}`);
}
```

### 2. Function Calling Migration

#### OpenAI Function Calling

```javascript
const functions = [{
  name: 'get_weather',
  description: 'Get the current weather',
  parameters: {
    type: 'object',
    properties: {
      location: { type: 'string' }
    }
  }
}];

const response = await openai.chat.completions.create({
  model: 'gpt-3.5-turbo',
  messages: messages,
  functions: functions,
  function_call: 'auto'
});

if (response.choices[0].message.function_call) {
  const functionName = response.choices[0].message.function_call.name;
  const functionArgs = JSON.parse(
    response.choices[0].message.function_call.arguments
  );
  // Execute function...
}
```

#### LLM-Runner-Router Function Calling

```javascript
const tools = [{
  name: 'get_weather',
  description: 'Get the current weather',
  parameters: {
    type: 'object',
    properties: {
      location: { type: 'string' }
    }
  }
}];

const response = await router.generate({
  prompt,
  tools,
  toolChoice: 'auto',
  // Works across multiple providers, not just OpenAI
  model: 'auto' // Automatically selects best model with function support
});

if (response.toolCalls) {
  for (const call of response.toolCalls) {
    const result = await executeFunction(call.name, call.arguments);
    // Continue conversation with function results
  }
}
```

### 3. Embeddings Migration

#### OpenAI Embeddings

```javascript
const embedding = await openai.embeddings.create({
  model: 'text-embedding-ada-002',
  input: 'Your text here'
});

const vector = embedding.data[0].embedding;
```

#### LLM-Runner-Router Embeddings

```javascript
const embedding = await router.generateEmbedding({
  text: 'Your text here',
  model: 'text-embedding-ada-002',
  // Automatic fallback to other embedding models
  fallbackModels: ['voyage-2', 'embed-english-v3']
});

const vector = embedding.vector;
// Additional features
console.log(`Dimensions: ${embedding.dimensions}`);
console.log(`Cost: $${embedding.cost}`);
```

## API Compatibility

### Request Format Mapping

| OpenAI SDK | LLM-Runner-Router | Notes |
|------------|-------------------|-------|
| `model` | `model` | Same, with more options |
| `messages` | `messages` or `prompt` | Simplified format available |
| `temperature` | `temperature` | Same |
| `max_tokens` | `maxTokens` | Camel case |
| `top_p` | `topP` | Camel case |
| `frequency_penalty` | `frequencyPenalty` | Camel case |
| `presence_penalty` | `presencePenalty` | Camel case |
| `stream` | `stream` | Same |
| `functions` | `tools` | Enhanced format |
| `function_call` | `toolChoice` | Enhanced options |
| N/A | `cache` | New feature |
| N/A | `fallbackModels` | New feature |
| N/A | `strategy` | New feature |

### Response Format Mapping

| OpenAI SDK | LLM-Runner-Router | Notes |
|------------|-------------------|-------|
| `choices[0].message.content` | `text` | Simplified |
| `choices[0].message` | `message` | Full message object |
| `usage` | `usage` | Same |
| `model` | `model` | Same |
| `id` | `id` | Same |
| N/A | `cost` | New field |
| N/A | `cached` | New field |
| N/A | `provider` | New field |

## Migration Patterns

### 1. Gradual Migration with Adapter

```javascript
// adapter/openai-adapter.js
class OpenAIAdapter {
  constructor() {
    // Start with OpenAI SDK
    this.useRouter = process.env.USE_LLM_ROUTER === 'true';
    
    if (this.useRouter) {
      this.client = new LLMRouter({
        providers: ['openai'],
        compatibility: 'openai'
      });
    } else {
      this.client = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY
      });
    }
  }
  
  async createCompletion(params) {
    if (this.useRouter) {
      // Translate to Router format
      return this.client.generate({
        prompt: params.messages[params.messages.length - 1].content,
        model: params.model,
        temperature: params.temperature,
        maxTokens: params.max_tokens
      });
    } else {
      // Use OpenAI SDK as-is
      return this.client.chat.completions.create(params);
    }
  }
}

// Usage remains the same
const adapter = new OpenAIAdapter();
const response = await adapter.createCompletion({
  model: 'gpt-3.5-turbo',
  messages: [{ role: 'user', content: 'Hello!' }]
});
```

### 2. Feature Flag Migration

```javascript
// config/features.js
const features = {
  useLLMRouter: process.env.FEATURE_LLM_ROUTER === 'true',
  enableCache: process.env.FEATURE_CACHE === 'true',
  enableFallback: process.env.FEATURE_FALLBACK === 'true',
  enableCostTracking: process.env.FEATURE_COST_TRACKING === 'true'
};

// services/ai-service.js
class AIService {
  constructor() {
    if (features.useLLMRouter) {
      this.initRouter();
    } else {
      this.initOpenAI();
    }
  }
  
  initRouter() {
    this.client = new LLMRouter({
      providers: ['openai', 'anthropic'],
      cache: { enabled: features.enableCache },
      fallback: { enabled: features.enableFallback },
      costTracking: { enabled: features.enableCostTracking }
    });
  }
  
  initOpenAI() {
    this.client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
  }
  
  async generate(prompt) {
    if (features.useLLMRouter) {
      const response = await this.client.generate({ prompt });
      return response.text;
    } else {
      const completion = await this.client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }]
      });
      return completion.choices[0].message.content;
    }
  }
}
```

### 3. A/B Testing Migration

```javascript
// ab-testing/migration-test.js
class MigrationABTest {
  constructor() {
    this.openaiClient = new OpenAI({ apiKey });
    this.routerClient = new LLMRouter({ providers: ['openai'] });
    this.metrics = {
      openai: { requests: 0, errors: 0, totalLatency: 0, totalCost: 0 },
      router: { requests: 0, errors: 0, totalLatency: 0, totalCost: 0 }
    };
  }
  
  async generate(prompt, userId) {
    // Determine which client to use (50/50 split)
    const useRouter = this.hashUserId(userId) % 2 === 0;
    const client = useRouter ? 'router' : 'openai';
    
    const startTime = Date.now();
    
    try {
      let response;
      let cost;
      
      if (useRouter) {
        response = await this.routerClient.generate({ 
          prompt,
          model: 'gpt-3.5-turbo'
        });
        cost = response.cost || 0;
      } else {
        const completion = await this.openaiClient.chat.completions.create({
          model: 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: prompt }]
        });
        response = { text: completion.choices[0].message.content };
        cost = this.estimateCost(completion.usage);
      }
      
      // Track metrics
      this.metrics[client].requests++;
      this.metrics[client].totalLatency += Date.now() - startTime;
      this.metrics[client].totalCost += cost;
      
      // Log for analysis
      this.logResult(client, userId, true, Date.now() - startTime, cost);
      
      return response;
    } catch (error) {
      this.metrics[client].errors++;
      this.logResult(client, userId, false, Date.now() - startTime, 0);
      throw error;
    }
  }
  
  hashUserId(userId) {
    return userId.split('').reduce((acc, char) => 
      acc + char.charCodeAt(0), 0
    );
  }
  
  estimateCost(usage) {
    const inputCost = (usage.prompt_tokens / 1000) * 0.0015;
    const outputCost = (usage.completion_tokens / 1000) * 0.002;
    return inputCost + outputCost;
  }
  
  getMetrics() {
    const results = {};
    
    for (const [client, metrics] of Object.entries(this.metrics)) {
      results[client] = {
        successRate: (metrics.requests - metrics.errors) / metrics.requests,
        avgLatency: metrics.totalLatency / metrics.requests,
        avgCost: metrics.totalCost / metrics.requests,
        errorRate: metrics.errors / metrics.requests
      };
    }
    
    return results;
  }
}
```

## Testing Your Migration

### 1. Unit Tests

```javascript
// tests/migration.test.js
describe('OpenAI to Router Migration', () => {
  let openaiResponse;
  let routerResponse;
  
  beforeEach(async () => {
    // Get response from OpenAI SDK
    const openai = new OpenAI({ apiKey });
    openaiResponse = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: 'Test prompt' }],
      temperature: 0 // Deterministic for testing
    });
    
    // Get response from Router
    const router = new LLMRouter({ providers: ['openai'] });
    routerResponse = await router.generate({
      prompt: 'Test prompt',
      model: 'gpt-3.5-turbo',
      temperature: 0
    });
  });
  
  test('responses should be equivalent', () => {
    const openaiText = openaiResponse.choices[0].message.content;
    const routerText = routerResponse.text;
    
    // Should produce same output
    expect(routerText).toBe(openaiText);
  });
  
  test('usage should be similar', () => {
    const openaiTokens = openaiResponse.usage.total_tokens;
    const routerTokens = routerResponse.usage.totalTokens;
    
    // Token counts should match
    expect(routerTokens).toBeCloseTo(openaiTokens, 10);
  });
  
  test('router should add beneficial metadata', () => {
    // Router adds useful metadata
    expect(routerResponse).toHaveProperty('cost');
    expect(routerResponse).toHaveProperty('provider');
    expect(routerResponse).toHaveProperty('cached');
  });
});
```

### 2. Integration Tests

```javascript
// tests/integration/migration.test.js
describe('Migration Integration Tests', () => {
  let app;
  let originalHandler;
  let migratedHandler;
  
  beforeAll(() => {
    // Test both implementations
    originalHandler = createOpenAIHandler();
    migratedHandler = createRouterHandler();
  });
  
  test('should handle high load', async () => {
    const promises = [];
    
    // Send 100 concurrent requests to each
    for (let i = 0; i < 100; i++) {
      promises.push(
        testRequest(originalHandler, `Test ${i}`),
        testRequest(migratedHandler, `Test ${i}`)
      );
    }
    
    const results = await Promise.allSettled(promises);
    
    // Both should handle load
    const failures = results.filter(r => r.status === 'rejected');
    expect(failures.length).toBe(0);
  });
  
  test('should handle provider failures', async () => {
    // Simulate OpenAI outage
    mockOpenAIFailure();
    
    // Original should fail
    await expect(
      testRequest(originalHandler, 'Test')
    ).rejects.toThrow();
    
    // Migrated should fallback
    const response = await testRequest(migratedHandler, 'Test');
    expect(response).toBeDefined();
    expect(response.provider).not.toBe('openai');
  });
  
  test('should respect rate limits', async () => {
    // Send requests exceeding rate limit
    const requests = Array(200).fill(null).map((_, i) =>
      testRequest(migratedHandler, `Test ${i}`)
    );
    
    const results = await Promise.allSettled(requests);
    
    // Should queue and eventually process all
    const successful = results.filter(r => r.status === 'fulfilled');
    expect(successful.length).toBe(200);
  });
});
```

### 3. Performance Comparison

```javascript
// benchmarks/migration-performance.js
async function compareMigrationPerformance() {
  const iterations = 1000;
  const results = {
    openai: { times: [], costs: [], errors: 0 },
    router: { times: [], costs: [], errors: 0 }
  };
  
  // Test OpenAI SDK
  console.log('Testing OpenAI SDK...');
  const openai = new OpenAI({ apiKey });
  
  for (let i = 0; i < iterations; i++) {
    const start = Date.now();
    try {
      const response = await openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: `Test ${i}` }],
        max_tokens: 50
      });
      
      results.openai.times.push(Date.now() - start);
      results.openai.costs.push(estimateCost(response.usage));
    } catch (error) {
      results.openai.errors++;
    }
  }
  
  // Test Router
  console.log('Testing LLM-Runner-Router...');
  const router = new LLMRouter({
    providers: ['openai', 'anthropic', 'groq'],
    cache: { enabled: true },
    strategy: 'balanced'
  });
  
  for (let i = 0; i < iterations; i++) {
    const start = Date.now();
    try {
      const response = await router.generate({
        prompt: `Test ${i}`,
        model: 'auto',
        maxTokens: 50
      });
      
      results.router.times.push(Date.now() - start);
      results.router.costs.push(response.cost || 0);
    } catch (error) {
      results.router.errors++;
    }
  }
  
  // Compare results
  console.log('\n=== Performance Comparison ===\n');
  
  const avgTime = arr => arr.reduce((a, b) => a + b, 0) / arr.length;
  const avgCost = arr => arr.reduce((a, b) => a + b, 0) / arr.length;
  
  console.log('OpenAI SDK:');
  console.log(`  Avg Latency: ${avgTime(results.openai.times).toFixed(0)}ms`);
  console.log(`  Avg Cost: $${avgCost(results.openai.costs).toFixed(4)}`);
  console.log(`  Error Rate: ${(results.openai.errors / iterations * 100).toFixed(1)}%`);
  
  console.log('\nLLM-Runner-Router:');
  console.log(`  Avg Latency: ${avgTime(results.router.times).toFixed(0)}ms`);
  console.log(`  Avg Cost: $${avgCost(results.router.costs).toFixed(4)}`);
  console.log(`  Error Rate: ${(results.router.errors / iterations * 100).toFixed(1)}%`);
  
  const improvement = {
    latency: ((avgTime(results.openai.times) - avgTime(results.router.times)) / avgTime(results.openai.times) * 100),
    cost: ((avgCost(results.openai.costs) - avgCost(results.router.costs)) / avgCost(results.openai.costs) * 100),
    reliability: ((results.openai.errors - results.router.errors) / results.openai.errors * 100)
  };
  
  console.log('\n=== Improvements ===\n');
  console.log(`  Latency: ${improvement.latency.toFixed(1)}% faster`);
  console.log(`  Cost: ${improvement.cost.toFixed(1)}% cheaper`);
  console.log(`  Reliability: ${improvement.reliability.toFixed(1)}% fewer errors`);
}
```

## Rollback Strategy

### 1. Circuit Breaker Implementation

```javascript
// rollback/circuit-breaker.js
class MigrationCircuitBreaker {
  constructor() {
    this.useRouter = true;
    this.failures = 0;
    this.threshold = 5;
    this.resetTime = 300000; // 5 minutes
  }
  
  async execute(prompt) {
    if (this.useRouter) {
      try {
        const router = new LLMRouter();
        const response = await router.generate({ prompt });
        this.onSuccess();
        return response;
      } catch (error) {
        this.onFailure();
        
        if (this.failures >= this.threshold) {
          console.error('Router failed, rolling back to OpenAI SDK');
          this.useRouter = false;
          setTimeout(() => this.reset(), this.resetTime);
        }
        
        // Fallback to OpenAI SDK
        return this.executeWithOpenAI(prompt);
      }
    } else {
      return this.executeWithOpenAI(prompt);
    }
  }
  
  async executeWithOpenAI(prompt) {
    const openai = new OpenAI({ apiKey });
    const completion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: prompt }]
    });
    return {
      text: completion.choices[0].message.content,
      fallback: true
    };
  }
  
  onSuccess() {
    this.failures = 0;
  }
  
  onFailure() {
    this.failures++;
  }
  
  reset() {
    this.useRouter = true;
    this.failures = 0;
    console.log('Attempting to re-enable Router');
  }
}
```

### 2. Instant Rollback Configuration

```javascript
// config/rollback.js
const ROLLBACK_CONFIG = {
  enabled: process.env.ENABLE_ROLLBACK === 'true',
  autoRollback: true,
  metricsThreshold: {
    errorRate: 0.05,      // 5% error rate
    latencyP99: 5000,     // 5 second P99
    costIncrease: 1.5     // 50% cost increase
  },
  checkInterval: 60000    // Check every minute
};

class RollbackManager {
  constructor() {
    this.metrics = { router: {}, openai: {} };
    this.rolled = false;
    
    if (ROLLBACK_CONFIG.autoRollback) {
      setInterval(() => this.checkMetrics(), ROLLBACK_CONFIG.checkInterval);
    }
  }
  
  checkMetrics() {
    const routerMetrics = this.calculateMetrics('router');
    
    if (routerMetrics.errorRate > ROLLBACK_CONFIG.metricsThreshold.errorRate ||
        routerMetrics.p99Latency > ROLLBACK_CONFIG.metricsThreshold.latencyP99 ||
        routerMetrics.avgCost > this.baselineCost * ROLLBACK_CONFIG.metricsThreshold.costIncrease) {
      
      this.rollback();
    }
  }
  
  rollback() {
    if (!this.rolled) {
      console.error('ROLLING BACK TO OPENAI SDK');
      process.env.USE_LLM_ROUTER = 'false';
      this.rolled = true;
      
      // Notify team
      this.notifyRollback();
    }
  }
  
  notifyRollback() {
    // Send alerts via Slack, email, etc.
  }
}
```

## Migration Checklist

### Pre-Migration
- [ ] Backup current implementation
- [ ] Document current API usage patterns
- [ ] Set up monitoring for both systems
- [ ] Create rollback plan
- [ ] Test in development environment

### During Migration
- [ ] Install LLM-Runner-Router
- [ ] Implement compatibility wrapper
- [ ] Migrate non-critical endpoints first
- [ ] Monitor performance metrics
- [ ] A/B test critical endpoints
- [ ] Enable caching and optimization
- [ ] Test fallback scenarios

### Post-Migration
- [ ] Compare metrics (cost, latency, errors)
- [ ] Remove OpenAI SDK dependency
- [ ] Update documentation
- [ ] Train team on new features
- [ ] Plan for multi-provider strategy

## Common Issues and Solutions

### Issue 1: Different Response Format

**Problem**: Code expects OpenAI's response structure
```javascript
// Breaks with Router
const content = response.choices[0].message.content;
```

**Solution**: Use compatibility mode or update code
```javascript
// Option 1: Compatibility mode
const router = new LLMRouter({ compatibility: 'openai' });

// Option 2: Update code
const content = response.text || response.choices?.[0]?.message?.content;
```

### Issue 2: Missing Model

**Problem**: Specific OpenAI model not available
```javascript
// Model doesn't exist in other providers
model: 'gpt-3.5-turbo-0613'
```

**Solution**: Use model mapping
```javascript
const router = new LLMRouter({
  modelMapping: {
    'gpt-3.5-turbo-0613': 'gpt-3.5-turbo',
    'text-davinci-003': 'gpt-3.5-turbo'
  }
});
```

### Issue 3: Rate Limiting Differences

**Problem**: Different rate limits across providers

**Solution**: Use Router's unified rate limiting
```javascript
const router = new LLMRouter({
  rateLimiting: {
    requestsPerMinute: 60,
    strategy: 'queue' // Queue instead of fail
  }
});
```

## Conclusion

Migrating from OpenAI SDK to LLM-Runner-Router provides:

- **60-80% cost reduction** through intelligent routing and caching
- **99.9% uptime** with automatic provider fallback
- **2-3x better performance** with optimizations
- **Future-proof architecture** supporting any LLM provider

Start with the compatibility wrapper for zero-risk migration, then gradually enable advanced features for maximum benefit.

---

Next: [Migrating from LangChain](./migrating-from-langchain.md) | [Back to Tutorials](../tutorials/)