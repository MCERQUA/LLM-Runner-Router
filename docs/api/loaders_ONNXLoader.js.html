<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/ONNXLoader.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/ONNXLoader.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * ONNX Model Loader
 * Supports loading and running ONNX format models
 * Uses ONNX Runtime Web for browser/Node.js compatibility
 */

import { BaseLoader } from './BaseLoader.js';
import { Logger } from '../utils/Logger.js';

class ONNXLoader extends BaseLoader {
  constructor() {
    super();
    this.logger = new Logger('ONNXLoader');
    this.sessions = new Map();
    this.ort = null;
  }

  /**
   * Initialize ONNX Runtime
   */
  async initialize() {
    try {
      // Dynamically import ONNX Runtime based on environment
      if (typeof window !== 'undefined') {
        // Browser environment
        this.ort = await import('onnxruntime-web');
      } else {
        // Node.js environment
        this.ort = await import('onnxruntime-node');
      }
      
      this.logger.info('ONNX Runtime initialized');
      return true;
    } catch (error) {
      this.logger.warn('ONNX Runtime not available, using fallback', error);
      return false;
    }
  }

  /**
   * Check if this loader supports the given source
   */
  supports(source) {
    if (typeof source !== 'string') return false;
    
    // Check file extensions
    if (source.endsWith('.onnx')) return true;
    if (source.endsWith('.ort')) return true;
    
    // Check for ONNX in path
    if (source.includes('onnx')) return true;
    
    return false;
  }

  /**
   * Load an ONNX model
   */
  async load(config) {
    const modelId = config.id || `onnx-${Date.now()}`;
    
    try {
      // Initialize ONNX Runtime if not already done
      if (!this.ort) {
        const initialized = await this.initialize();
        if (!initialized) {
          throw new Error('Failed to initialize ONNX Runtime');
        }
      }

      this.logger.info(`Loading ONNX model: ${config.source}`);
      
      // Create inference session
      let session;
      
      if (config.source.startsWith('http')) {
        // Load from URL
        session = await this.ort.InferenceSession.create(config.source, {
          executionProviders: this.getExecutionProviders(),
          graphOptimizationLevel: 'all',
          ...config.sessionOptions
        });
      } else if (typeof config.source === 'string') {
        // Load from file path
        session = await this.ort.InferenceSession.create(config.source, {
          executionProviders: this.getExecutionProviders(),
          graphOptimizationLevel: 'all',
          ...config.sessionOptions
        });
      } else if (config.source instanceof ArrayBuffer) {
        // Load from ArrayBuffer
        session = await this.ort.InferenceSession.create(config.source, {
          executionProviders: this.getExecutionProviders(),
          graphOptimizationLevel: 'all',
          ...config.sessionOptions
        });
      } else {
        throw new Error('Invalid source type for ONNX model');
      }

      // Store session
      this.sessions.set(modelId, session);

      // Get model metadata
      const metadata = this.extractMetadata(session);

      this.logger.info(`ONNX model loaded successfully: ${modelId}`);
      
      return {
        id: modelId,
        name: config.name || 'ONNX Model',
        format: 'onnx',
        loaded: true,
        session,
        metadata,
        predict: (input) => this.predict(modelId, input),
        stream: (input) => this.stream(modelId, input),
        unload: () => this.unload(modelId)
      };
    } catch (error) {
      this.logger.error(`Failed to load ONNX model: ${error.message}`);
      throw error;
    }
  }

  /**
   * Get execution providers based on environment
   */
  getExecutionProviders() {
    const providers = [];
    
    if (typeof window !== 'undefined') {
      // Browser environment
      providers.push('webgl'); // WebGL backend for GPU acceleration
      providers.push('wasm'); // WebAssembly fallback
    } else {
      // Node.js environment
      providers.push('cpu'); // CPU backend
    }
    
    return providers;
  }

  /**
   * Extract metadata from ONNX session
   */
  extractMetadata(session) {
    const metadata = {
      inputNames: session.inputNames,
      outputNames: session.outputNames,
      inputs: {},
      outputs: {}
    };

    // Get input shapes and types
    for (const name of session.inputNames) {
      const input = session.inputs.find(i => i.name === name);
      if (input) {
        metadata.inputs[name] = {
          shape: input.dims,
          type: input.type
        };
      }
    }

    // Get output shapes and types
    for (const name of session.outputNames) {
      const output = session.outputs.find(o => o.name === name);
      if (output) {
        metadata.outputs[name] = {
          shape: output.dims,
          type: output.type
        };
      }
    }

    return metadata;
  }

  /**
   * Run prediction with the model
   */
  async predict(modelId, input) {
    const session = this.sessions.get(modelId);
    if (!session) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    try {
      // Prepare input tensors
      const feeds = {};
      
      if (input instanceof Object &amp;&amp; !Array.isArray(input)) {
        // Input is already formatted as feeds
        for (const [name, data] of Object.entries(input)) {
          if (data instanceof this.ort.Tensor) {
            feeds[name] = data;
          } else {
            // Create tensor from data
            const inputMeta = session.inputs.find(i => i.name === name);
            feeds[name] = new this.ort.Tensor(
              inputMeta?.type || 'float32',
              data.data || data,
              data.shape || inputMeta?.dims
            );
          }
        }
      } else {
        // Assume single input
        const inputName = session.inputNames[0];
        const inputMeta = session.inputs[0];
        
        if (input instanceof this.ort.Tensor) {
          feeds[inputName] = input;
        } else {
          feeds[inputName] = new this.ort.Tensor(
            inputMeta?.type || 'float32',
            Array.isArray(input) ? input : [input],
            inputMeta?.dims || [1, input.length]
          );
        }
      }

      // Run inference
      const results = await session.run(feeds);
      
      // Process outputs
      const outputs = {};
      for (const [name, tensor] of Object.entries(results)) {
        outputs[name] = {
          data: Array.from(tensor.data),
          shape: tensor.dims,
          type: tensor.type
        };
      }

      // Return single output if only one, otherwise return all
      const outputNames = Object.keys(outputs);
      if (outputNames.length === 1) {
        return outputs[outputNames[0]].data;
      }
      
      return outputs;
    } catch (error) {
      this.logger.error(`Prediction failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Stream predictions (for compatible models)
   */
  async *stream(modelId, input) {
    // ONNX doesn't natively support streaming, but we can simulate it
    // by running inference in chunks for sequence models
    
    const session = this.sessions.get(modelId);
    if (!session) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    try {
      // Check if this is a sequence model
      const hasSequenceInput = session.inputNames.some(name => 
        name.toLowerCase().includes('sequence') || 
        name.toLowerCase().includes('token')
      );

      if (hasSequenceInput) {
        // Process as sequence model
        let context = input;
        let generated = [];
        const maxTokens = 100; // Default max tokens
        
        for (let i = 0; i &lt; maxTokens; i++) {
          const output = await this.predict(modelId, context);
          
          // Extract next token (simplified - actual implementation would vary)
          const nextToken = Array.isArray(output) ? output[0] : output;
          generated.push(nextToken);
          
          yield nextToken;
          
          // Update context for next iteration
          context = [...context, nextToken];
          
          // Check for end token (simplified)
          if (nextToken === 0 || nextToken === '&lt;/s>') {
            break;
          }
        }
      } else {
        // For non-sequence models, just return the full prediction
        const output = await this.predict(modelId, input);
        yield output;
      }
    } catch (error) {
      this.logger.error(`Streaming failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Unload a model
   */
  async unload(modelId) {
    const session = this.sessions.get(modelId);
    if (session) {
      // ONNX Runtime doesn't have explicit disposal in JS
      // but we can clear our reference
      this.sessions.delete(modelId);
      this.logger.info(`Model ${modelId} unloaded`);
      return true;
    }
    return false;
  }

  /**
   * Get model info
   */
  getModelInfo(modelId) {
    const session = this.sessions.get(modelId);
    if (!session) {
      return null;
    }

    return {
      id: modelId,
      format: 'onnx',
      loaded: true,
      inputNames: session.inputNames,
      outputNames: session.outputNames,
      executionProviders: this.getExecutionProviders()
    };
  }

  /**
   * Validate model compatibility
   */
  async validate(config) {
    try {
      // Try to create a session to validate
      if (!this.ort) {
        await this.initialize();
      }
      
      if (!this.ort) {
        return {
          valid: false,
          error: 'ONNX Runtime not available'
        };
      }

      // Quick validation by trying to create session
      const session = await this.ort.InferenceSession.create(config.source, {
        executionProviders: this.getExecutionProviders()
      });
      
      return {
        valid: true,
        metadata: this.extractMetadata(session)
      };
    } catch (error) {
      return {
        valid: false,
        error: error.message
      };
    }
  }
}
export default ONNXLoader;
export { ONNXLoader };
</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Sun Aug 17 2025 22:28:58 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
