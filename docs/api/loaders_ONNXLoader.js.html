<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/ONNXLoader.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/ONNXLoader.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * ONNX Model Loader
 * Supports loading and running ONNX format models
 * Uses ONNX Runtime Web for browser/Node.js compatibility
 */

import { BaseLoader } from './BaseLoader.js';
import { Logger } from '../utils/Logger.js';

class ONNXLoader extends BaseLoader {
  constructor() {
    super();
    this.logger = new Logger('ONNXLoader');
    this.sessions = new Map();
    this.ort = null;
  }

  /**
   * Initialize ONNX Runtime
   * 
   * @example
   * // Initialize loader before use
   * const loader = new ONNXLoader();
   * const initialized = await loader.initialize();
   * if (initialized) {
   *   console.log('ONNX Runtime ready');
   * } else {
   *   console.log('ONNX Runtime not available, using fallback');
   * }
   * 
   * @example
   * // Check runtime availability
   * const loader = new ONNXLoader();
   * try {
   *   await loader.initialize();
   *   // Proceed with ONNX model loading
   * } catch (error) {
   *   console.error('ONNX initialization failed:', error.message);
   *   // Use alternative loader
   * }
   */
  async initialize() {
    try {
      // Dynamically import ONNX Runtime based on environment
      if (typeof window !== 'undefined') {
        // Browser environment
        this.ort = await import('onnxruntime-web');
      } else {
        // Node.js environment
        this.ort = await import('onnxruntime-node');
      }
      
      this.logger.info('ONNX Runtime initialized');
      return true;
    } catch (error) {
      this.logger.warn('ONNX Runtime not available, using fallback', error);
      return false;
    }
  }

  /**
   * Check if this loader supports the given source
   * 
   * @example
   * // Check file extension support
   * const loader = new ONNXLoader();
   * console.log(loader.supports('./model.onnx')); // true
   * console.log(loader.supports('./model.ort'));  // true
   * console.log(loader.supports('./model.bin'));  // false
   * 
   * @example
   * // Validate model sources before loading
   * const sources = [
   *   './models/resnet50.onnx',
   *   'https://example.com/bert.onnx',
   *   './models/gpt2.ort',
   *   './invalid/model.bin'
   * ];
   * 
   * const supportedSources = sources.filter(src => loader.supports(src));
   * console.log(`${supportedSources.length} supported sources found`);
   * 
   * @example
   * // Dynamic loader selection
   * const modelPath = './models/transformer.onnx';
   * if (onnxLoader.supports(modelPath)) {
   *   await onnxLoader.load({ source: modelPath });
   * } else if (ggufLoader.supports(modelPath)) {
   *   await ggufLoader.load({ source: modelPath });
   * }
   */
  supports(source) {
    if (typeof source !== 'string') return false;
    
    // Check file extensions
    if (source.endsWith('.onnx')) return true;
    if (source.endsWith('.ort')) return true;
    
    // Check for ONNX in path
    if (source.includes('onnx')) return true;
    
    return false;
  }

  /**
   * Load an ONNX model
   * 
   * @example
   * // Load from local file
   * const model = await loader.load({
   *   id: 'resnet-50',
   *   name: 'ResNet-50 Image Classifier',
   *   source: './models/resnet50.onnx'
   * });
   * 
   * @example
   * // Load from URL with custom session options
   * const model = await loader.load({
   *   source: 'https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v1-7.onnx',
   *   sessionOptions: {
   *     executionProviders: ['webgl', 'wasm'],
   *     graphOptimizationLevel: 'all',
   *     enableMemPattern: true
   *   }
   * });
   * 
   * @example
   * // Load from ArrayBuffer
   * const response = await fetch('./model.onnx');
   * const arrayBuffer = await response.arrayBuffer();
   * 
   * const model = await loader.load({
   *   source: arrayBuffer,
   *   name: 'Loaded from Buffer'
   * });
   * 
   * @example
   * // Load with error handling and metadata inspection
   * try {
   *   const model = await loader.load({
   *     source: './models/bert-base.onnx'
   *   });
   *   
   *   console.log('Model loaded successfully');
   *   console.log('Input names:', model.metadata.inputNames);
   *   console.log('Output names:', model.metadata.outputNames);
   *   
   *   // Use the model
   *   const result = await model.predict(inputData);
   * } catch (error) {
   *   console.error('Failed to load ONNX model:', error.message);
   * }
   */
  async load(config) {
    const modelId = config.id || `onnx-${Date.now()}`;
    
    try {
      // Initialize ONNX Runtime if not already done
      if (!this.ort) {
        const initialized = await this.initialize();
        if (!initialized) {
          throw new Error('Failed to initialize ONNX Runtime');
        }
      }

      this.logger.info(`Loading ONNX model: ${config.source}`);
      
      // Create inference session
      let session;
      
      if (config.source.startsWith('http')) {
        // Load from URL
        session = await this.ort.InferenceSession.create(config.source, {
          executionProviders: this.getExecutionProviders(),
          graphOptimizationLevel: 'all',
          ...config.sessionOptions
        });
      } else if (typeof config.source === 'string') {
        // Load from file path
        session = await this.ort.InferenceSession.create(config.source, {
          executionProviders: this.getExecutionProviders(),
          graphOptimizationLevel: 'all',
          ...config.sessionOptions
        });
      } else if (config.source instanceof ArrayBuffer) {
        // Load from ArrayBuffer
        session = await this.ort.InferenceSession.create(config.source, {
          executionProviders: this.getExecutionProviders(),
          graphOptimizationLevel: 'all',
          ...config.sessionOptions
        });
      } else {
        throw new Error('Invalid source type for ONNX model');
      }

      // Store session
      this.sessions.set(modelId, session);

      // Get model metadata
      const metadata = this.extractMetadata(session);

      this.logger.info(`ONNX model loaded successfully: ${modelId}`);
      
      return {
        id: modelId,
        name: config.name || 'ONNX Model',
        format: 'onnx',
        loaded: true,
        session,
        metadata,
        predict: (input) => this.predict(modelId, input),
        stream: (input) => this.stream(modelId, input),
        unload: () => this.unload(modelId)
      };
    } catch (error) {
      this.logger.error(`Failed to load ONNX model: ${error.message}`);
      throw error;
    }
  }

  /**
   * Get execution providers based on environment
   * 
   * @example
   * // Check available execution providers
   * const loader = new ONNXLoader();
   * const providers = loader.getExecutionProviders();
   * console.log('Available providers:', providers);
   * // Browser: ['webgl', 'wasm']
   * // Node.js: ['cpu']
   * 
   * @example
   * // Use providers for session configuration
   * const providers = loader.getExecutionProviders();
   * const model = await loader.load({
   *   source: './model.onnx',
   *   sessionOptions: {
   *     executionProviders: providers,
   *     logSeverityLevel: 0 // Enable detailed logging
   *   }
   * });
   * 
   * @example
   * // Conditional provider selection
   * const providers = loader.getExecutionProviders();
   * if (providers.includes('webgl')) {
   *   console.log('GPU acceleration available');
   * } else {
   *   console.log('Using CPU execution');
   * }
   */
  getExecutionProviders() {
    const providers = [];
    
    if (typeof window !== 'undefined') {
      // Browser environment
      providers.push('webgl'); // WebGL backend for GPU acceleration
      providers.push('wasm'); // WebAssembly fallback
    } else {
      // Node.js environment
      providers.push('cpu'); // CPU backend
    }
    
    return providers;
  }

  /**
   * Extract metadata from ONNX session
   */
  extractMetadata(session) {
    const metadata = {
      inputNames: session.inputNames,
      outputNames: session.outputNames,
      inputs: {},
      outputs: {}
    };

    // Get input shapes and types
    for (const name of session.inputNames) {
      const input = session.inputs.find(i => i.name === name);
      if (input) {
        metadata.inputs[name] = {
          shape: input.dims,
          type: input.type
        };
      }
    }

    // Get output shapes and types
    for (const name of session.outputNames) {
      const output = session.outputs.find(o => o.name === name);
      if (output) {
        metadata.outputs[name] = {
          shape: output.dims,
          type: output.type
        };
      }
    }

    return metadata;
  }

  /**
   * Run prediction with the model
   * 
   * @example
   * // Simple prediction with array input
   * const input = [1.0, 2.0, 3.0, 4.0];
   * const output = await loader.predict('model-id', input);
   * console.log('Prediction result:', output);
   * 
   * @example
   * // Multi-input prediction with named inputs
   * const inputs = {
   *   'input_ids': [101, 7592, 2088, 102],      // Token IDs
   *   'attention_mask': [1, 1, 1, 1],          // Attention mask
   *   'token_type_ids': [0, 0, 0, 0]           // Segment IDs
   * };
   * 
   * const result = await loader.predict('bert-model', inputs);
   * console.log('BERT output:', result);
   * 
   * @example
   * // Image classification with tensor input
   * const imageData = new Float32Array(224 * 224 * 3); // RGB image data
   * // ... populate imageData with normalized pixel values ...
   * 
   * const input = {
   *   'data': {
   *     data: imageData,
   *     shape: [1, 3, 224, 224] // NCHW format
   *   }
   * };
   * 
   * const prediction = await loader.predict('resnet-50', input);
   * const probabilities = prediction.data;
   * const topClass = probabilities.indexOf(Math.max(...probabilities));
   * console.log('Predicted class:', topClass);
   * 
   * @example
   * // Batch prediction with error handling
   * const batchInputs = [
   *   [1.0, 2.0, 3.0],
   *   [4.0, 5.0, 6.0],
   *   [7.0, 8.0, 9.0]
   * ];
   * 
   * const results = [];
   * for (const input of batchInputs) {
   *   try {
   *     const result = await loader.predict('model-id', input);
   *     results.push(result);
   *   } catch (error) {
   *     console.error('Prediction failed for input:', input, error.message);
   *     results.push(null);
   *   }
   * }
   * 
   * console.log(`Processed ${results.filter(r => r !== null).length}/${batchInputs.length} predictions`);
   */
  async predict(modelId, input) {
    const session = this.sessions.get(modelId);
    if (!session) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    try {
      // Prepare input tensors
      const feeds = {};
      
      if (input instanceof Object &amp;&amp; !Array.isArray(input)) {
        // Input is already formatted as feeds
        for (const [name, data] of Object.entries(input)) {
          if (data instanceof this.ort.Tensor) {
            feeds[name] = data;
          } else {
            // Create tensor from data
            const inputMeta = session.inputs.find(i => i.name === name);
            feeds[name] = new this.ort.Tensor(
              inputMeta?.type || 'float32',
              data.data || data,
              data.shape || inputMeta?.dims
            );
          }
        }
      } else {
        // Assume single input
        const inputName = session.inputNames[0];
        const inputMeta = session.inputs[0];
        
        if (input instanceof this.ort.Tensor) {
          feeds[inputName] = input;
        } else {
          feeds[inputName] = new this.ort.Tensor(
            inputMeta?.type || 'float32',
            Array.isArray(input) ? input : [input],
            inputMeta?.dims || [1, input.length]
          );
        }
      }

      // Run inference
      const results = await session.run(feeds);
      
      // Process outputs
      const outputs = {};
      for (const [name, tensor] of Object.entries(results)) {
        outputs[name] = {
          data: Array.from(tensor.data),
          shape: tensor.dims,
          type: tensor.type
        };
      }

      // Return single output if only one, otherwise return all
      const outputNames = Object.keys(outputs);
      if (outputNames.length === 1) {
        return outputs[outputNames[0]].data;
      }
      
      return outputs;
    } catch (error) {
      this.logger.error(`Prediction failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Stream predictions (for compatible models)
   * 
   * @example
   * // Stream text generation tokens
   * const prompt = [101, 7592, 2088]; // Token IDs for "hello world"
   * 
   * console.log('Streaming tokens:');
   * for await (const token of loader.stream('gpt-model', prompt)) {
   *   process.stdout.write(token + ' ');
   * }
   * console.log('\nGeneration complete');
   * 
   * @example
   * // Stream with token accumulation
   * const prompt = [101, 7592];
   * let generatedTokens = [];
   * let tokenCount = 0;
   * 
   * for await (const token of loader.stream('language-model', prompt)) {
   *   generatedTokens.push(token);
   *   tokenCount++;
   *   
   *   console.log(`Token ${tokenCount}: ${token}`);
   *   
   *   // Stop after generating 20 tokens
   *   if (tokenCount >= 20) {
   *     break;
   *   }
   * }
   * 
   * console.log('Generated sequence:', generatedTokens.join(' '));
   * 
   * @example
   * // Non-streaming model fallback
   * try {
   *   let hasTokens = false;
   *   for await (const token of loader.stream('classification-model', input)) {
   *     hasTokens = true;
   *     console.log('Token:', token);
   *   }
   *   
   *   if (!hasTokens) {
   *     console.log('Model does not support streaming, got full result');
   *   }
   * } catch (error) {
   *   console.error('Streaming failed:', error.message);
   *   // Fall back to regular prediction
   *   const result = await loader.predict('classification-model', input);
   *   console.log('Fallback result:', result);
   * }
   * 
   * @example
   * // Real-time streaming with UI updates
   * const outputElement = document.getElementById('output');
   * let partialText = '';
   * 
   * for await (const token of loader.stream('text-model', inputTokens)) {
   *   partialText += token;
   *   outputElement.textContent = partialText;
   *   
   *   // Add typing delay for better UX
   *   await new Promise(resolve => setTimeout(resolve, 50));
   * }
   */
  async *stream(modelId, input) {
    // ONNX doesn't natively support streaming, but we can simulate it
    // by running inference in chunks for sequence models
    
    const session = this.sessions.get(modelId);
    if (!session) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    try {
      // Check if this is a sequence model
      const hasSequenceInput = session.inputNames.some(name => 
        name.toLowerCase().includes('sequence') || 
        name.toLowerCase().includes('token')
      );

      if (hasSequenceInput) {
        // Process as sequence model
        let context = input;
        let generated = [];
        const maxTokens = 100; // Default max tokens
        
        for (let i = 0; i &lt; maxTokens; i++) {
          const output = await this.predict(modelId, context);
          
          // Extract next token (simplified - actual implementation would vary)
          const nextToken = Array.isArray(output) ? output[0] : output;
          generated.push(nextToken);
          
          yield nextToken;
          
          // Update context for next iteration
          context = [...context, nextToken];
          
          // Check for end token (simplified)
          if (nextToken === 0 || nextToken === '&lt;/s>') {
            break;
          }
        }
      } else {
        // For non-sequence models, just return the full prediction
        const output = await this.predict(modelId, input);
        yield output;
      }
    } catch (error) {
      this.logger.error(`Streaming failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Unload a model
   */
  async unload(modelId) {
    const session = this.sessions.get(modelId);
    if (session) {
      // ONNX Runtime doesn't have explicit disposal in JS
      // but we can clear our reference
      this.sessions.delete(modelId);
      this.logger.info(`Model ${modelId} unloaded`);
      return true;
    }
    return false;
  }

  /**
   * Get model info
   */
  getModelInfo(modelId) {
    const session = this.sessions.get(modelId);
    if (!session) {
      return null;
    }

    return {
      id: modelId,
      format: 'onnx',
      loaded: true,
      inputNames: session.inputNames,
      outputNames: session.outputNames,
      executionProviders: this.getExecutionProviders()
    };
  }

  /**
   * Validate model compatibility
   * 
   * @example
   * // Validate before loading
   * const validation = await loader.validate({
   *   source: './models/questionable-model.onnx'
   * });
   * 
   * if (validation.valid) {
   *   console.log('Model is valid, loading...');
   *   console.log('Metadata:', validation.metadata);
   *   await loader.load({ source: './models/questionable-model.onnx' });
   * } else {
   *   console.error('Model validation failed:', validation.error);
   * }
   * 
   * @example
   * // Batch validate multiple models
   * const modelPaths = [
   *   './models/resnet50.onnx',
   *   './models/bert-base.onnx',
   *   './models/corrupted.onnx',
   *   './models/not-onnx.bin'
   * ];
   * 
   * const validationResults = await Promise.all(
   *   modelPaths.map(async (path) => {
   *     const result = await loader.validate({ source: path });
   *     return { path, ...result };
   *   })
   * );
   * 
   * const validModels = validationResults.filter(r => r.valid);
   * console.log(`${validModels.length}/${modelPaths.length} models are valid`);
   * 
   * validModels.forEach(model => {
   *   console.log(`${model.path}: ${model.metadata.inputNames.length} inputs, ${model.metadata.outputNames.length} outputs`);
   * });
   * 
   * @example
   * // Validate with detailed error handling
   * try {
   *   const validation = await loader.validate({
   *     source: 'https://example.com/remote-model.onnx'
   *   });
   *   
   *   if (validation.valid) {
   *     const metadata = validation.metadata;
   *     console.log('Input shapes:', Object.values(metadata.inputs).map(i => i.shape));
   *     console.log('Output shapes:', Object.values(metadata.outputs).map(o => o.shape));
   *   } else {
   *     console.error('Validation failed:', validation.error);
   *   }
   * } catch (error) {
   *   console.error('Validation error:', error.message);
   *   // Network error, file not found, etc.
   * }
   */
  async validate(config) {
    try {
      // Try to create a session to validate
      if (!this.ort) {
        await this.initialize();
      }
      
      if (!this.ort) {
        return {
          valid: false,
          error: 'ONNX Runtime not available'
        };
      }

      // Quick validation by trying to create session
      const session = await this.ort.InferenceSession.create(config.source, {
        executionProviders: this.getExecutionProviders()
      });
      
      return {
        valid: true,
        metadata: this.extractMetadata(session)
      };
    } catch (error) {
      return {
        valid: false,
        error: error.message
      };
    }
  }
}
export default ONNXLoader;
export { ONNXLoader };
</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="APILoader.html">APILoader</a></li><li><a href="AnthropicAdapter.html">AnthropicAdapter</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthManager.html">AuthManager</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="AzureOpenAIAdapter.html">AzureOpenAIAdapter</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BedrockAdapter.html">BedrockAdapter</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="CohereAdapter.html">CohereAdapter</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="DeepSeekAdapter.html">DeepSeekAdapter</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FireworksAdapter.html">FireworksAdapter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="GroqAdapter.html">GroqAdapter</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MistralAdapter.html">MistralAdapter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="NovitaAdapter.html">NovitaAdapter</a></li><li><a href="OpenAIAdapter.html">OpenAIAdapter</a></li><li><a href="OpenRouterAdapter.html">OpenRouterAdapter</a></li><li><a href="PerplexityAdapter.html">PerplexityAdapter</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TogetherAdapter.html">TogetherAdapter</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="VertexAIAdapter.html">VertexAIAdapter</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#ADAPTER_REGISTRY">ADAPTER_REGISTRY</a></li><li><a href="global.html#API_KEY_PATTERNS">API_KEY_PATTERNS</a></li><li><a href="global.html#AUTH_TYPES">AUTH_TYPES</a></li><li><a href="global.html#AZURE_API_VERSIONS">AZURE_API_VERSIONS</a></li><li><a href="global.html#AZURE_OPENAI_MODELS">AZURE_OPENAI_MODELS</a></li><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BEDROCK_MODELS">BEDROCK_MODELS</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#CLAUDE_MODELS">CLAUDE_MODELS</a></li><li><a href="global.html#COHERE_MODELS">COHERE_MODELS</a></li><li><a href="global.html#COMPLIANCE_FEATURES">COMPLIANCE_FEATURES</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#DEEPSEEK_ENDPOINTS">DEEPSEEK_ENDPOINTS</a></li><li><a href="global.html#DEEPSEEK_MODELS">DEEPSEEK_MODELS</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#FIREWORKS_MODELS">FIREWORKS_MODELS</a></li><li><a href="global.html#GROQ_MODELS">GROQ_MODELS</a></li><li><a href="global.html#INPUT_TYPES">INPUT_TYPES</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#MISTRAL_MODELS">MISTRAL_MODELS</a></li><li><a href="global.html#MODEL_CATEGORIES">MODEL_CATEGORIES</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#NOVITA_ENDPOINTS">NOVITA_ENDPOINTS</a></li><li><a href="global.html#NOVITA_MODELS">NOVITA_MODELS</a></li><li><a href="global.html#OPENAI_MODELS">OPENAI_MODELS</a></li><li><a href="global.html#PERPLEXITY_MODELS">PERPLEXITY_MODELS</a></li><li><a href="global.html#POPULAR_MODELS">POPULAR_MODELS</a></li><li><a href="global.html#PROVIDER_AUTH_CONFIG">PROVIDER_AUTH_CONFIG</a></li><li><a href="global.html#PROVIDER_CATEGORIES">PROVIDER_CATEGORIES</a></li><li><a href="global.html#PROVIDER_CONFIGS">PROVIDER_CONFIGS</a></li><li><a href="global.html#PROVIDER_FEATURES">PROVIDER_FEATURES</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SAFETY_LEVELS">SAFETY_LEVELS</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TOGETHER_MODELS">TOGETHER_MODELS</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#VERTEX_AI_MODELS">VERTEX_AI_MODELS</a></li><li><a href="global.html#VERTEX_REGIONS">VERTEX_REGIONS</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createAdapter">createAdapter</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getAdapter">getAdapter</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getProviderAuthType">getProviderAuthType</a></li><li><a href="global.html#getProviderInfo">getProviderInfo</a></li><li><a href="global.html#getProvidersByCategory">getProvidersByCategory</a></li><li><a href="global.html#getProvidersByFeature">getProvidersByFeature</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#getSupportedProviders">getSupportedProviders</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#isProviderSupported">isProviderSupported</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Wed Aug 20 2025 19:41:21 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
