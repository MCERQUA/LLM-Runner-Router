<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: loaders/adapters/TogetherAdapter.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: loaders/adapters/TogetherAdapter.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/**
 * üöÄ Together AI Adapter
 * High-performance inference platform with 200+ open-source models
 * Features: Production-ready scaling, model fine-tuning, batch inference
 */

import APILoader from '../APILoader.js';
import { Logger } from '../../utils/Logger.js';
import { AuthManager } from '../../utils/AuthManager.js';

const logger = new Logger('TogetherAdapter');

/**
 * Together AI model configurations
 */
const TOGETHER_MODELS = {
  // Meta Llama models
  'meta-llama/Llama-2-70b-chat-hf': {
    name: 'Llama 2 70B Chat',
    contextWindow: 4096,
    maxOutput: 2048,
    cost: { input: 0.9, output: 0.9 },
    features: ['conversational', 'open_source', 'apache_license'],
    provider_model: 'meta-llama/Llama-2-70b-chat-hf',
    category: 'chat'
  },
  'meta-llama/Llama-2-13b-chat-hf': {
    name: 'Llama 2 13B Chat',
    contextWindow: 4096,
    maxOutput: 2048,
    cost: { input: 0.225, output: 0.225 },
    features: ['conversational', 'efficient', 'open_source'],
    provider_model: 'meta-llama/Llama-2-13b-chat-hf',
    category: 'chat'
  },
  'meta-llama/Llama-2-7b-chat-hf': {
    name: 'Llama 2 7B Chat',
    contextWindow: 4096,
    maxOutput: 2048,
    cost: { input: 0.2, output: 0.2 },
    features: ['lightweight', 'fast', 'open_source'],
    provider_model: 'meta-llama/Llama-2-7b-chat-hf',
    category: 'chat'
  },
  'meta-llama/CodeLlama-34b-Instruct-hf': {
    name: 'Code Llama 34B Instruct',
    contextWindow: 16384,
    maxOutput: 8192,
    cost: { input: 0.776, output: 0.776 },
    features: ['code_generation', 'instruction_following', 'long_context'],
    provider_model: 'meta-llama/CodeLlama-34b-Instruct-hf',
    category: 'code'
  },

  // Mistral models
  'mistralai/Mixtral-8x7B-Instruct-v0.1': {
    name: 'Mixtral 8x7B Instruct',
    contextWindow: 32768,
    maxOutput: 8192,
    cost: { input: 0.6, output: 0.6 },
    features: ['mixture_of_experts', 'multilingual', 'instruction_following'],
    provider_model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',
    category: 'instruct'
  },
  'mistralai/Mistral-7B-Instruct-v0.1': {
    name: 'Mistral 7B Instruct',
    contextWindow: 8192,
    maxOutput: 4096,
    cost: { input: 0.2, output: 0.2 },
    features: ['instruction_following', 'efficient', 'multilingual'],
    provider_model: 'mistralai/Mistral-7B-Instruct-v0.1',
    category: 'instruct'
  },

  // Microsoft models
  'microsoft/DialoGPT-large': {
    name: 'DialoGPT Large',
    contextWindow: 1024,
    maxOutput: 1024,
    cost: { input: 0.1, output: 0.1 },
    features: ['conversational', 'dialogue'],
    provider_model: 'microsoft/DialoGPT-large',
    category: 'chat'
  },

  // EleutherAI models
  'EleutherAI/gpt-neox-20b': {
    name: 'GPT-NeoX 20B',
    contextWindow: 2048,
    maxOutput: 2048,
    cost: { input: 0.5, output: 0.5 },
    features: ['general_purpose', 'open_source'],
    provider_model: 'EleutherAI/gpt-neox-20b',
    category: 'completion'
  },

  // Salesforce models
  'Salesforce/codegen-16B-multi': {
    name: 'CodeGen 16B Multi',
    contextWindow: 2048,
    maxOutput: 2048,
    cost: { input: 0.4, output: 0.4 },
    features: ['code_generation', 'multilingual_code'],
    provider_model: 'Salesforce/codegen-16B-multi',
    category: 'code'
  },

  // BigScience models
  'bigscience/bloom': {
    name: 'BLOOM 176B',
    contextWindow: 2048,
    maxOutput: 2048,
    cost: { input: 5.0, output: 5.0 },
    features: ['multilingual', 'large_scale', 'open_source'],
    provider_model: 'bigscience/bloom',
    category: 'completion'
  },

  // Stability AI models
  'stabilityai/stablelm-tuned-alpha-7b': {
    name: 'StableLM Tuned Alpha 7B',
    contextWindow: 4096,
    maxOutput: 2048,
    cost: { input: 0.2, output: 0.2 },
    features: ['stable_ai', 'tuned', 'conversational'],
    provider_model: 'stabilityai/stablelm-tuned-alpha-7b',
    category: 'chat'
  }
};

/**
 * Together AI model categories
 */
const MODEL_CATEGORIES = {
  'chat': 'Conversational models',
  'instruct': 'Instruction-following models', 
  'code': 'Code generation models',
  'completion': 'Text completion models',
  'embedding': 'Embedding models'
};

/**
 * Together AI adapter with high-performance features
 */
class TogetherAdapter extends APILoader {
  constructor(config = {}) {
    super({
      ...config,
      provider: 'together',
      baseURL: config.baseURL || 'https://api.together.xyz/v1',
      apiKey: config.apiKey || process.env.TOGETHER_API_KEY
    });

    // Together-specific configuration
    this.enableBatchMode = config.enableBatchMode || false;
    this.maxBatchSize = config.maxBatchSize || 10;
    this.enableCaching = config.enableCaching !== false;
    this.cacheDirectory = config.cacheDirectory || './together_cache';

    // Performance optimization
    this.enableGPUOptimization = config.enableGPUOptimization !== false;
    this.preferredRegion = config.preferredRegion || 'us-west-2';
    this.enableLoadBalancing = config.enableLoadBalancing !== false;

    this.authManager = new AuthManager();
    this.models = new Map();
    this.batchQueue = [];
    this.modelCache = new Map();

    // Validate configuration
    this.validateConfig();

    logger.info(`üöÄ Together AI Adapter initialized (Region: ${this.preferredRegion})`);
  }

  /**
   * Validate Together AI configuration
   */
  validateConfig() {
    if (!this.apiKey) {
      throw new Error('Together AI API key is required');
    }

    const validation = this.authManager.validateApiKey('together', this.apiKey);
    if (!validation.valid) {
      logger.warn(`Together AI API key format validation: ${validation.error || validation.warning}`);
    }
  }

  /**
   * Get Together AI headers
   */
  getHeaders() {
    return {
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json',
      'User-Agent': 'LLM-Runner-Router/2.0.0'
    };
  }

  /**
   * Load Together AI model
   */
  async load(modelId, options = {}) {
    try {
      const modelConfig = TOGETHER_MODELS[modelId];
      
      if (!modelConfig) {
        // Try to fetch from Together AI API
        const availableModels = await this.fetchAvailableModels();
        const foundModel = availableModels.find(m => m.id === modelId || m.display_name === modelId);
        
        if (!foundModel) {
          logger.warn(`Unknown Together AI model: ${modelId}. Using default config.`);
        }
      }

      // Test model availability
      if (options.testConnection !== false) {
        await this.testModelAvailability(modelId);
      }

      const model = {
        id: `together:${modelId}`,
        provider: 'together',
        modelId: modelId,
        providerModel: modelConfig?.provider_model || modelId,
        type: 'together',
        config: {
          enableCaching: this.enableCaching,
          preferredRegion: this.preferredRegion,
          ...options
        },
        metadata: {
          ...modelConfig,
          streaming: true,
          high_performance: true,
          open_source: true,
          batch_support: this.enableBatchMode,
          loaded: true,
          loadedAt: Date.now()
        }
      };

      this.models.set(modelId, model);
      this.model = model;
      this.loaded = true;

      logger.success(`‚úÖ Loaded Together AI model: ${model.id}`);
      return model;

    } catch (error) {
      logger.error(`Failed to load Together AI model ${modelId}:`, error);
      throw error;
    }
  }

  /**
   * Test Together AI model availability
   */
  async testModelAvailability(modelId) {
    try {
      const response = await fetch(`${this.baseURL}/models`, {
        headers: this.getHeaders()
      });

      if (!response.ok) {
        throw new Error(`Model availability check failed: ${response.status}`);
      }

      const data = await response.json();
      const availableModels = data.data?.map(m => m.id) || [];

      const providerModel = TOGETHER_MODELS[modelId]?.provider_model || modelId;
      if (!availableModels.includes(providerModel)) {
        logger.warn(`Model ${modelId} (${providerModel}) may not be available`);
      }

      logger.debug(`‚úÖ Model availability test passed for ${modelId}`);
      return true;
    } catch (error) {
      logger.warn(`‚ö†Ô∏è Model availability test failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Fetch available models from Together AI
   */
  async fetchAvailableModels() {
    if (this.modelCache.has('available_models')) {
      const cached = this.modelCache.get('available_models');
      if (Date.now() - cached.timestamp &lt; 3600000) { // 1 hour cache
        return cached.models;
      }
    }

    try {
      const response = await fetch(`${this.baseURL}/models`, {
        headers: this.getHeaders()
      });

      if (response.ok) {
        const data = await response.json();
        const models = data.data || [];
        
        this.modelCache.set('available_models', {
          models,
          timestamp: Date.now()
        });

        return models;
      }
    } catch (error) {
      logger.warn('Failed to fetch available models:', error);
    }

    return [];
  }

  /**
   * Generate completion using Together AI
   */
  async complete(prompt, options = {}) {
    const model = options.model || this.model;
    
    if (!model) {
      throw new Error('No model loaded');
    }

    // Handle batch mode
    if (this.enableBatchMode &amp;&amp; options.batch) {
      return this.addToBatch(prompt, model, options);
    }

    // Build Together AI request
    const request = this.buildTogetherRequest(prompt, model, options);
    const endpoint = this.getTogetherEndpoint(model);

    try {
      if (options.stream &amp;&amp; model.metadata.streaming) {
        return await this.streamCompletion(request, endpoint, model, options);
      } else {
        return await this.standardCompletion(request, endpoint, model, options);
      }
    } catch (error) {
      logger.error(`Together AI request failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Build Together AI request
   */
  buildTogetherRequest(prompt, model, options) {
    const modelCategory = model.metadata?.category || 'completion';
    const providerModel = model.providerModel || model.modelId;

    const baseRequest = {
      model: providerModel,
      max_tokens: options.maxTokens || model.metadata.maxOutput || 1000,
      temperature: options.temperature || 0.7,
      top_p: options.topP || 1.0,
      top_k: options.topK || 50,
      repetition_penalty: options.repetitionPenalty || 1.0,
      stream: options.stream || false
    };

    // Format request based on model category
    switch (modelCategory) {
      case 'chat':
      case 'instruct':
        return {
          ...baseRequest,
          messages: options.messages || [
            { role: 'user', content: prompt }
          ]
        };

      case 'completion':
      case 'code':
      default:
        return {
          ...baseRequest,
          prompt: prompt,
          stop: options.stop || []
        };
    }
  }

  /**
   * Get Together AI endpoint
   */
  getTogetherEndpoint(model) {
    const modelCategory = model.metadata?.category || 'completion';
    
    switch (modelCategory) {
      case 'chat':
      case 'instruct':
        return `${this.baseURL}/chat/completions`;
      
      default:
        return `${this.baseURL}/completions`;
    }
  }

  /**
   * Standard (non-streaming) completion
   */
  async standardCompletion(request, endpoint, model, options) {
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: this.getHeaders(),
      body: JSON.stringify(request)
    });

    if (!response.ok) {
      await this.handleTogetherError(response);
    }

    const data = await response.json();
    return this.parseTogetherResponse(data, model);
  }

  /**
   * Streaming completion
   */
  async streamCompletion(request, endpoint, model, options) {
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: this.getHeaders(),
      body: JSON.stringify({ ...request, stream: true })
    });

    if (!response.ok) {
      await this.handleTogetherError(response);
    }

    return this.handleTogetherStream(response, model);
  }

  /**
   * Handle Together AI errors
   */
  async handleTogetherError(response) {
    const errorText = await response.text();
    let errorData;
    
    try {
      errorData = JSON.parse(errorText);
    } catch {
      errorData = { message: errorText };
    }

    // Together-specific error handling
    if (response.status === 401) {
      throw new Error('Together AI authentication failed. Check API key.');
    } else if (response.status === 402) {
      throw new Error('Together AI quota exceeded. Check billing.');
    } else if (response.status === 429) {
      const retryAfter = response.headers.get('retry-after');
      throw new Error(`Together AI rate limit exceeded. Retry after ${retryAfter} seconds.`);
    } else if (response.status === 503) {
      throw new Error('Together AI service temporarily unavailable. Try again later.');
    }

    const message = errorData.error?.message || errorData.message || `Together AI error (${response.status})`;
    throw new Error(message);
  }

  /**
   * Parse Together AI response
   */
  parseTogetherResponse(data, model) {
    const modelCategory = model.metadata?.category || 'completion';
    let text, usage, finishReason;

    if (modelCategory === 'chat' || modelCategory === 'instruct') {
      // Chat completion format
      const choice = data.choices?.[0];
      text = choice?.message?.content || '';
      finishReason = choice?.finish_reason || 'unknown';
      usage = data.usage || {};
    } else {
      // Text completion format
      const choice = data.choices?.[0];
      text = choice?.text || '';
      finishReason = choice?.finish_reason || 'unknown';
      usage = data.usage || {};
    }

    return {
      text,
      model: model.id,
      provider: 'together',
      usage: {
        promptTokens: usage.prompt_tokens || 0,
        completionTokens: usage.completion_tokens || 0,
        totalTokens: usage.total_tokens || 0
      },
      cost: this.calculateCost(usage, model.modelId),
      finishReason,
      metadata: {
        category: model.metadata?.category,
        openSource: true,
        highPerformance: true
      },
      timestamp: Date.now()
    };
  }

  /**
   * Handle Together AI streaming response
   */
  async *handleTogetherStream(response, model) {
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') return;

            try {
              const parsed = JSON.parse(data);
              let content = '';

              // Handle different response formats
              if (parsed.choices?.[0]?.delta?.content) {
                content = parsed.choices[0].delta.content;
              } else if (parsed.choices?.[0]?.text) {
                content = parsed.choices[0].text;
              }
              
              if (content) {
                yield {
                  text: content,
                  model: model.id,
                  provider: 'together',
                  chunk: true,
                  metadata: {
                    category: model.metadata?.category
                  },
                  timestamp: Date.now()
                };
              }
            } catch (e) {
              // Skip invalid JSON
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * Add request to batch queue
   */
  addToBatch(prompt, model, options) {
    const batchItem = {
      id: `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      prompt,
      model,
      options,
      timestamp: Date.now()
    };

    this.batchQueue.push(batchItem);

    // Process batch if queue is full
    if (this.batchQueue.length >= this.maxBatchSize) {
      return this.processBatch();
    }

    return Promise.resolve({
      batchId: batchItem.id,
      queued: true,
      queueSize: this.batchQueue.length
    });
  }

  /**
   * Process batch queue
   */
  async processBatch() {
    if (this.batchQueue.length === 0) {
      return [];
    }

    const batch = this.batchQueue.splice(0, this.maxBatchSize);
    logger.info(`Processing batch of ${batch.length} requests`);

    try {
      const results = await Promise.all(
        batch.map(item => this.complete(item.prompt, {
          ...item.options,
          model: item.model,
          batch: false // Prevent recursion
        }))
      );

      return batch.map((item, index) => ({
        id: item.id,
        result: results[index],
        success: true
      }));
    } catch (error) {
      logger.error('Batch processing failed:', error);
      return batch.map(item => ({
        id: item.id,
        error: error.message,
        success: false
      }));
    }
  }

  /**
   * Calculate cost based on Together AI pricing
   */
  calculateCost(usage, modelId) {
    const modelConfig = TOGETHER_MODELS[modelId];
    if (!modelConfig || !modelConfig.cost) return 0;

    const inputCost = (usage.prompt_tokens / 1000000) * modelConfig.cost.input;
    const outputCost = (usage.completion_tokens / 1000000) * modelConfig.cost.output;
    
    return inputCost + outputCost;
  }

  /**
   * List available Together AI models
   */
  async listModels() {
    try {
      const apiModels = await this.fetchAvailableModels();
      
      // Combine API models with known configurations
      const models = apiModels.map(apiModel => ({
        id: apiModel.id,
        name: apiModel.display_name || apiModel.id,
        provider: 'together',
        description: apiModel.description,
        context_length: apiModel.context_length,
        metadata: TOGETHER_MODELS[apiModel.id] || {
          name: apiModel.display_name || apiModel.id,
          contextWindow: apiModel.context_length || 2048,
          category: 'completion'
        }
      }));

      return models;
    } catch (error) {
      logger.warn('Failed to fetch Together models, using defaults');
      
      // Return default models
      return Object.keys(TOGETHER_MODELS).map(id => ({
        id,
        name: TOGETHER_MODELS[id].name,
        provider: 'together',
        metadata: TOGETHER_MODELS[id]
      }));
    }
  }

  /**
   * Get adapter information
   */
  getInfo() {
    return {
      name: 'TogetherAdapter',
      version: '1.0.0',
      provider: 'together',
      baseURL: this.baseURL,
      preferredRegion: this.preferredRegion,
      enableBatchMode: this.enableBatchMode,
      maxBatchSize: this.maxBatchSize,
      queueSize: this.batchQueue.length,
      modelsLoaded: this.models.size,
      features: ['streaming', 'batch_processing', 'high_performance', 'open_source', 'model_variety'],
      categories: Object.keys(MODEL_CATEGORIES),
      models: Object.keys(TOGETHER_MODELS),
      status: 'ready'
    };
  }

  /**
   * Unload model
   */
  async unload(modelId) {
    if (this.models.has(modelId)) {
      this.models.delete(modelId);
      logger.info(`Together AI model ${modelId} unloaded`);
      return true;
    }
    return false;
  }

  /**
   * Clean up resources
   */
  async dispose() {
    // Process remaining batch items
    if (this.batchQueue.length > 0) {
      logger.info(`Processing remaining ${this.batchQueue.length} batch items`);
      await this.processBatch();
    }

    this.models.clear();
    this.modelCache.clear();
    this.batchQueue = [];
    logger.info('Together AI adapter disposed');
  }
}

export default TogetherAdapter;</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="ABTestingManager.html">ABTestingManager</a></li><li><a href="APILoader.html">APILoader</a></li><li><a href="AnthropicAdapter.html">AnthropicAdapter</a></li><li><a href="AuditLogger.html">AuditLogger</a></li><li><a href="AuthManager.html">AuthManager</a></li><li><a href="AuthMiddleware.html">AuthMiddleware</a></li><li><a href="AzureOpenAIAdapter.html">AzureOpenAIAdapter</a></li><li><a href="BPETokenizer.html">BPETokenizer</a></li><li><a href="BaseEngine.html">BaseEngine</a></li><li><a href="BaseLoader.html">BaseLoader</a></li><li><a href="BedrockAdapter.html">BedrockAdapter</a></li><li><a href="BinaryLoader.html">BinaryLoader</a></li><li><a href="BinaryModel.html">BinaryModel</a></li><li><a href="BitNetLoader.html">BitNetLoader</a></li><li><a href="CohereAdapter.html">CohereAdapter</a></li><li><a href="ConversionConfig.html">ConversionConfig</a></li><li><a href="ConversionResult.html">ConversionResult</a></li><li><a href="DeepSeekAdapter.html">DeepSeekAdapter</a></li><li><a href="EnterpriseAuthManager.html">EnterpriseAuthManager</a></li><li><a href="EnterpriseManager.html">EnterpriseManager</a></li><li><a href="EnterpriseRouter.html">EnterpriseRouter</a></li><li><a href="ErrorHandler.html">ErrorHandler</a></li><li><a href="FireworksAdapter.html">FireworksAdapter</a></li><li><a href="FormatConverter.html">FormatConverter</a></li><li><a href="GGUFLoader.html">GGUFLoader</a></li><li><a href="GGUFModel.html">GGUFModel</a></li><li><a href="GRPCClient.html">GRPCClient</a></li><li><a href="GroqAdapter.html">GroqAdapter</a></li><li><a href="LLMRouter.html">LLMRouter</a></li><li><a href="MistralAdapter.html">MistralAdapter</a></li><li><a href="MockLoader.html">MockLoader</a></li><li><a href="MockModel.html">MockModel</a></li><li><a href="ModelError.html">ModelError</a></li><li><a href="ModelInterface.html">ModelInterface</a></li><li><a href="ModelQuantizer.html">ModelQuantizer</a></li><li><a href="ModelRegistry.html">ModelRegistry</a></li><li><a href="ModelTemplates.html">ModelTemplates</a></li><li><a href="MultiTenancyManager.html">MultiTenancyManager</a></li><li><a href="NovitaAdapter.html">NovitaAdapter</a></li><li><a href="OpenAIAdapter.html">OpenAIAdapter</a></li><li><a href="OpenRouterAdapter.html">OpenRouterAdapter</a></li><li><a href="PerplexityAdapter.html">PerplexityAdapter</a></li><li><a href="Pipeline.html">Pipeline</a></li><li><a href="PyTorchLoader.html">PyTorchLoader</a></li><li><a href="PyTorchModel.html">PyTorchModel</a></li><li><a href="QuantizationConfig.html">QuantizationConfig</a></li><li><a href="QuantizationResult.html">QuantizationResult</a></li><li><a href="Router.html">Router</a></li><li><a href="SLAMonitor.html">SLAMonitor</a></li><li><a href="SentencePieceTokenizer.html">SentencePieceTokenizer</a></li><li><a href="SimpleLoader.html">SimpleLoader</a></li><li><a href="SimpleModel.html">SimpleModel</a></li><li><a href="TogetherAdapter.html">TogetherAdapter</a></li><li><a href="TokenizationResult.html">TokenizationResult</a></li><li><a href="TokenizerConfig.html">TokenizerConfig</a></li><li><a href="UniversalTokenizer.html">UniversalTokenizer</a></li><li><a href="ValidationConfig.html">ValidationConfig</a></li><li><a href="ValidationSuite.html">ValidationSuite</a></li><li><a href="ValidationSuiteResult.html">ValidationSuiteResult</a></li><li><a href="ValidationTestResult.html">ValidationTestResult</a></li><li><a href="VertexAIAdapter.html">VertexAIAdapter</a></li><li><a href="WordPieceTokenizer.html">WordPieceTokenizer</a></li></ul><h3>Global</h3><ul><li><a href="global.html#ADAPTER_REGISTRY">ADAPTER_REGISTRY</a></li><li><a href="global.html#API_KEY_PATTERNS">API_KEY_PATTERNS</a></li><li><a href="global.html#AUTH_TYPES">AUTH_TYPES</a></li><li><a href="global.html#AZURE_API_VERSIONS">AZURE_API_VERSIONS</a></li><li><a href="global.html#AZURE_OPENAI_MODELS">AZURE_OPENAI_MODELS</a></li><li><a href="global.html#Architectures">Architectures</a></li><li><a href="global.html#AuditEventTypes">AuditEventTypes</a></li><li><a href="global.html#AuthMethods">AuthMethods</a></li><li><a href="global.html#BEDROCK_MODELS">BEDROCK_MODELS</a></li><li><a href="global.html#BreachSeverity">BreachSeverity</a></li><li><a href="global.html#CLAUDE_MODELS">CLAUDE_MODELS</a></li><li><a href="global.html#COHERE_MODELS">COHERE_MODELS</a></li><li><a href="global.html#COMPLIANCE_FEATURES">COMPLIANCE_FEATURES</a></li><li><a href="global.html#Capabilities">Capabilities</a></li><li><a href="global.html#ComplianceFrameworks">ComplianceFrameworks</a></li><li><a href="global.html#DEEPSEEK_ENDPOINTS">DEEPSEEK_ENDPOINTS</a></li><li><a href="global.html#DEEPSEEK_MODELS">DEEPSEEK_MODELS</a></li><li><a href="global.html#EnterpriseFeatures">EnterpriseFeatures</a></li><li><a href="global.html#ExperimentStatus">ExperimentStatus</a></li><li><a href="global.html#FIREWORKS_MODELS">FIREWORKS_MODELS</a></li><li><a href="global.html#GROQ_MODELS">GROQ_MODELS</a></li><li><a href="global.html#INPUT_TYPES">INPUT_TYPES</a></li><li><a href="global.html#IsolationLevels">IsolationLevels</a></li><li><a href="global.html#MISTRAL_MODELS">MISTRAL_MODELS</a></li><li><a href="global.html#MODEL_CATEGORIES">MODEL_CATEGORIES</a></li><li><a href="global.html#ModelFormat">ModelFormat</a></li><li><a href="global.html#ModelFormats">ModelFormats</a></li><li><a href="global.html#NOVITA_ENDPOINTS">NOVITA_ENDPOINTS</a></li><li><a href="global.html#NOVITA_MODELS">NOVITA_MODELS</a></li><li><a href="global.html#OPENAI_MODELS">OPENAI_MODELS</a></li><li><a href="global.html#PERPLEXITY_MODELS">PERPLEXITY_MODELS</a></li><li><a href="global.html#POPULAR_MODELS">POPULAR_MODELS</a></li><li><a href="global.html#PROVIDER_AUTH_CONFIG">PROVIDER_AUTH_CONFIG</a></li><li><a href="global.html#PROVIDER_CATEGORIES">PROVIDER_CATEGORIES</a></li><li><a href="global.html#PROVIDER_CONFIGS">PROVIDER_CONFIGS</a></li><li><a href="global.html#PROVIDER_FEATURES">PROVIDER_FEATURES</a></li><li><a href="global.html#Permissions">Permissions</a></li><li><a href="global.html#QuantizationMethod">QuantizationMethod</a></li><li><a href="global.html#QuantizationPrecision">QuantizationPrecision</a></li><li><a href="global.html#QuotaTypes">QuotaTypes</a></li><li><a href="global.html#RiskLevels">RiskLevels</a></li><li><a href="global.html#RoutingStrategies">RoutingStrategies</a></li><li><a href="global.html#SAFETY_LEVELS">SAFETY_LEVELS</a></li><li><a href="global.html#SLAMetricTypes">SLAMetricTypes</a></li><li><a href="global.html#SLAStatus">SLAStatus</a></li><li><a href="global.html#SessionTypes">SessionTypes</a></li><li><a href="global.html#SplittingAlgorithms">SplittingAlgorithms</a></li><li><a href="global.html#StatisticalTests">StatisticalTests</a></li><li><a href="global.html#TOGETHER_MODELS">TOGETHER_MODELS</a></li><li><a href="global.html#TimeWindows">TimeWindows</a></li><li><a href="global.html#TokenizerType">TokenizerType</a></li><li><a href="global.html#UserRoles">UserRoles</a></li><li><a href="global.html#VERTEX_AI_MODELS">VERTEX_AI_MODELS</a></li><li><a href="global.html#VERTEX_REGIONS">VERTEX_REGIONS</a></li><li><a href="global.html#ValidationSeverity">ValidationSeverity</a></li><li><a href="global.html#ValidationTestType">ValidationTestType</a></li><li><a href="global.html#adjustTimeouts">adjustTimeouts</a></li><li><a href="global.html#attemptRecovery">attemptRecovery</a></li><li><a href="global.html#checkConnectivity">checkConnectivity</a></li><li><a href="global.html#clearCache">clearCache</a></li><li><a href="global.html#colors">colors</a></li><li><a href="global.html#createAdapter">createAdapter</a></li><li><a href="global.html#createEnterpriseExpressRoutes">createEnterpriseExpressRoutes</a></li><li><a href="global.html#createEnterpriseRouter">createEnterpriseRouter</a></li><li><a href="global.html#createEnterpriseWebSocketHandlers">createEnterpriseWebSocketHandlers</a></li><li><a href="global.html#createMissingResources">createMissingResources</a></li><li><a href="global.html#defaultEnterpriseConfig">defaultEnterpriseConfig</a></li><li><a href="global.html#emergencyShutdown">emergencyShutdown</a></li><li><a href="global.html#enterpriseVersion">enterpriseVersion</a></li><li><a href="global.html#errorMonitoringMiddleware">errorMonitoringMiddleware</a></li><li><a href="global.html#escalateError">escalateError</a></li><li><a href="global.html#executeRecovery">executeRecovery</a></li><li><a href="global.html#getAdapter">getAdapter</a></li><li><a href="global.html#getEnabledFeatures">getEnabledFeatures</a></li><li><a href="global.html#getMonitoringStatus">getMonitoringStatus</a></li><li><a href="global.html#getProviderAuthType">getProviderAuthType</a></li><li><a href="global.html#getProviderInfo">getProviderInfo</a></li><li><a href="global.html#getProvidersByCategory">getProvidersByCategory</a></li><li><a href="global.html#getProvidersByFeature">getProvidersByFeature</a></li><li><a href="global.html#getStats">getStats</a></li><li><a href="global.html#getSupportedProviders">getSupportedProviders</a></li><li><a href="global.html#gracefulShutdown">gracefulShutdown</a></li><li><a href="global.html#handleCriticalError">handleCriticalError</a></li><li><a href="global.html#handleMemoryLeak">handleMemoryLeak</a></li><li><a href="global.html#httpMonitoringMiddleware">httpMonitoringMiddleware</a></li><li><a href="global.html#isFeatureEnabled">isFeatureEnabled</a></li><li><a href="global.html#isProviderSupported">isProviderSupported</a></li><li><a href="global.html#logError">logError</a></li><li><a href="global.html#performHealthCheck">performHealthCheck</a></li><li><a href="global.html#recordCustomMetric">recordCustomMetric</a></li><li><a href="global.html#registerAlertRule">registerAlertRule</a></li><li><a href="global.html#registerDependency">registerDependency</a></li><li><a href="global.html#registerHealthCheck">registerHealthCheck</a></li><li><a href="global.html#reinstallDependencies">reinstallDependencies</a></li><li><a href="global.html#reload">reload</a></li><li><a href="global.html#restartProcess">restartProcess</a></li><li><a href="global.html#retryConnection">retryConnection</a></li><li><a href="global.html#selectRecoveryStrategy">selectRecoveryStrategy</a></li><li><a href="global.html#setupHandlers">setupHandlers</a></li><li><a href="global.html#setupMonitoring">setupMonitoring</a></li><li><a href="global.html#softRestart">softRestart</a></li><li><a href="global.html#startHealthMonitoring">startHealthMonitoring</a></li><li><a href="global.html#startPerformanceProfile">startPerformanceProfile</a></li><li><a href="global.html#validateEnterpriseConfig">validateEnterpriseConfig</a></li><li><a href="global.html#withCacheMonitoring">withCacheMonitoring</a></li><li><a href="global.html#withDatabaseMonitoring">withDatabaseMonitoring</a></li><li><a href="global.html#withModelMonitoring">withModelMonitoring</a></li><li><a href="global.html#withQueueMonitoring">withQueueMonitoring</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc/jsdoc">JSDoc 4.0.4</a> on Wed Aug 20 2025 19:41:21 GMT+0000 (Coordinated Universal Time)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
