# LLM Runner Router - Environment Variables Example
# Configuration for HuggingFace model demo
# Copy this file to .env for local development if needed

# ===========================================
# HUGGINGFACE MODEL CONFIGURATION
# ===========================================

# HuggingFace Access Token (Optional - for private models or higher rate limits)
# Get yours at: https://huggingface.co/settings/tokens
# This demo works with public models so token is optional
HUGGINGFACE_TOKEN=hf_your_token_here_optional

# Model Cache Directory (Optional - defaults to browser cache)
MODEL_CACHE_DIR=./models

# CORS Proxy (Optional - for development only)
# Some models may require a CORS proxy for browser access
CORS_PROXY_URL=https://cors-anywhere.herokuapp.com

# ===========================================
# ROUTER CONFIGURATION
# ===========================================

# Default routing strategy (balanced, quality-first, speed-priority)
DEFAULT_STRATEGY=balanced

# Maximum tokens per request (default: 150)
MAX_TOKENS=150

# Default temperature (0.0-1.0, default: 0.7)
DEFAULT_TEMPERATURE=0.7

# Enable debug logging (true/false, default: false)
DEBUG_LOGGING=false

# WebGPU preference (true/false, default: true)
# Falls back to WASM if WebGPU unavailable
PREFER_WEBGPU=true

# ===========================================
# DEVELOPMENT SETUP
# ===========================================
# For local development:
# 1. Clone the repository
# 2. Run: npm install
# 3. Run: npm run dev
# 4. Access: http://localhost:3000/chat/
# 
# For production build:
# 1. Run: npm run build
# 2. Deploy the dist/ folder to any static hosting

# ===========================================
# SUPPORTED MODELS
# ===========================================
# The demo uses these HuggingFace models:
# - microsoft/DialoGPT-small (Fast, conversational)
# - microsoft/DialoGPT-medium (Balanced quality/speed)
# - HuggingFaceH4/zephyr-7b-beta (High quality, slower)
# 
# Models are automatically downloaded and cached on first use

# ===========================================
# COST & PERFORMANCE
# ===========================================
# HuggingFace Models are FREE to use!
# - No API costs or rate limits
# - Models run locally in your browser
# - First download may take time (models cached afterward)
# - Performance depends on device capabilities

# Model Sizes (approximate):
# - DialoGPT-small: ~117MB
# - DialoGPT-medium: ~345MB  
# - Zephyr-7B-beta: ~4.1GB (GGUF quantized)

# ===========================================
# BROWSER REQUIREMENTS
# ===========================================
# Minimum Requirements:
# - Modern browser with WebAssembly support
# - 2GB+ available RAM for medium models
# - 8GB+ RAM recommended for larger models
# 
# Optimal Performance:
# - WebGPU-enabled browser (Chrome 113+, Edge 113+)
# - Dedicated GPU with 4GB+ VRAM
# - Fast internet for initial model downloads

# ===========================================
# TROUBLESHOOTING
# ===========================================
# Common issues:
# 1. "WebGPU not supported" = Falls back to WASM automatically
# 2. "Out of memory" = Try smaller models or close other tabs
# 3. "Model loading failed" = Check internet connection, try refresh
# 4. "CORS errors" = Some models need CORS proxy for dev
# 5. "Slow performance" = First run downloads models, subsequent runs are faster

# ===========================================
# SUPPORT
# ===========================================
# Need help? 
# - GitHub Issues: https://github.com/MCERQUA/LLM-Runner-Router/issues
# - Documentation: See project README.md
# - Email: echoaisystems@gmail.com