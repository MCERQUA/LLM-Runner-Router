# VPS-Optimized Environment Configuration for LLM Router
# For 4 vCPU, 16GB RAM Virtual Private Server

# Server Configuration
NODE_ENV=production
PORT=3000
HOST=0.0.0.0

# Memory Management
MAX_OLD_SPACE_SIZE=2560  # 2.5GB heap limit
MAX_MODEL_SIZE=2GB        # Maximum size for individual models
MODEL_CACHE_SIZE=1GB      # Total cache size for models
MEMORY_CHECK_INTERVAL=30000  # Check memory every 30 seconds

# CPU Configuration
USE_CPU_ONLY=true         # Force CPU-only mode (no GPU on VPS)
ENABLE_GPU=false          # Explicitly disable GPU
CPU_THREADS=2             # Use 2 threads (half of 4 vCPUs)
BATCH_SIZE=1              # Small batch size for stability

# Model Loading Strategy
CONCURRENT_MODELS=1       # Load only 1 model at a time
AUTO_UNLOAD=true         # Automatically unload inactive models
UNLOAD_TIMEOUT=300000    # Unload after 5 minutes of inactivity
PRELOAD_MODELS=false     # Don't preload models on startup
LAZY_LOADING=true        # Load models only when needed

# Performance Tuning
INFERENCE_TIMEOUT=60000   # 60 second timeout for inference
MAX_QUEUE_SIZE=10        # Limit request queue
RATE_LIMIT=100           # Max 100 requests per minute
CACHE_TTL=3600000        # Cache for 1 hour

# Logging
LOG_LEVEL=info           # info, debug, warn, error
LOG_TO_FILE=true
LOG_DIR=./logs
LOG_MAX_SIZE=10m         # Rotate logs at 10MB
LOG_MAX_FILES=5          # Keep 5 log files

# Health Monitoring
HEALTH_CHECK_ENABLED=true
HEALTH_CHECK_INTERVAL=30000  # Check every 30 seconds
AUTO_RESTART_ON_FAILURE=true
MAX_FAILURES_BEFORE_ALERT=3

# Model Fallback Strategy
FALLBACK_TO_SIMPLE_LOADER=true  # Use SimpleLoader if GGUF fails
ENABLE_MODEL_FALLBACK=true      # Enable fallback chain
FALLBACK_TIMEOUT=5000           # 5 second timeout before fallback

# API Keys (comma-separated if multiple)
API_KEYS=

# Debug Options
DEBUG=false
VERBOSE_LOGGING=false
PROFILE_MEMORY=true     # Monitor memory usage

# Safety Limits
MAX_REQUEST_SIZE=10mb
MAX_RESPONSE_SIZE=10mb
REQUEST_TIMEOUT=120000   # 2 minute total request timeout

# Experimental Features
ENABLE_STREAMING=true    # Enable token streaming
ENABLE_CACHING=true      # Enable response caching
ENABLE_COMPRESSION=true  # Enable response compression