/**
 * SmolLM3-3B Loader using Transformers.js
 * Real SmolLM3-3B model with proper chat templates and project context
 */

import { BaseLoader } from './BaseLoader.js';
import { Logger } from '../utils/Logger.js';
import { pipeline } from '@xenova/transformers';

class SmolLM3Loader extends BaseLoader {
  constructor() {
    super();
    this.logger = new Logger('SmolLM3Loader');
    this.pipelines = new Map();
    this.models = new Map();
    
    // SmolLM3 chat template
    this.chatTemplate = {
      system: "<|system|>\n{content}<|end|>\n",
      user: "<|user|>\n{content}<|end|>\n",
      assistant: "<|assistant|>\n{content}<|end|>\n",
      start: "<|assistant|>\n"
    };
    
    // Project-specific system prompt
    this.systemPrompt = `You are SmolLM3, an AI assistant specialized in helping with the LLM Runner Router project. This is a universal LLM orchestration system that provides format-agnostic model loading and intelligent routing.

Key project features you should know about:
- Universal model loading (GGUF, ONNX, Safetensors, HuggingFace, etc.)
- Intelligent routing strategies (quality-first, cost-optimized, speed-priority, balanced)
- Real-time model inference and streaming support
- WebGPU, WASM, and Node.js runtime engines
- Simple API design for easy integration

Respond helpfully about the project, its architecture, usage patterns, and technical implementation. Be concise and technical when appropriate.`;
  }

  supports(source) {
    return typeof source === 'string' && 
           (source.includes('smollm') || source.includes('SmolLM') || source.includes('gpt'));
  }

  async load(config) {
    const modelId = config.id || `model_${Date.now()}`;
    
    this.logger.info(`üöÄ Loading SmolLM3-3B model: ${modelId}`);

    try {
      let generator;
      
      // Try to load actual SmolLM3-3B model
      try {
        this.logger.info('Loading SmolLM3-3B-Base model...');
        generator = await pipeline('text-generation', 'HuggingFaceTB/SmolLM3-3B-Base', {
          device: 'cpu',
          dtype: 'fp32'
        });
        this.logger.success('‚úÖ SmolLM3-3B-Base model loaded');
      } catch (error) {
        this.logger.warn('SmolLM3-3B not available, trying GPT-2 fallback...');
        generator = await pipeline('text-generation', 'Xenova/gpt2', {
          device: 'cpu',
          dtype: 'fp32'
        });
        this.logger.success('‚úÖ GPT-2 fallback model loaded');
      }
      
      this.pipelines.set(modelId, generator);
      
      const model = {
        id: modelId,
        name: 'SmolLM3-3B',
        format: 'transformers',
        loader: 'SmolLM3Loader',
        loaded: true,
        
        predict: async (input, options = {}) => {
          const start = Date.now();
          
          // Format input with chat template and system prompt
          const formattedInput = this.formatChatInput(input, options.useSystemPrompt !== false);
          
          const result = await generator(formattedInput, {
            max_new_tokens: options.maxTokens || 100,
            temperature: options.temperature || 0.7,
            do_sample: options.temperature > 0,
            top_p: options.topP || 0.9,
            top_k: options.topK || 50,
            repetition_penalty: options.repetitionPenalty || 1.1
          });
          
          const inference_time = Date.now() - start;
          const generated_text = result[0].generated_text;
          const new_text = this.extractResponse(generated_text, formattedInput);
          
          this.logger.success(`‚úÖ Generated ${new_text.length} chars in ${inference_time}ms`);
          
          return {
            text: new_text,
            response: new_text,
            inference_time_ms: inference_time,
            tokens: Math.floor(new_text.length / 4)
          };
        },
        
        stream: async function() {
          throw new Error('Streaming not implemented');
        },
        
        chat: async function(messages, options = {}) {
          // Format all messages with proper chat template
          const formattedInput = this.formatMessages(messages);
          
          const start = Date.now();
          const result = await generator(formattedInput, {
            max_new_tokens: options.maxTokens || 100,
            temperature: options.temperature || 0.7,
            do_sample: options.temperature > 0,
            top_p: options.topP || 0.9,
            top_k: options.topK || 50,
            repetition_penalty: options.repetitionPenalty || 1.1
          });
          
          const inference_time = Date.now() - start;
          const generated_text = result[0].generated_text;
          const new_text = this.extractResponse(generated_text, formattedInput);
          
          this.logger.success(`‚úÖ Chat response generated in ${inference_time}ms`);
          
          return {
            text: new_text,
            response: new_text,
            inference_time_ms: inference_time,
            tokens: Math.floor(new_text.length / 4)
          };
        }.bind(this),
        
        unload: () => {
          this.pipelines.delete(modelId);
          this.models.delete(modelId);
        }
      };
      
      this.models.set(modelId, model);
      this.logger.success(`‚úÖ Model ready: ${modelId}`);
      return model;
      
    } catch (error) {
      this.logger.error(`‚ùå Failed to load model: ${error.message}`);
      throw new Error(`Model loading failed: ${error.message}`);
    }
  }

  async unload(modelId) {
    this.pipelines.delete(modelId);
    this.models.delete(modelId);
  }

  /**
   * Format input with SmolLM3 chat template
   */
  formatChatInput(input, includeSystem = true) {
    let formatted = '';
    
    // Add system prompt if requested
    if (includeSystem) {
      formatted += this.chatTemplate.system.replace('{content}', this.systemPrompt);
    }
    
    // Add user input
    formatted += this.chatTemplate.user.replace('{content}', input);
    
    // Start assistant response
    formatted += this.chatTemplate.start;
    
    return formatted;
  }

  /**
   * Extract clean response from generated text
   */
  extractResponse(generatedText, originalInput) {
    // Remove the input prompt from the response
    let response = generatedText.substring(originalInput.length);
    
    // Clean up any remaining template tokens
    response = response.replace(/<\|end\|>.*$/s, '');
    response = response.replace(/<\|.*?\|>/g, '');
    
    return response.trim();
  }

  /**
   * Format messages for chat conversation
   */
  formatMessages(messages) {
    let formatted = this.chatTemplate.system.replace('{content}', this.systemPrompt);
    
    for (const message of messages) {
      const role = message.role || 'user';
      const content = message.content || message;
      
      if (role === 'user') {
        formatted += this.chatTemplate.user.replace('{content}', content);
      } else if (role === 'assistant') {
        formatted += this.chatTemplate.assistant.replace('{content}', content);
      }
    }
    
    // Start new assistant response
    formatted += this.chatTemplate.start;
    
    return formatted;
  }
}

export default SmolLM3Loader;