syntax = "proto3";

package llm_router;

// Service definition for LLM Router
service LLMRouterService {
  // Load a model
  rpc LoadModel(LoadModelRequest) returns (LoadModelResponse);
  
  // Unload a model
  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse);
  
  // List available models
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);
  
  // Get model status
  rpc GetModelStatus(GetModelStatusRequest) returns (GetModelStatusResponse);
  
  // Single inference
  rpc Inference(InferenceRequest) returns (InferenceResponse);
  
  // Streaming inference
  rpc StreamInference(InferenceRequest) returns (stream InferenceStreamResponse);
  
  // Bidirectional streaming chat
  rpc Chat(stream ChatRequest) returns (stream ChatResponse);
  
  // Batch inference
  rpc BatchInference(BatchInferenceRequest) returns (BatchInferenceResponse);
  
  // Health check
  rpc HealthCheck(HealthCheckRequest) returns (HealthCheckResponse);
  
  // Service discovery
  rpc ServiceDiscovery(ServiceDiscoveryRequest) returns (ServiceDiscoveryResponse);
  
  // Model routing
  rpc RouteModel(RouteModelRequest) returns (RouteModelResponse);
  
  // Performance metrics
  rpc GetMetrics(GetMetricsRequest) returns (GetMetricsResponse);
}

// Load model request
message LoadModelRequest {
  string source = 1;
  string format = 2;
  string id = 3;
  string name = 4;
  map<string, string> parameters = 5;
  bool force_reload = 6;
}

// Load model response
message LoadModelResponse {
  bool success = 1;
  string message = 2;
  ModelInfo model = 3;
  string error = 4;
}

// Unload model request
message UnloadModelRequest {
  string model_id = 1;
  bool force = 2;
}

// Unload model response
message UnloadModelResponse {
  bool success = 1;
  string message = 2;
  string error = 3;
}

// List models request
message ListModelsRequest {
  bool include_unloaded = 1;
  string filter = 2;
  int32 limit = 3;
  int32 offset = 4;
}

// List models response
message ListModelsResponse {
  repeated ModelInfo models = 1;
  int32 total_count = 2;
  bool has_more = 3;
}

// Get model status request
message GetModelStatusRequest {
  string model_id = 1;
}

// Get model status response
message GetModelStatusResponse {
  bool exists = 1;
  ModelInfo model = 2;
  ModelMetrics metrics = 3;
  string error = 4;
}

// Inference request
message InferenceRequest {
  string prompt = 1;
  string model_id = 2;
  InferenceOptions options = 3;
  map<string, string> metadata = 4;
  string session_id = 5;
}

// Inference response
message InferenceResponse {
  string text = 1;
  string model_id = 2;
  InferenceMetrics metrics = 3;
  bool success = 4;
  string error = 5;
  map<string, string> metadata = 6;
}

// Streaming inference response
message InferenceStreamResponse {
  string token = 1;
  bool is_complete = 2;
  string model_id = 3;
  InferenceMetrics metrics = 4;
  string error = 5;
}

// Chat request
message ChatRequest {
  repeated ChatMessage messages = 1;
  string model_id = 2;
  InferenceOptions options = 3;
  string session_id = 4;
  map<string, string> metadata = 5;
}

// Chat response
message ChatResponse {
  ChatMessage message = 1;
  string model_id = 2;
  InferenceMetrics metrics = 3;
  bool is_complete = 4;
  string error = 5;
}

// Batch inference request
message BatchInferenceRequest {
  repeated InferenceRequest requests = 1;
  BatchOptions options = 2;
}

// Batch inference response
message BatchInferenceResponse {
  repeated InferenceResponse responses = 1;
  BatchMetrics metrics = 2;
  bool success = 3;
  string error = 4;
}

// Health check request
message HealthCheckRequest {
  string service = 1;
}

// Health check response
message HealthCheckResponse {
  HealthStatus status = 1;
  string message = 2;
  map<string, string> details = 3;
  int64 timestamp = 4;
}

// Service discovery request
message ServiceDiscoveryRequest {
  string service_type = 1;
  string region = 2;
}

// Service discovery response
message ServiceDiscoveryResponse {
  repeated ServiceInfo services = 1;
  string version = 2;
}

// Route model request
message RouteModelRequest {
  string prompt = 1;
  map<string, string> requirements = 2;
  string strategy = 3;
}

// Route model response
message RouteModelResponse {
  string selected_model_id = 1;
  string strategy = 2;
  double confidence_score = 3;
  repeated string alternative_models = 4;
  string reasoning = 5;
}

// Get metrics request
message GetMetricsRequest {
  string model_id = 1;
  int64 start_time = 2;
  int64 end_time = 3;
  repeated string metric_types = 4;
}

// Get metrics response
message GetMetricsResponse {
  SystemMetrics system_metrics = 1;
  repeated ModelMetrics model_metrics = 2;
  map<string, double> custom_metrics = 3;
}

// Model information
message ModelInfo {
  string id = 1;
  string name = 2;
  string format = 3;
  string source = 4;
  bool loaded = 5;
  int64 load_time = 6;
  int64 memory_usage = 7;
  map<string, string> parameters = 8;
  string version = 9;
  repeated string capabilities = 10;
}

// Model metrics
message ModelMetrics {
  string model_id = 1;
  int64 total_requests = 2;
  int64 total_tokens = 3;
  double average_latency = 4;
  double tokens_per_second = 5;
  int64 memory_usage = 6;
  double cpu_usage = 7;
  int64 last_used = 8;
  double error_rate = 9;
}

// Inference options
message InferenceOptions {
  int32 max_tokens = 1;
  double temperature = 2;
  double top_p = 3;
  int32 top_k = 4;
  double frequency_penalty = 5;
  double presence_penalty = 6;
  repeated string stop_sequences = 7;
  bool stream = 8;
  int32 seed = 9;
}

// Inference metrics
message InferenceMetrics {
  int64 latency_ms = 1;
  int32 tokens_generated = 2;
  double tokens_per_second = 3;
  int64 memory_used = 4;
  int64 processing_time = 5;
  int64 queue_time = 6;
}

// Chat message
message ChatMessage {
  string role = 1;
  string content = 2;
  map<string, string> metadata = 3;
  int64 timestamp = 4;
}

// Batch options
message BatchOptions {
  int32 max_concurrent = 1;
  int32 timeout_ms = 2;
  bool fail_fast = 3;
  string priority = 4;
}

// Batch metrics
message BatchMetrics {
  int32 total_requests = 1;
  int32 successful_requests = 2;
  int32 failed_requests = 3;
  int64 total_time_ms = 4;
  double average_latency_ms = 5;
}

// Health status
enum HealthStatus {
  UNKNOWN = 0;
  HEALTHY = 1;
  UNHEALTHY = 2;
  DEGRADED = 3;
  MAINTENANCE = 4;
}

// Service information
message ServiceInfo {
  string id = 1;
  string name = 2;
  string host = 3;
  int32 port = 4;
  string version = 5;
  HealthStatus status = 6;
  map<string, string> metadata = 7;
}

// System metrics
message SystemMetrics {
  double cpu_usage = 1;
  int64 memory_usage = 2;
  int64 memory_total = 3;
  int64 disk_usage = 4;
  int64 disk_total = 5;
  int32 active_connections = 6;
  int64 uptime_seconds = 7;
  double load_average = 8;
}